[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About UVic Spring School 2023",
    "section": "",
    "text": "This spring school is offered by Simon Fraser University on behalf of Western Universities and the Digital Research Alliance of Canada. It is hosted by the University of Victoria and is open to all researchers at UVic and other Canadian post-secondary institutions."
  },
  {
    "objectID": "about.html#registration",
    "href": "about.html#registration",
    "title": "About UVic Spring School 2023",
    "section": "Registration",
    "text": "Registration\nTo register, please fill in this form using your Canadian post-secondary institution email address."
  },
  {
    "objectID": "about.html#instructors",
    "href": "about.html#instructors",
    "title": "About UVic Spring School 2023",
    "section": "Instructors",
    "text": "Instructors\n\n\n\n\n\n\nAlex Razoumov earned his PhD in computational astrophysics from the University of British Columbia and held postdoctoral positions in Urbana-Champaign, San Diego, Oak Ridge, and Halifax. He spent five years as HPC Analyst in SHARCNET and in 2014 moved back to Vancouver to focus on scientific visualization and training researchers to use advanced computing tools. Alex is currently at Simon Fraser University.\n\n\n\n\n\n\n\n\n\n\n\n\nEvolutionary and behavioural ecologist by training, Software/Data Carpentry instructor, and open source advocate, Marie-Hélène Burle develops and delivers training for researchers on high-performance computing tools (R, Python, Julia, Git, Bash scripting, machine learning, parallel scientific programming, and HPC) for Simon Fraser University and the Digital Research Alliance of Canada."
  },
  {
    "objectID": "bash/index.html",
    "href": "bash/index.html",
    "title": "Bash",
    "section": "",
    "text": "Date:\nMonday May 1, 2023\nTime:\n9am–noon\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:\nThis introductory course does not require any previous experience.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there)."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please email us at: training at westdri dot ca.\nTo email the UVic organizing team, please use: xxx."
  },
  {
    "objectID": "hpc/index.html",
    "href": "hpc/index.html",
    "title": "HPC",
    "section": "",
    "text": "Date:\nMonday May 1, 2023\nTime:\n2pm–5pm\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:\nWorking knowledge of the Bash shell.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UVic Spring School 2023",
    "section": "",
    "text": "Each day we will have morning (9am–noon) and afternoon (2pm–5pm) sessions. There will be short coffee breaks at 10:30am and 3:30pm (coffee and snacks on us!), and we will provide lunch on Friday. The first four days participants will have to find lunch on their own.\nYou can attend as many or as few of the courses as you want. All sessions will be in the Clearihue building, room C112.\nTo participate in the hands-on exercises, you will need to bring your own laptop. We will provide guest accounts on our HPC training cluster. For more specific requirements, please see the individual course pages.\n\n\n\n\nMondayMay 1, 2023\n\n\n\n\nBash\nHalf-day introduction to Bash & the Unix shell\n\n\n\n\nHPC\nHalf-day introduction to high-performance research computing\n\n\n\n\n\n\nTuesdayMay 2, 2023\n\n\n\n\nIntro R\nHalf-day introduction to programming in R\n\n\n\n\nParallel R\nHalf-day introduction to high-performance R\n\n\n\n\n\n\nWednesdayMay 3, 2023\n\n\n\n\nParallel Julia\nFull-day introduction to parallel programming in Julia\n\n\n\n\n\n\nThursdayMay 4, 2023\n\n\n\n\nDeep learning with PyTorch\nFull-day introduction to deep learning with the PyTorch framework\n\n\n\n\n\n\nFridayMay 5, 2023\n\n\n\n\nScientific visualization with ParaView\nFull-day introduction to scientific visualization with ParaView\n\n\n\n\n \n\n\nHosted by:"
  },
  {
    "objectID": "julia/asm.html",
    "href": "julia/asm.html",
    "title": "Parallelizing iterative additive Schwarz method",
    "section": "",
    "text": "In this section we will implement the iterative additive Schwarz method (ASM) 1 in Julia, starting with a serial version. We will then parallelize it with DistributedArrays.jl.1 “An Introduction to Domain Decomposition Methods” by Victorita Dolean, Pierre Jolivet, and Frédéric Nataf\nWe will be solving the 1D Poisson problem\n\nDiscretizing this equation on a uniform grid of \\(m\\) points, we have\n\nor in matrix notation \\(AU=F\\), where\n\nLet’s break our grid into two domains \\(\\Omega=\\Omega_1\\bigcup\\Omega_2\\), where we are looking for the solution\n\nIn matrix notation the solution can be written as \\(U_i=R_iU\\), where the restriction operator \\(R_1\\) is a \\(m_s\\times m\\) matrix consisting of two parts of sizes \\(m_s\\times m_s\\) and \\(m_s\\times(m-m_s)\\), and \\(R_2\\) is a \\((m-m_s)\\times m\\) matrix consisting of two parts of sizes \\((m-m_s)\\times m_s\\) and \\((m-m_s)\\times(m-m_s)\\), respectively:\n\nThe iterative additive Schwarz method (eq. 1.30 of 2) lets you compute the next iteration of the solution as2 “An Introduction to Domain Decomposition Methods” by Victorita Dolean, Pierre Jolivet, and Frédéric Nataf\n\nwhere the matrix\n\nis called the ASM preconditioner.\n\nSerial additive Schwarz method with explicit matrices\nLet’s generalize our computation to three 1D domains. For now, we are writing the serial code, so all arrays are local. Also in this first code, we will define matrices explicitly, even though \\(A\\) is sparse, and \\(R_i\\) are Boolean matrices. All these shortcomings will be corrected in the parallel code.\nFor matrix inversion \\((R_iAT_i^T)^{-1}\\) we will use Julia’s builtin inv() function, but presumably for larger calculations you might want to replace this with your own (more efficient?) code.\n\n\n\nFor the right-hand side \\(F\\), we’ll use a constant piece in the middle, mimicking a uniform bar with empty spaces on each end.\nm = 21               # total number of points\nN = 3                # number of domains\nms = zeros(Int, N)   # number of points in each domain\nms[1:2] = [7,7]      # number of points in the first two domains, respectively\nms[3] = m - sum(ms)\nh = 1.0 / (m-1)      # grid spacing\n\nU = zeros(Float64, m, 1);   # 0th iteration\n\nF = zeros(Float64, m, 1);   # RHS\nF[trunc(Int,m/2)-3:trunc(Int,m/2)+3] .= h^2 * 1.0;\n\nA = zeros(Float64, m, m);\nA[1,1], A[m,m] = 1, 1\nfor i in 2:m-1\n    A[i, i-1] = -1\n    A[i,i] = 2\n    A[i, i+1] = -1\nend\n\nR1 = zeros(Int32, ms[1], m);\nfor j in 1:ms[1]\n    R1[j,j] = 1\nend\n\nR2 = zeros(Int32, ms[2], m);\nfor j in 1:ms[2]\n    R2[j,ms[1]+j] = 1\nend\n\nR3 = zeros(Int32, ms[3], m);\nfor j in 1:ms[3]\n    R3[j,ms[1]+ms[2]+j] = 1\nend\n\n# compute three terms in the ASM preconditioner\nM = transpose(R1) * inv(R1*A*transpose(R1)) * R1;\nM += transpose(R2) * inv(R2*A*transpose(R2)) * R2;\nM += transpose(R3) * inv(R3*A*transpose(R3)) * R3;\n\nusing LinearAlgebra: norm\nfor iter in 1:50\n    dU = M * (F-A*U)\n    global U += dU\n    println(norm(dU))\nend\n\nexact = A \\ F;   # Julia's left division, i.e. A^{-1}*F\nusing Plots\nplot(1:m, exact, label = \"exact\")\nplot!(1:m, U, label=\"approximate\")   # overplot\nsavefig(\"serial.png\")\n\n\nSerial additive Schwarz method with structures\nWe will generalize our code to an arbitrary number of domains. Instead of using R1, R2, R3, we will define a structure domainType that will contain the restriction operator for each domain. It could contain other variables and matrices, but for now we don’t need any other elements in it.\nSimilar to the previous version of the code, here we are still storing all sparse and Boolean matrices as dense matrices, which is Ok only for testing purposes (for real problems these matrices will be large).\nm = 21               # total number of points\nN = 3                # number of domains\nms = zeros(Int, N)   # number of points in each domain\nms[1:2] = [7,7]      # number of points in the first two domains, respectively\nms[3] = m - sum(ms)\nh = 1.0 / (m-1)      # grid spacing\n\nU = zeros(Float64, m, 1);   # 0th iteration; will be a distributed 2D array\n\nF = zeros(Float64, m, 1);   # RHS; will be a distributed 2D array\nF[trunc(Int,m/2)-3:trunc(Int,m/2)+3] .= h^2 * 1.0;\n\nA = zeros(Float64, m, m);\nA[1,1], A[m,m] = 1, 1\nfor i in 2:m-1\n    A[i, i-1] = -1\n    A[i,i] = 2\n    A[i, i+1] = -1\nend\n\nstruct domainType\n    R::Array{Int32}\nend\n\ndomain = Array{domainType, 1}(undef, 3)   # 3-element 1D array of domains\nM = zeros(Float64, m, m);\n\noffset = [0, ms[1], ms[1]+ms[2]];\nfor i in 1:3\n    domain[i] = domainType(zeros(Int32, ms[i], m)) # construct a new object of domainType\n    for j in 1:ms[i]\n        domain[i].R[j,offset[i]+j] = 1\n    end\n    global M += transpose(domain[i].R) * inv(domain[i].R*A*transpose(domain[i].R)) * domain[i].R;\nend\n\nusing LinearAlgebra: norm\nfor iter in 1:50\n    dU = M * (F-A*U)\n    global U += dU\n    println(norm(dU))\nend\n\nexact = A \\ F;\nusing Plots\nplot(1:m, exact, label = \"exact\")\nplot!(1:m, U, label=\"approximate\")\nsavefig(\"serial.png\")\n\n\nParallel additive Schwarz method\n\n\nImagine that now we are dealing with a very large problem, and we are breaking it into pieces, with each piece being processed by one worker. Now \\(U\\) and \\(F\\) will be 1D distributed arrays split between workers; we’ll implement them with distributed arrays.\nIdeally, we would like to partition domain into subdomains with DistributedArrays.jl, and then on each processor use a combination of sparse and Boolean (coded explicitly with indices) matrices to solve the problem. However, at this point DistributedArrays.jl does not seem to support distributed arrays of structures.\nAt the heart of our implementation is the distributed preconditioner matrix pre. DistributedArrays.jl does not seem to support an uneven distribution of an array across processes, and therefore we are limited to having an equal number of points in all subdomains.\n\\(A\\) is a sparse matrix. While Julia supports sparse matrices, instead we will code it algebraically with indices, so that we don’t have to worry about its distribution among processes.\n\\(R_i\\) is Boolean. The matrix \\(R_iAR_i^T\\) applies the domain restriction operator \\(R_i\\) to both rows and columns of \\(A\\), and the result is coded algebraically with the function fillPre(). It is then inverted locally in each process with invertPre().\n\\(R_i^T\\left(R_iAR_i^T\\right)^{-1}R_i\\) takes the result of this inversion and puts it as a dense block into the \\(m\\times m\\) ASM preconditioner \\(M^{-1}_\\textrm{ASM}\\). Each process computes its own dense block and stores it locally inside the distributed preconditioner matrix pre.\nNext, we start iterations. In computeUpdate() we compute \\(F-AU^n\\) as a 1D distributed array tmp, and multiply the preconditioner pre by tmp. Since pre is block-diagonal, this multiplication can be done separately in each process. Finally, we call addUpdate() to update the distributed solution U, again separately in each process.\nBig assumptions:\n\nAll distributed arrays are partitioned in exactly the same way, i.e. the same range of indices is assigned to each worker for 1D arrays U, tmp, dU, F.\nThe 2D array pre is partitioned purely along the second dimension (columns), i.e. each worker gets an \\(m_s\\times m_s\\) array, and the range of column indices on each worker is exactly the same as for the 1D arrays above.\n\nYou can force (2) by replacing the line\npre = dzeros(Float64, ms, m);\nwith the following block\nd1 = @spawnat 2 zeros(Float64, ms, ms);\nd2 = @spawnat 3 zeros(Float64, ms, ms);\nd3 = @spawnat 4 zeros(Float64, ms, ms);\npre = DArray([d1 d2 d3]);\nHere is the entire parallel code:\nN = 3    # number of domains and processes\nm = 21   # total number of points; must be a multiple of N\n@assert m%N == 0 \"m must be a multiple of N\"\nms = round(Int, m/N)   # the size of each subdomain\nh = 1.0 / (m-1)        # grid spacing\n\nusing Distributed\naddprocs(N)\n@everywhere using DistributedArrays\n\nU = dzeros(Float64, m, 1);     # 0th iteration; a distributed 2D array\ntmp = dzeros(Float64, m, 1);   # work area array\ndU = dzeros(Float64, m, 1);    # update array\n\nF = dzeros(Float64, m, 1);   # RHS; a distributed 2D array\n@everywhere function fillF(data,m,h)\n    rows = localindices(data)[1]\n    for iGlobal in rows\n        iLoc = iGlobal - rows.start + 1\n        if iGlobal >= trunc(Int,m/2)-3 && iGlobal <= trunc(Int,m/2)+3\n            data.localpart[iLoc] = h^2 * 1.0;\n        end\n    end\nend\nfor w in workers()\n    @spawnat w fillF(F, m, h)\nend\n\npre = dzeros(Float64, ms, m);\n\n@everywhere function fillPre(data, rank, ms, N)\n    if rank == 1\n        data.localpart[1,1] = 1\n        for iLoc in 2:ms   # main diagonal\n            data.localpart[iLoc,iLoc] = 2\n        end\n        for iLoc in 3:ms   # above main diagonal\n            data.localpart[iLoc-1,iLoc] = -1\n        end\n        for iLoc in 1:ms-1   # below main diagonal\n            data.localpart[iLoc+1,iLoc] = -1\n        end\n    end\n    if rank > 1 && rank < N\n        for iLoc in 1:ms   # main diagonal\n            data.localpart[iLoc,iLoc] = 2\n        end\n        for iLoc in 2:ms   # above main diagonal\n            data.localpart[iLoc-1,iLoc] = -1\n        end\n        for iLoc in 1:ms-1   # below main diagonal\n            data.localpart[iLoc+1,iLoc] = -1\n        end\n    end\n    if rank == N\n        data.localpart[ms,ms] = 1\n        for iLoc in 1:ms-1   # main diagonal\n            data.localpart[iLoc,iLoc] = 2\n        end\n        for iLoc in 2:ms   # above main diagonal\n            data.localpart[iLoc-1,iLoc] = -1\n        end\n        for iLoc in 1:ms-2   # below main diagonal\n            data.localpart[iLoc+1,iLoc] = -1\n        end\n    end\nend\nfor (rank,w) in enumerate(workers())\n    @spawnat w fillPre(pre, rank, ms, N)\nend\n\n@everywhere function invertPre(data)\n    data.localpart = inv(data.localpart)\n    # println(data.localpart)\nend\nfor w in workers()\n    @spawnat w invertPre(pre)\nend\n\n@everywhere function computeUpdate(data, F, U, ms, rank, N, tmp, dU)\n    # (1) compute tmp = (F-A*U)\n    if rank == 1\n        tmp.localpart[1] = F.localpart[1] - U[1]\n        # for rank==1 we always have iGlobal = iLoc\n        for iLoc in 2:ms\n            tmp.localpart[iLoc] = F.localpart[iLoc] + U[iLoc-1] - 2*U[iLoc] + U[iLoc+1] # last one has U[ms+1] => domains communicate\n        end\n    end\n    if rank > 1 && rank < N\n        iGlobal = (rank-1)*ms\n        for iLoc in 1:ms\n            iGlobal += 1\n            tmp.localpart[iLoc] = F.localpart[iLoc] + U[iGlobal-1] - 2*U[iGlobal] + U[iGlobal+1]\n        end\n    end\n    if rank == N\n        iGlobal = (rank-1)*ms\n        for iLoc in 1:ms-1\n            iGlobal += 1\n            tmp.localpart[iLoc] = F.localpart[iLoc] + U[iGlobal-1] - 2*U[iGlobal] + U[iGlobal+1]\n        end\n        tmp.localpart[ms] = F.localpart[ms] - U[rank*ms]\n    end\n    # (2) compute pre*tmp\n    dU.localpart = data.localpart*tmp.localpart\n    if rank == 1\n        println(norm(dU.localpart))\n    end\nend\n@everywhere function addUpdate(U, dU)\n    U.localpart += dU.localpart\nend\n@everywhere using LinearAlgebra: norm\n\nfor iter in 1:50\n    @sync for (rank,w) in enumerate(workers())\n        @spawnat w computeUpdate(pre, F, U, ms, rank, N, tmp, dU)\n    end\n    @sync for w in workers()\n        @spawnat w addUpdate(U, dU)\n    end\nend\n\nusing Plots\nUlocal = zeros(Float64, m, 1);\nUlocal .= U\nplot(1:m, Ulocal, label=\"approximate\")\nsavefig(\"parallel.png\")\nUlocal is necessary since the plot() function won’t take the distributed array U as an argument."
  },
  {
    "objectID": "julia/distributed-arrays.html",
    "href": "julia/distributed-arrays.html",
    "title": "DistributedArrays.jl",
    "section": "",
    "text": "Distributed arrays at a glance\nDistributedArrays package provides DArray object that can be split across several processes (set of workers), either on the same or multiple nodes. This allows use of arrays that are too large to fit in memory on one node. Each process operates on the part of the array that it owns – this provides a very natural way to achieve parallelism for large problems.\n\nEach worker can read any elements using their global indices\nEach worker can write only to the part that it owns \\(~\\Rightarrow~\\) automatic parallelism and safe execution\n\nDistributedArrays is not part of the standard library, so usually you need to install it yourself (it will typically write into ~/.julia/environments/versionNumber directory):\n] add DistributedArrays\nWe need to load DistributedArrays on every worker:\nusing Distributed\naddprocs(4)\n@everywhere using DistributedArrays\nn = 10\ndata = dzeros(Float32, n, n);          # distributed 2D array of 0's\ndata                                   # can access the entire array\ndata[1,1], data[n,5]                   # can use global indices\ndata.dims                              # global dimensions (10, 10)\ndata[1,1] = 1.0                        # error: cannot write from the control process!\n@spawnat 2 data.localpart[1,1] = 1.5   # success: can write locally\ndata\nLet’s check data distribution across workers:\nfor i in workers()\n    @spawnat i println(localindices(data))\nend\nrows, cols = @fetchfrom 3 localindices(data)\nprintln(rows)     # the rows owned by worker 3\nWe can only write into data from its “owner” workers using local indices on these workers:\n@everywhere function fillLocalBlock(data)\n    h, w = localindices(data)\n    for iGlobal in h                         # or collect(h)\n        iLoc = iGlobal - h.start + 1         # always starts from 1\n        for jGlobal in w                     # or collect(w)\n            jLoc = jGlobal - w.start + 1     # always starts from 1\n            data.localpart[iLoc,jLoc] = iGlobal + jGlobal\n        end\n    end\nend\nfor i in workers()\n    @spawnat i fillLocalBlock(data)\nend\ndata   # now the distributed array is filled\n@fetchfrom 3 data.localpart    # stored on worker 3\nminimum(data), maximum(data)   # parallel reduction\nOne-liners to generate distributed arrays:\na = dzeros(100,100,100);      # 100^3 distributed array of 0's\nb = dones(100,100,100);       # 100^3 distributed array of 1's\nc = drand(100,100,100);       # 100^3 uniform [0,1]\nd = drandn(100,100,100);      # 100^3 drawn from a Gaussian distribution\nd[1:10,1:10,1]\ne = dfill(1.5,100,100,100);   # 100^3 fixed value\nYou can find more information about the arguments by typing ?DArray. For example, you have a lot of control over the DArray’s distribution across workers. Before I show the examples, let’s define a convenient function to show the array’s distribution:\nfunction showDistribution(x::DArray)\n    for i in workers()\n        @spawnat i println(localindices(x))\n    end\nend\nnworkers()                                  # 4\ndata = dzeros((100,100), workers()[1:2]);   # define only on the first two workers\nshowDistribution(data)\nsquare = dzeros((100,100), workers()[1:4], [2,2]);   # 2x2 decomposition\nshowDistribution(square)\nslab = dzeros((100,100), workers()[1:4], [1,4]);   # 1x4 decomposition\nshowDistribution(slab)\nYou can take a local array and distribute it across workers:\ne = fill(1.5, (10,10))   # local array\nde = distribute(e)       # distribute `e` across all workers\nshowDistribution(de)\n\nExercise “DArrays.1”\nUsing either top or htop command on Uu, study memory usage with DistributedArrays. Are these arrays really distributed across processes? Use a largish array for this: large enough to spot memory usage, but not too large not to exceed physical memory and not to block other participants (especially if you do this on the login node).\n\n\n\nBuilding a distributed array from local pieces 11 This example was adapted from Ge Baolai’s presentation “Julia: A third perspective - parallel computing explained” (https://www.youtube.com/watch?v=HWLV6oTmfO8&t=2420s), Western University, SHARCNET, 2020.\nLet’s restart Julia with julia (single control process) and load the packages:\nusing Distributed\naddprocs(4)\nusing DistributedArrays       # important to load this after addprocs()\n@everywhere using LinearAlgebra\nWe will define an \\(8 \\times 8\\) matrix with the main diagonal and two off-diagonals (tridiagonal matrix). The lines show our matrix distribution across workers:\n\nNotice that with the 2x2 decomposition two of the 4 blocks are also tridiagonal matrices. We’ll define a function to initiate them:\n@everywhere function tridiagonal(n)\n    la = zeros(n,n)\n    la[diagind(la,0)] .= 2.     # diagind(la,k) provides indices of the kth diagonal of a matrix\n    la[diagind(la,1)] .= -1.    # below the main diagonal\n    la[diagind(la,-1)] .= -1.   # above the main diagonal\n    return la\nend\nWe also need functions to define the other two blocks:\n@everywhere function upperRight(n)\n    la = zeros(n,n)\n    la[n,1] = -1.\n    return la\nend\n@everywhere function lowerLeft(n)\n    la = zeros(n,n)\n    la[1,n] = -1.\n    return la\nend\nWe use these functions to define local pieces on each block and then create a distributed 8x8 matrix on a 2x2 process grid:\nd11 = @spawnat 2 tridiagonal(4)\nd21 = @spawnat 3 lowerLeft(4)\nd12 = @spawnat 4 upperRight(4)\nd22 = @spawnat 5 tridiagonal(4)\nd = DArray(reshape([d11 d21 d12 d22],(2,2)))   # create a distributed 8x8 matrix on a 2x2 process grid\nd\n\nExercise: “DArrays.2”\nRedefine showDistribution() on the control process and run showDistribution(d).\n\n\n\n\nAccessing distributed arrays\nWhile using distributed array saves memory and allows one to access the entire array in global address, it is tedious and sometimes challenging to do book keeping. For instance, setting values to a specific location in a distributed array is no easy job, as the DistributedArrays package only allows the process that owns the portion of the data to alter the values. Finding the boundary of the portion of data owned by each process is the first step towards setting values at the right locations within the boundaries.\nConsider two scenarios in which we are to write a slice of data to an one dimensional array A[istart:iend]:\n\nThe range istart:iend falls into the range of indices of data owned by a process;\nThe range istart:iend falls across two adjacent portions of data owned by two different processes.\n\nas depicted in the diagram below, the shaped areas represent the portion of the array A to be updated\n\nTo set the value of A[i], we need to use the method .localpart\nA.localpart[i-offset+1] = value\nwhere offset is the offset of the global index of the first element in the portion owned by the current worker process, i-offset+1 gives the local index relative to the first element in the local portion.\nWe wish this can be improved in the future, such that we can simply do\nA[i] = value\nBut the time being, this is the way it is.\nNow let’s have a look at how we to accomplish the task of setting a slice of data A[istart:iend]. Logically we need perform the following\n\nOn each worker, find the lower and upper indices ilo and iup, respectively, of the data portion owned by the worker process;\nFind the intersection iset of indices of range istart and iend and the set of indices of local portion of data ranging from ilo to `iup;\nFind the range of local indices of iset\n\nlstart = iset[1] - ilo + 1;\nlend = iset[2] - ilo + 1;\n\nSet the values with\n\nA.localpart[lstart:lend] = ...\nTo practice this, one may do the following exercise\n\nExercise: Setting a slice of values in a distributed array\nRun Julia with 4 worker processes. Create a distributed array u of length n=17, initialized with zeros. Set slice of u[7:11] to 1.0, as shown in the diagram below On may see the actual partitions by running the following commands\nfor p in workers()\n    @spawnat p println(localindices(u))\nend\nHint: The array u is partitioned into four subarrays. Perform the following operations on each worker. 1. For each subarray, find the lower and upper indices of the subarray ilo and iup; 2. For each subarray, find the start and end indices that fall in [7:11], that is, a subset of [7:11] in the partition. One may use the following operations\niset = intersect(Vector(7:11),Vector(ilo,iup));\n# If iset is empty, skip\nistart = iset[1];\niend = iset[end];\n\nSet the value 1.0 to the slices with local indices\n\nlstart = istart - ilo + 1;\nlend = iend - ilo + 1;\nu.localpart[lstart:lend] .= 1.0;\n\nThe operations need to be performed on each worker, unless you can locate the subarrays that contain the slice of indices [7:11] otherwise.\n\n\nThe same idea can be extended to multidimensional arrays.\n\nExercise: Setting values in a submatrix\nHomework: Write a function setval(a,va1,istart,iend,jstart,jend) that will set the value val in a distributed array a at a[istart:iend,jstart:jend].\n\n\n\nSolving 1D heat equation using distributed array22 This example is included in some of the SHARCNET training courses including Modern Fortran, Python, MPI and a few others.\nConsider a (simplified) physics problem: A rod of length \\([-L,L]\\) heated in the middle, then the heat source is removed. The temperature distribution \\(T(x,t)\\) across the rod over time can be simulated by the following equation\n\\[ T(x,t+t) = T(x-x,t) + T(x+x,t) \\]\non a set of evenly spaced points apart by \\(\\Delta x\\). The initial condition is shown in the diagram below. At \\(t=0\\), \\(T(x,0) = T_0\\) for \\(-h \\leq x \\leq h\\) and \\(T(x,0) = 0\\) elsewhere.\n\nAt both ends, we impose the boundary conditions \\(T(-L,t)=0\\) and \\(T(L,t)=0\\).\nThe solution of the temperature across the rod is a bell shaped curve being flattened over time. The figure below shows a snapshot of the solution (with \\(T_0 = 1\\)) on an interval \\([-1,1]\\) at certain time.\n\nThis example is used in many of our training courses, for example, MPI, Fortran and Python courses to illustrate parallel processing in advanced research computing. A more “accurate” formula involving three spatial points \\(x_i - \\Delta x\\), \\(x_i\\) and \\(x_i + \\Delta x\\) for computing the temperature at the next time step \\(t_n+\\Delta t\\) for interior points \\(i=1,\\ldots,N\\) is given below\n\\[ T(x_i,t_n+t) = (1-2k)T(x_i,t_n) + k((T(x_{i-1},t_n)+T(x_{i+1},t_n)) \\]\nwhere \\(k\\) is a parameter that shall be chosen properly in order for the numerical procedure to be stable.\nNote in the formula, the temperature \\(T(x_i,t_n+\\Delta t)\\) at the next time step \\(t_{n+1} = t_n +\\Delta t\\) can be computed explicitly using the values of \\(T\\) at three points at current time step \\(t_n\\), which are all known. This allows us to compute all grid values of \\(T\\) at time \\(t_n+\\Delta t\\) independent of each other, hence to achieve parallelism.\nLet \\(U_i^n\\) denote the value of \\(T(x_i,t_n)\\) at grid points \\(x_i\\), \\(i=1,\\ldots,N\\) at time \\(t_n\\), we use the short notation\n\\[ U_i^{n+1} = (1-2k)U_i^n + k(U_{i-1}^n + U_{i+1}^n) \\]\nfor \\(i=1,\\ldots,N\\). This can be translated into the following code with two one dimensional arrays unew[1:N] and u[1:N] holding values at the \\(N\\) grid points at \\(t_{n+1}\\) and \\(t_n\\) respectively33 Note that 2k is not a typo, it is a legal Julia expression, meaning 2*k.\nfor i=1:N\n    unew[i] = (1-2k)u[i] + k*(u[i-1] + u[i+1])\nend\nThis loop in fact can be replaced by the following one line of code using a single array u[1:N]\nu[2:N-1] = (1-2k)*u[2:N-1] + k*(u[1:N-2) + u[3:N])\nIn this case, vectorized operations on the right hand side take place first before the individual elements on the left hand side are updated.\nSerial code. A serial code is given below. The time evolution loop is at the end of the code.\nusing Plots, Base\n\n# Input parameters\na = 1.0\nn = 65          # Number of end points 1 to n (n-1 intervals).\ndt = -0.0005        # dt <= 0.5*dx^2/a, ignored if set negative\nk = 0.1\nnum_steps = 10000       # Number of stemps in t.\noutput_freq = 1     # Number of stemps per display.\nxlim = zeros(Float64,2)\nxlim[1] = -1.0\nxlim[2] = 1.0\nheat_range = zeros(Float64,2)\nheat_range[1] = -0.1\nheat_range[2] = 0.1\nheat_temp = 1.0\n\n# Set the k value\ndx = (xlim[2] - xlim[1])/(n-1)\nif (dt > 0)\n    k = a*dt/(dx*dx)\nend\n\n# Create space for u; create coordinates x for demo\nx = xlim[1] .+ dx*(collect(1:n) .-1);\nu = zeros(Float64,n);\n\n# Set initial condition\nix = findall(x->(heat_range[1] .< x .&& x .< heat_range[2]),x);\n@. u[ix] = heat_temp;\n\n# Display plot (it could be really slow on some systems to launch)\ndisplay(plot(x,u[1:n],lw=3,ylim=(0,1),label=(\"u\")))\n\n# Compute the solution over time\nfor j=1:num_steps\n    # Compute the solution for the next time step\n    u[2:n-1] = (1.0-2.0k)*u[2:n-1] + k*(u[1:n-2]+u[3:n])\n  \n    # Display the solution (comment it out for pro\n    if (j % output_freq == 0)\n        display(plot(x,u[1:n],lw=3,ylim=(0,1),label=(\"u\")))\n    end        \nend\nParallel code. To demonstrate the use of distributed arrays, we implement the parallel version of the code using a distributed array u to store the solution. Since we can’t write directly to a distributed array, the loop\nfor j=1:num_steps\n    # Compute the solution for the next time step\n    u[2:n-1] = (1.0-2.0k)*u[2:n-1] + k*(u[1:n-2]+u[3:n])\n  \n    # Display the solution (comment it out for pro\n    if (j % output_freq == 0)\n        display(plot(x,u[1:n],lw=3,ylim=(0,1),label=(\"u\")))\n    end        \nend\nneeds to be modified. Note the array u is distributed across workers. The grid is partitioned accordingly. The following diagram illustrates the partition of the grid and corresponding solution array u\n\nWe are going to modify the line\nu[2:n-1] = (1.0-2.0k)*u[2:n-1] + k*(u[1:n-2]+u[3:n])\nsuch that the computation is done on each worker locally.\nThe indices on the right hand side need to be replaced by the start and end indices on the current process. On the left hand side, as we’ve seen before, we need to use the function localindices(local_start, local_end) to replace the global index (which is a huge draw back with the current design of the package DistributedArrays)\nll1 = ... # Local start index\nlln = ... # Local end index\nu.localpart[ll1:lln] = (1.0-2.0*k)*u[l1:ln] + k*(u[l1-1:ln-1]+u[l1+1:ln+1])\nwhere .localpart is a method associated with the distributed array object that allows us to access and alter the values of the portion owned by the current worker process.\nTo avoid retrieving the start and end indices l1 and ln on each worker repeatedly during the time loop, we extract that information before the time loop using a function\n@everywhere function get_partition_info()\n    global u;\n    global l1, ln, ilo, iup;\n\n    # Get the lower and upper boundary indices from distributed array\n    idx = localindices(u);\n    index_range = idx[1];\n    ilo = index_range.start;\n    iup = index_range.stop;\n    l1 = ilo;\n    ln = iup;\n\n    # Local compute end indices (skip the left and right most end points)\n    me = myid() - 1\n    if (me == 1) \n        l1 = 2;\n    end\n    if (me == num_workers)\n        ln = iup - 1;\n    end\nend\n\nfor p in workers()\n    @async remotecall_fetch(get_partition_info, p)\nend\nNote we use the julia function localindices() to get the index range of the distributed array u. It returns a range, we then use the method .start and .stop to get the lower and upper indices ilo and iup, respectively, of the array owned by the current worker process. We adjust the start and end indices for the very first and very end of the sub-grid for skipping the boundary point there.\nWe also define a function update that computes the solution\n@everywhere function update()\n    global u, ilo, iup, l1, ln;\n    global k;\n\n    # Compute updated solution for the next time steop\n    # The line below does not work: method setting value not defined\n    #u[l1:ln] = (1.0-2.0*k)*u[l1:ln] + k*(u[l1-1:ln-1]+u[l1+1:ln+1])\n    ll1 = l1 - ilo + 1;\n    lln = ln - ilo + 1;\n    u.localpart[ll1:lln] = (1.0-2.0*k)*u[l1:ln] + k*(u[l1-1:ln-1]+u[l1+1:ln+1])\nend\nAnother tricky task we need to accomplish is setting initial condition in u. We need to locate the range of indices corresponding to the condition\n\\[ T(x,0) = 1,   -h x h. \\]\nWe’ve done such in the previous exercise. We leave it to the reader as an exercise. The skeleton of the code is shown below\nusing Base, Distributed, DistributedArrays\nusing Plots\n\n# Input parameters\n... ...\n\n# Set x-coordinates for plot\nx = xlim[1] .+ dx*(collect(1:n) .-1);\n\n# Allocate spaces\n@everywhere using DistributedArrays\nu = dzeros(Float64,n);\n\n# Broadcast parameters to all\n... ...\n\n# Define gloabl vars and functions on all worker processes\n@everywhere using Distributed, DistributedArrays\n@everywhere ilo=1\n@everywhere iup=1\n@everywhere l1=1\n@everywhere ln=1 # Local start and end indices\n\n# Get lower and upper indices of array u owned by this process\n@everywhere function get_partition_info()\n    ... ...\nend\n\n# Define the function to compute the soluton on a process\n@everywhere function update()\n    ... ...\nend\n\n# Set initial condition\nihot = findall(x->(heat_range[1] .< x .&& x .< heat_range[2]),x);\n@everywhere ihot=$ihot\n\n# Set the initial condition within this process\n@everywhere function set_init_cond()\n    ... ...\nend\n\n# Initialize variables and set initial conditions\nfor p in workers()\n    @async remotecall_fetch(get_partition_info,p)\nend\nfor p in workers()\n    @async remotecall_fetch(set_init_cond,p)\nend\n\n# Display the initial value of u\nv = zeros(Float64,n)\nv .= u;\ndisplay(plot(x,v,lw=3,ylim=(0,1),label=(\"u\")))\n\n# Update u in time on workers\nfor j=1:num_steps \n    @sync begin\n        for p in workers()\n            @async remotecall_fetch(update,p);\n        end\n    end           \n    if (j % output_freq == 0)\n        v .= u;\n        display(plot(x,v,lw=3,ylim=(0,1),label=(\"u\")))\n    end\nend\nA complete sample parallel can be found {{<a “/files/heat1d_darray.jl” “here”>}}.\nTechnical Points: This is a toy example for the demonstration of domain decomposition in one dimensional case. Because of the lack of enough computational work on each subdomain, the overhead due to the underlying data communications necessary between adjacent array elements may dominate the execution time, hence one may hardly observe any gain using multiple cores.\nIf one attempts to increase number of grid points to increase the amount of work, an unexpected numerical instability problem can occur. The explicit finite difference scheme suffers from a major drawback of instability. It can be shown theoretically that if \\(k \\ge 0.5\\) or for a given grid size \\(\\Delta t \\ge 0.5\\Delta x^2\\), the solution of \\(U_i^{n+1}\\) will quickly go divergent wildly. The following graph shows when \\(k=0.52\\), the solution starts to diverge after 20 steps.\n\nIn other words, there is a constraint on the sizes of time step and grid size. Implicit methods are preferred in practice. One may refer to works on the numerical solution of partial differential equations for details."
  },
  {
    "objectID": "julia/distributed-julia-set.html",
    "href": "julia/distributed-julia-set.html",
    "title": "Parallelizing Julia set",
    "section": "",
    "text": "The Julia set problem was described in one of the earlier sections.\n\nParallelizing\nHow would we parallelize this problem with multi-processing? We have a large array, so we can use DistributedArrays and compute it in parallel. Here are the steps:\n\nLoad Distributed on the control process.\nLoad DistributedArrays on all processes.\nstability array should be distributed:\n\nstability = dzeros(Int32, height, width);   # distributed 2D array of 0's\n\nDefine function pixel() on all processes.\nCreate fillLocalBlock(stability) to compute local pieces stability.localpart on each worker in parallel. If you don’t know where to start, begin with checking the complete example with fillLocalBlock() from the previous section. This function will cycle through all local indices localindices(stability). This function needs to be defined on all processes.\nReplace the loop\n\n@btime for i in 1:height, j in 1:width\n    point = (2*(j-0.5)/width-1) + (2*(i-0.5)/height-1)im\n    stability[i,j] = pixel(point)\nend\nwith\n@btime @sync for w in workers()\n    @spawnat w fillLocalBlock(stability)\nend\n\nWhy do we need @sync in the previous for block?\nTo the best of my knowledge, both Plots’ heatmap() and NetCDF’s ncwrite() are serial in Julia, and they cannot take distributed arrays. How do we convert a distributed array to a local array to pass to one of these functions?\nIs your parallel code faster?\n\n\n\nResults for 1000^2\nFinally, here are my timings on Uu:\n\n\n\n\n\n\n\n\nCode\nTime on login node (p-flavour vCPUs)\nTime on compute node (c-flavour vCPUs)\n\n\n\n\njulia juliaSetSerial.jl (serial runtime)\n147.214 ms\n123.195 ms\n\n\njulia -p 1 juliaSetDistributedArrays.jl (on 1 worker)\n157.043 ms\n128.601 ms\n\n\njulia -p 2 juliaSetDistributedArrays.jl (on 2 workers)\n80.198 ms\n66.449 ms\n\n\njulia -p 4 juliaSetDistributedArrays.jl (on 4 workers)\n42.965 ms\n66.849 ms\n\n\njulia -p 8 juliaSetDistributedArrays.jl (on 8 workers)\n36.067 ms\n67.644 ms\n\n\n\n\nLots of things here to discuss!\nOne could modify our parallel code to offload some computation to the control process (not just compute on workers as we do now), so that you would see speedup when running on 2 CPUs (control process + 1 worker)."
  },
  {
    "objectID": "julia/distributed1.html",
    "href": "julia/distributed1.html",
    "title": "Distributed.jl - part 1",
    "section": "",
    "text": "Parallelizing with multiple Unix processes (MPI tasks)\nJulia’s Distributed package provides multiprocessing environment to allow programs to run on multiple processors in shared or distributed memory. On each CPU core you start a separate Unix / MPI process, and these processes communicate via messages. Unlike in MPI, Julia’s implementation of message passing is one-sided, typically with higher-level operations like calls to user functions on a remote process.\n\na remote call is a request by one processor to call a function on another processor; returns a remote/future reference\nthe processor that made the call proceeds to its next operation while the remote call is computing, i.e. the call is non-blocking\nyou can obtain the remote result with fetch() or make the calling processor block with wait()\n\nIn this workflow you have a single control process + multiple worker processes. Processes pass information via messages underneath, not via variables in shared memory.\n\n\nLaunching worker processes\nThere are three different ways you can launch worker processes:\n\nwith a flag from bash:\n\njulia -p 8             # open REPL, start Julia control process + 8 worker processes\njulia -p 8 code.jl     # run the code with Julia control process + 8 worker processes\n\nfrom a job submission script:\n\n#!/bin/bash\n#SBATCH --ntasks=8\n#SBATCH --cpus-per-task=1\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\n#SBATCH --account=def-someuser\nsrun hostname -s > hostfile   # parallel I/O\nsleep 5\nmodule load julia/1.7.0\njulia --machine-file ./hostfile ./code.jl\n\nfrom the control process, after starting Julia as usual with julia:\n\nusing Distributed\naddprocs(8)\n\nNote: All three methods launch workers, so combining them will result in 16 (or 24!) workers (probably not the best idea). Select one method and use it.\n\nWith Slurm, methods (1) and (3) work very well, so—when working on a CC cluster—usually there is no need to construct a machine file.\n\n\nProcess control\nLet’s start an interactive MPI job:\nsource /project/def-sponsor00/shared/julia/config/loadJulia.sh\nsalloc --mem-per-cpu=3600M --time=01:00:00 --ntasks=4\nInside this job, start Julia with julia (single control process).\nusing Distributed\naddprocs(4)   # add 4 worker processes; this might take a while on uu\nprintln(\"number of cores = \", nprocs())       # 5 cores\nprintln(\"number of workers = \", nworkers())   # 4 workers\nworkers()                                     # list worker IDs\nYou can easily remove selected workers from the pool:\nrmprocs(2, 3, waitfor=0)   # remove processes 2 and 3 immediately\nworkers()\nor you can remove all of them:\nfor i in workers()     # cycle through all workers\n    t = rmprocs(i, waitfor=0)\n    wait(t)            # wait for this operation to finish\nend\nworkers()\ninterrupt()   # will do the same (remove all workers)\naddprocs(4)   # add 4 new worker processes (notice the new IDs!)\nworkers()\n\nDiscussion\nIf from the control process we start \\(N=8\\) workers, where will these processes run? Consider the following cases: 1. a laptop with 2 CPU cores, 1. a cluster login node with 16 CPU cores, 1. a cluster Slurm job with 4 CPU cores.\n\n\n\nRemote calls\nLet’s restart Julia with julia (single control process).\nusing Distributed\naddprocs(4)       # add 4 worker processes\nLet’s define a function on the control process and all workers and run it:\n@everywhere function showid()   # define the function everywhere\n    println(\"my id = \", myid())\nend\nshowid()                        # run the function on the control process\n@everywhere showid()            # run the function on the control process + all workers\n@everywhere does not capture any local variables (unlike @spawnat that we’ll study below), so on workers we don’t see any variables from the control process:\nx = 5     # local (control process only)\n@everywhere println(x)    # get errors: x is not defined elsewhere\nHowever, you can still obtain the value of x from the control process by using this syntax:\n@everywhere println($x)   # use the value of `x` from the control process\nThe macro that we’ll use a lot today is @spawnat. If we type:\na = 12\n@spawnat 2 println(a)     # will print 12 from worker 2\nit will do the following:\n\npass the namespace of local variables to worker 2\nspawn function execution on worker 2\nreturn a Future handle (referencing this running instance) to the control process\nreturn the REPL to the control process (while the function is running on worker 2), so we can continue running commands\n\nNow let’s modify our code slightly:\na = 12\n@spawnat 2 a+10          # Future returned but no visible calculation\nThere is no visible calculation happening; we need to fetch the result from the remote function before we can print it:\nr = @spawnat 2 a+10\ntypeof(r)\nfetch(r)                 # get the result from the remote function; this will pause\n                         #         the control process until the result is returned\nYou can combine both @spawnat and fetch() in one line:\nfetch(@spawnat 2 a+10)   # combine both in one line; the control process will pause\n@fetchfrom 2 a+10        # shorter notation; exactly the same as the previous command\n\nExercise “Distributed.1”\nTry to define and run a function on one of the workers, e.g.\nfunction cube(x)\n    return x*x*x\nend\nHint: Use @everywhere to define the function on all workers. Julia may not have a high-level mechanism to define a function on a specific worker, short of loading that function as a module from a file. Something like this\n@fetchfrom 2 function cube(x)\n    return x*x*x\nend\ndoes not seem to have any effect.\n\n\nExercise “Distributed.2”\nNow run the same function on all workers, but not on the control process. Hint: use workers() to cycle through all worker processes and println() to print from each worker.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can also spawn computation on any available worker:\nr = @spawnat :any log10(a)   # start running on one of the workers\nfetch(r)\n\n\nBack to the slow series: serial code\nLet’s restart Julia with julia -p 2 (control process + 2 workers). We’ll start with our serial code (below), save it as serialDistributed.jl, and run it.\nusing Distributed\nusing BenchmarkTools\n\n@everywhere function digitsin(digits::Int, num)\n    base = 10\n    while (digits ÷ base > 0)\n        base *= 10\n    end\n    while num > 0\n        if (num % base) == digits\n            return true\n        end\n        num ÷= 10\n    end\n    return false\nend\n\n@everywhere function slow(n::Int64, digits::Int)\n    total = Int64(0)\n    for i in 1:n\n        if !digitsin(digits, i)\n            total += 1.0 / i\n        end\n    end\n    return total\nend\n\n@btime slow(Int64(1e8), 9)     # serial run: total = 13.277605949858103\nFor me this serial run takes 3.192 s on uu. Next, let’s run it on 3 (control + 2 workers) cores simultaneously:\n@everywhere using BenchmarkTools\n@everywhere @btime slow(Int64(1e8), 9)   # runs on 3 (control + 2 workers) cores simultaneously\nHere we are being silly: this code is serial, so each core performs the same calculation … I see the following times printed on my screen: 3.220 s, 2.927 s, 3.211 s—each is from a separate process running the code in a serial fashion.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can we make this code parallel and faster?\n\n\nParallelizing our slow series: non-scalable version\nLet’s restart Julia with julia (single control process) and add 2 worker processes:\nusing Distributed\naddprocs(2)\nworkers()\nWe need to redefine digitsin() everywhere, and then let’s modify slow() to compute a partial sum:\n@everywhere function slow(n::Int, digits::Int, taskid, ntasks)   # two additional arguments\n    println(\"running on worker \", myid())\n    total = 0.0\n    @time for i in taskid:ntasks:n   # partial sum with a stride `ntasks`\n        if !digitsin(digits, i)\n            total += 1.0 / i\n        end\n    end\n    return(total)\nend\nNow we can actually use it:\n# slow(Int64(10), 9, 1, 2)   # precompile the code\nprecompile(slow, (Int, Int, Int, Int))\na = @spawnat :any slow(Int64(1e8), 9, 1, 2)\nb = @spawnat :any slow(Int64(1e8), 9, 2, 2)\nprint(\"total = \", fetch(a) + fetch(b))   # 13.277605949852546\nFor timing I got 1.30 s and 1.66 s, running concurrently, which is a 2X speedup compared to the serial run—this is great! Notice that we received a slightly different numerical result, due to a different order of summation.\nHowever, our code is not scalable: it is limited to a small number of sums each spawned with its own Future reference. If we want to scale it to 100 workers, we’ll have a problem.\nHow do we solve this problem—any idea before I show the solution in the next section?"
  },
  {
    "objectID": "julia/distributed2.html",
    "href": "julia/distributed2.html",
    "title": "Distributed.jl - part 2",
    "section": "",
    "text": "Solution 1: an array of Future references\nWe could create an array (using array comprehension) of Future references and then add up their respective results. An array comprehension is similar to Python’s list comprehension:\na = [i for i in 1:5];   # array comprehension in Julia\ntypeof(a)               # 1D array of Int64\nWe can cycle through all available workers:\n[w for w in workers()]                      # array of worker IDs\n[(i,w) for (i,w) in enumerate(workers())]   # array of tuples (counter, worker ID)\n\nExercise “Distributed.3”\nUsing this syntax, construct an array r of Futures, and then get their results and sum them up with\nprint(\"total = \", sum([fetch(r[i]) for i in 1:nworkers()]))\nYou can do this exercise using either the array comprehension from above, or the good old for loops.\n\n\n\n\n\n\nWith two workers and two CPU cores, we should get times very similar to the last run. However, now our code can scale to much larger numbers of cores!\n\nExercise “Distributed.4”\nIf you did the previous exercise with an interactive job, now submit a Slurm batch job running the same code on 4 CPU cores. Next, try 8 cores. Did your timing change?\n\n\n\nSolution 2: parallel for loop with summation reduction\nUnlike the Base.Threads module, Distributed provides a parallel loop with reduction. This means that we can implement a parallel loop for computing the sum. Let’s write parallelFor.jl with this version of the function:\nfunction slow(n::Int64, digits::Int)\n    @time total = @distributed (+) for i in 1:n\n        !digitsin(digits, i) ? 1.0 / i : 0\n    end\n    println(\"total = \", total);\nend\nA couple of important points:\n\nWe don’t need @everywhere to define this function. It is a parallel function defined on the control process, and running on the control process.\nThe only expression inside the loop is the compact if/else statement. Consider this:\n\n1==2 ? println(\"Yes\") : println(\"No\")\nThe outcome of the if/else statement is added to the partial sums at each loop iteration, and all these partial sums are added together.\nNow let’s measure the times:\n# slow(10, 9)\nprecompile(slow, (Int, Int))\nslow(Int64(1e8), 9)   # total = 13.277605949855722\n\nExercise “Distributed.5”\nSwitch from using @time to using @btime in this code. What changes did you have to make?\n\n\n\n\nThis will produce the single time for the entire parallel loop (1.498s in my case).\n\nExercise “Distributed.6”\nRepeat on 8 CPU cores. Did your timing improve?\n\nI tested this code (parallelFor.jl) on Cedar with v1.5.2 and n=Int64(1e9):\n#!/bin/bash\n#SBATCH --ntasks=...   # number of MPI tasks\n#SBATCH --cpus-per-task=1\n#SBATCH --nodes=1-1   # change process distribution across nodes\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=0:5:0\n#SBATCH --account=...\nmodule load julia\necho $SLURM_NODELIST\n# comment out addprocs() in the code\njulia -p $SLURM_NTASKS parallelFor.jl\n\n\n\nCode\nTime\n\n\n\n\nserial\n48.2s\n\n\n4 cores, same node\n12.2s\n\n\n8 cores, same node\n7.6s\n\n\n16 cores, same node\n6.8s\n\n\n32 cores, same node\n2.0s\n\n\n32 cores across 6 nodes\n11.3s\n\n\n\n\n\nSolution 3: use pmap to map arguments to processes\nLet’s write mappingArguments.jl with a new version of slow() that will compute partial sum on each worker:\n@everywhere function slow((n, digits, taskid, ntasks))   # the argument is now a tuple\n    println(\"running on worker \", myid())\n    total = 0.0\n    @time for i in taskid:ntasks:n   # partial sum with a stride `ntasks`\n        !digitsin(digits, i) && (total += 1.0 / i)   # compact if statement\n    end\n    return(total)\nend\nand launch the function on each worker:\nslow((10, 9, 1, 1))   # package arguments in a tuple\nnw = nworkers()\nargs = [(Int64(1e8), 9, j, nw) for j in 1:nw]   # array of tuples to be mapped to workers\nprintln(\"total = \", sum(pmap(slow, args)))   # launch the function on each worker and sum the results\nThese two syntaxes are equivalent:\nsum(pmap(slow, args))\nsum(pmap(x->slow(x), args))\nWe see the following times from individual processes:\nFrom worker 2:  running on worker 2\nFrom worker 3:  running on worker 3\nFrom worker 4:  running on worker 4\nFrom worker 5:  running on worker 5\nFrom worker 2:    0.617099 seconds\nFrom worker 3:    0.619604 seconds\nFrom worker 4:    0.656923 seconds\nFrom worker 5:    0.675806 seconds\ntotal = 13.277605949854518\n\n\nHybrid parallelism\nHere is a simple example of a hybrid multi-threaded / multi-processing code contributed by Xavier Vasseur following the October 2021 workshop:\nusing Distributed\n@everywhere using Base.Threads\n\n@everywhere function greetings_from_task(())\n    @threads for i=1:nthreads()\n    println(\"Hello from thread $(threadid()) on proc $(myid())\")\n    end\nend\n\nargs_pmap  = [() for j in workers()];\npmap(x->greetings_from_task(x), args_pmap)\nSave this code as hybrid.jl and then run it specifying the number of workers with -p and the number of threads per worker with -t flags:\n[~/tmp]$ julia -p 4 -t 2 hybrid.jl \n      From worker 5:    Hello, I am thread 2 on proc 5\n      From worker 5:    Hello, I am thread 1 on proc 5\n      From worker 2:    Hello, I am thread 1 on proc 2\n      From worker 2:    Hello, I am thread 2 on proc 2\n      From worker 3:    Hello, I am thread 1 on proc 3\n      From worker 3:    Hello, I am thread 2 on proc 3\n      From worker 4:    Hello, I am thread 1 on proc 4\n      From worker 4:    Hello, I am thread 2 on proc 4\n\n\nOptional integration with Slurm\nClusterManagers.jl package lets you submit Slurm jobs from your Julia code. This way you can avoid writing a separate Slurm script in bash, and put everything (Slurm submission + parallel launcher + computations) into a single code. Moreover, you can use Julia as a language for writing complex HPC workflows, i.e. write your own distributed worflow manager for the cluster.\nHowever, for the types of workflows we consider in this workshop ClusterManagers.jl is an overkill, and we don’t recommend it for beginner Julia users."
  },
  {
    "objectID": "julia/index.html",
    "href": "julia/index.html",
    "title": "Parallel Julia",
    "section": "",
    "text": "Date:\nWednesday May 3, 2023\nTime:\n9am–5pm (with a two-hour break from noon to 2pm)\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:  Basic working knowledge of HPC (how to submit Slurm jobs and view their output). Some scientific programming experience (in any language) would be ideal, but we will start slowly explaining basic principles along the way. No previous parallel programming experience needed.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there). You don’t need to install Julia on your computer, unless you want to."
  },
  {
    "objectID": "julia/intro-language.html",
    "href": "julia/intro-language.html",
    "title": "Introduction to the Julia language",
    "section": "",
    "text": "High-performance, dynamically typed programming language for scientific computing\nUses just-in-time (JIT) compiler to compile all code, includes an interactive command line (REPL = read–eval–print loop, and can also be run in Jupyter), i.e. tries to combine the advantages of both compiled and interpreted languages\nBuilt-in package manager\nLots of interesting design decisions, e.g. macros, support for Unicode, etc. — covered in our introductory Julia course\nSupport for parallel and distributed computing via its Standard Library and many 3rd party packages\n\nbeing added along the way, e.g. @threads were first introduced in v0.5\ncurrently under active development, both in features and performance"
  },
  {
    "objectID": "julia/intro-language.html#running-julia-locally",
    "href": "julia/intro-language.html#running-julia-locally",
    "title": "Introduction to the Julia language",
    "section": "Running Julia locally",
    "text": "Running Julia locally\nIf you have Julia installed on your own computer, you can run it there: on a multi-core laptop/desktop you can launch multiple threads and processes and run them in parallel.\nIf you would like to install Julia later on, you can find some information here."
  },
  {
    "objectID": "julia/intro-language.html#using-julia-on-supercomputers",
    "href": "julia/intro-language.html#using-julia-on-supercomputers",
    "title": "Introduction to the Julia language",
    "section": "Using Julia on supercomputers",
    "text": "Using Julia on supercomputers\n\nJulia on Compute Canada production clusters\nJulia is among hundreds of software packages installed on the CC clusters. To use Julia on one of them, you would load the following module:\n$ module load julia\n\nInstalling Julia packages on a production cluster\nBy default, all Julia packages you install from REPL will go into $HOME/.julia. If you want to put packages into another location, you will need to (1) install inside your Julia session with (from within Julia):\nempty!(DEPOT_PATH)\npush!(DEPOT_PATH,\"/scratch/path/to/julia/packages\") \n] add BenchmarkTools\nand (2) before running Julia modify two variables (from the command line):\n$ module load julia\n$ export JULIA_DEPOT_PATH=/home/\\$USER/.julia:/scratch/path/to/julia/packages\n$ export JULIA_LOAD_PATH=@:@v#.#:@stdlib:/scratch/path/to/julia/packages\nDon’t do this on the training cluster! We already have everything installed in a central location for all guest accounts.\n\nSome Julia packages rely on precompiled bits that developers think would work on all architectures, but they don’t. For example, Plots package comes with several precompiled libraries, it installs without problem on Compute Canada clusters, but then at runtime you will see an error about “GLIBC_2.18 not found”. The proper solution would be to recompile the package on the cluster, but it is not done correctly in Julia packaging, and the error persists even after “recompilation”. There is a solution for this, and you can always contact us at support@computecanada.ca and ask for help. Another example if Julia’s NetCDF package: it installs fine on Apple Silicon Macs, but it actually comes with a precompiled C package that was compiled only for Intel Macs and does not work on M1.\n\n\n\n\nJulia on the training cluster for this workshop\nWe have Julia on our training cluster uu.c3.ca.\n\nOur training cluster has:\n\none login node with 16 “persistent” cores and 32GB of memory,\n17 compute nodes with 16 “compute” cores and 60GB of memory (272 cores in total),\none GPU node with 4 “compute” cores, 1 vGPU (8GB) and 22GB of memory.\n\n\nIn our introductory Julia course we use Julia inside a Jupyter notebook. Today we will be starting multiple threads and processes, with the eventual goal of running our workflows as batch jobs on an HPC cluster, so we’ll be using Julia from the command line.\n\nPause: We will now distribute accounts and passwords to connect to the cluster.\n\nNormally, you would install Julia packages yourself. A typical package installation however takes several hundred MBs of RAM, a fairly long time, and creates many small files. Our training cluster runs on top of virtualized hardware with a shared filesystem. If several dozen workshop participants start installing packages at the same time, this will hammer the filesystem and will make it slow for all participants for quite a while.\nInstead, for this workshop, you will run:\n$ source /project/def-sponsor00/shared/julia/config/loadJulia.sh\nThis script loads the Julia module and sets environment variables to point to a central environment in which we have pre-installed all the packages for this workshop.\n\nNote that you can still install additional packages if you want. These will install in your own environment at ~/.julia."
  },
  {
    "objectID": "julia/intro-language.html#running-julia-in-the-repl",
    "href": "julia/intro-language.html#running-julia-in-the-repl",
    "title": "Introduction to the Julia language",
    "section": "Running Julia in the REPL",
    "text": "Running Julia in the REPL\n\nWhere to run the REPL\nYou could now technically launch a Julia REPL (Read-Eval-Print-Loop). However, this would launch it on the login node and if everybody does this at the same time, we would probably stall our training cluster.\nInstead, you will first launch an interactive job by running the Slurm command salloc:\n$ salloc --mem-per-cpu=3600M --time=01:00:00\nThis puts you on a compute node for up to one hour.\nNow you can launch the Julia REPL and try to run a couple of commands:\n$ julia\n_\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.7.0 (2021-11-30)\n _/ |\\__'_|_|_|\\__'_|  |  \n|__/                   |\n\njulia> using BenchmarkTools\n\njulia> @btime sqrt(2)\n  1.825 ns (0 allocations: 0 bytes)\n1.4142135623730951\n\n\nREPL modes\nThe Julia REPL has 4 modes:\n\n(env is the name of your current project environment.)\n\n\nREPL keybindings\nIn the REPL, you can use standard command line (Emacs) keybindings:\nC-c     cancel\nC-d     quit\nC-l     clear console\n\nC-u     kill from the start of line\nC-k     kill until the end of line\n\nC-a     go to start of line\nC-e     go to end of line\n\nC-f     move forward one character\nC-b     move backward one character\n\nM-f     move forward one word\nM-b     move backward one word\n\nC-d     delete forward one character\nC-h     delete backward one character\n\nM-d     delete forward one word\nM-Backspace delete backward one word\n\nC-p     previous command\nC-n     next command\n\nC-r     backward search\nC-s     forward search\n\n\nREPL for parallel work\nRemember our workflow to launch a Julia REPL:\n# Run our script to load the Julia module\n# and set our special environment with pre-installed packages\n$ source /project/def-sponsor00/shared/julia/config/loadJulia.sh\n\n# Launch an interactive job on a compute node for one hour\n$ salloc --mem-per-cpu=3600M --time=01:00:00\n\n# Launch the Julia REPL\n$ julia\nThis is great to run serial work.\nWhen we will run parallel work however, we will want to use multiple CPUs per task in order to see a speedup.\nSo instead, you will run:\n$ source /project/def-sponsor00/shared/julia/config/loadJulia.sh\n\n# Request 2 CPUs per task from Slurm\n$ salloc --mem-per-cpu=3600M --cpus-per-task=2 --time=01:00:00\n\n# Launch Julia on 2 threads\n$ julia -t 2"
  },
  {
    "objectID": "julia/intro-language.html#running-scripts",
    "href": "julia/intro-language.html#running-scripts",
    "title": "Introduction to the Julia language",
    "section": "Running scripts",
    "text": "Running scripts\nNow, if we want to get an even bigger speedup, we could use even more CPUs per task. The problem is that our cluster only has around 300 CPUs. So some of us would be left waiting for Slurm while the others can play with a bunch of CPUs for an hour.\nThis is not an efficient approach. This is equally true on production clusters: if you want to run an interactive job using a lot of resources, you might have to wait for a long time.\nA much better approach in this case is to put our Julia code in a Julia script and run it through a batch job by using the Slurm function sbatch.\nYou can run a Julia script with julia julia_script.jl.\nSo all we need to do is to submit a shell script to sbatch that contains information for Slurm and the code to run (julia julia_script.jl).\n\nExample:\n\nWe can save into job_script.sh:\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\n\njulia -t 8 julia_script.jl\nThen we run our job script:\n$ sbatch job_script.sh"
  },
  {
    "objectID": "julia/intro-language.html#serial-julia-features-worth-noting-in-10-mins",
    "href": "julia/intro-language.html#serial-julia-features-worth-noting-in-10-mins",
    "title": "Introduction to the Julia language",
    "section": "Serial Julia features worth noting in 10 mins",
    "text": "Serial Julia features worth noting in 10 mins\n\nJIT compilation\nProgramming languages are either interpreted or compiled.\nInterpreted languages use interpreters: programs that execute your code directly (Python, for instance, uses the interpreter CPython, written in C). Interpreted languages are very convenient since you can run sections of your code as soon as you write them. However, they are slow.\nCompiled languages use compilers: programs that translate your code into machine code. Machine code is extremely efficient, but of course, having to compile your code before being able to run it makes for less convenient workflows when it comes to writing or debugging code.\nJulia uses just-in-time compilation or JIT based on LLVM: the source code is compiled at run time. This combines the flexibility of interpretation with the speed of compilation, bringing speed to an interactive language. It also allows for dynamic recompilation, continuous weighing of gains and costs of the compilation of parts of the code, and other on the fly optimizations.\nHere is a great blog post covering this topic if you want to dive deeper into the functioning of JIT compilers.\n\n\nMacros\nIn the tradition of Lisp, Julia has strong metaprogramming capabilities, in particular in the form of macros.\nMacros are parsed and evaluated first, then their output gets evaluated like a classic expression. This allows the language to modify itself in a reflective manner.\nMacros have a @ prefix and are defined with a syntax similar to that of functions.\n@time for instance is a macro that executes an expression and prints the execution time and other information.\n\n\nFun fact\nJulia supports unicode. In a Julia REPL, type the following, followed by the TAB key:\n\\:snail:\nand you get:\n🐌\nWhile assigning values to a “snail variable” might not be all that useful, a wide access to—for instance—Greek letters, makes Julia’s code look nicely similar to the equations it represents. For instance, if you type TAB after each variable name, the following:\n\\pgamma = ((\\alpha \\pi + \\beta) / \\delta) + \\upepsilon\nlooks like:\nɣ = ((α π + β) / δ) + ε\n\n\nAdditional basic information\nOur introduction to Julia course has, amongst others, sections on:\n\nhow to access help/documentation and links to a lot of resources\npackages\ncollections\nbasic functions\ncontrol flow"
  },
  {
    "objectID": "julia/intro-parallel.html",
    "href": "julia/intro-parallel.html",
    "title": "Parallel Julia",
    "section": "",
    "text": "In Unix a process is the smallest independent unit of processing, with its own memory space – think of an instance of a running application. The operating system tries its best to isolate processes so that a problem with one process doesn’t corrupt or cause havoc with another process. Context switching between processes is relatively expensive.\nA process can contain multiple threads, each running on its own CPU core (parallel execution), or sharing CPU cores if there are too few CPU cores relative to the number of threads (parallel + concurrent execution). All threads in a Unix process share the virtual memory address space of that process, e.g. several threads can update the same variable, whether it is safe to do so or not (we’ll talk about thread-safe programming in this course). Context switching between threads of the same process is less expensive.\n\n\n\n\n\nAlt text here\n\n\n\nThreads within a process communicate via shared memory, so multi-threading is always limited to shared memory within one node.\nProcesses communicate via messages (over the cluster interconnect or via shared memory). Multi-processing can be in shared memory (one node, multiple CPU cores) or distributed memory (multiple cluster nodes). With multi-processing there is no scaling limitation, but traditionally it has been more difficult to write code for distributed-memory systems. Julia tries to simplify it with high-level abstractions.\n\nIn Julia you can parallelize your code with multiple threads, or with multiple processes, or both (hybrid parallelization).\n\n\nWhat are the benefits of each: threads vs. processes? Consider (1) context switching, e.g. starting and terminating or concurrent execution, (2) communication, (3) scaling up."
  },
  {
    "objectID": "julia/intro-parallel.html#parallelism-in-julia",
    "href": "julia/intro-parallel.html#parallelism-in-julia",
    "title": "Parallel Julia",
    "section": "Parallelism in Julia",
    "text": "Parallelism in Julia\nThe main goal of this course is to teach you the basic tools for parallel programming in Julia, targeting both multi-core PCs and distributed-memory clusters. We will cover the following topics:\n\nmulti-threading with Base.Threads and ThreadsX.jl\nmulti-processing with Distributed.jl\nClusterManagers.jl (very briefly)\nDistributedArrays.jl—distributing large arrays across multiple processes\nSharedArrays.jl—shared-memory access to large arrays from multiple processes\n\nWe will not be covering the following topics today (although we hope to cover them in our future webinars!):\n\nMPI.jl—a port of the standard MPI library to Julia\nMPIArrays.jl\nLoopVectorization.jl\nFLoops.jl\nTransducers.jl\nDagger.jl—a task graph scheduler heavily inspired by Python’s Dask\nDistributedData.jl\nGPU-related packages"
  },
  {
    "objectID": "julia/nbody.html",
    "href": "julia/nbody.html",
    "title": "Parallelizing N-body",
    "section": "",
    "text": "In this section I will describe a project that you can work on this afternoon: the direct N-body solver.\nImagine that you place \\(N\\) identical particles randomly into a unit cube, with zero initial velocities. Then you turn on gravity, so that the particles start attracting each other. There are no boundary conditions: these are the only particles in the Universe, and they can fly to \\(\\infty\\).\n\nQuestion\nWhat do you expect these particles will do?\n\nWe will adopt the following numerical method:\n\nforce evaluation via direct summation\nsingle variable (adaptive) time step: smaller \\(\\Delta t\\) when any two particles are close\ntime integration: more accurate than simple forward Euler + one force evaluation per time step\ntwo parameters: softening length and Courant number (I will explain these when we study the code)\n\nIn a real simulation, you would replace:\n\ndirect summation with a tree- or mesh-based \\(O(N\\log N)\\) code\ncurrent integrator with a higher-order scheme, e.g. Runge-Kutta\ncurrent timestepping with hierarchical particle updates\nfor long-term stable evolution with a small number of particles, use a symplectic orbit integrator\n\nExpected solutions:\n\n2 particles: should pass through each other, infinite oscillations\n3 particles: likely form a close binary + distant 3\\(^{\\rm rd}\\) particle (hierarchical triple system)\nmany particles: likely form a gravitationally bound system, with occasional ejection\n\nIn these clips below the time arrow is not physical time but the time step number. Consequently, the animations slow down when any two particles come close to each other.\n\n \n\nBelow you will find the serial code nbodySerial.jl. I removed all parts related to plotting the results, as it’s slow in Julia, and you would need to install Plots package (takes a while with many dependencies!).\nusing ProgressMeter\n\nnpart = 20\nniter = Int(1e5)\ncourant = 1e-3\nsofteningLength = 0.01\n\nx = rand(npart, 3);   # uniform [0,1]\nv = zeros(npart, 3);\n\nsoft = softeningLength^2;\n\nprintln(\"Computing ...\");\nforce = zeros(Float32, npart, 3);\noldforce = zeros(Float32, npart, 3);\n@showprogress for iter = 1:niter\n    tmin = 1.e10\n    for i = 1:npart\n        force[i,:] .= 0.\n        for j = 1:npart\n            if i != j\n                distSquared = sum((x[i,:] .- x[j,:]).^2) + soft;\n                force[i,:] -= (x[i, :] .- x[j,:]) / distSquared^1.5;\n                tmin = min(tmin, sqrt(distSquared / sum((v[i,:] .- v[j,:]).^2)));\n            end\n        end\n    end\n    dt = min(tmin*courant, 0.001);   # limit the initial step\n    for i = 1:npart\n        x[i,:] .+= v[i,:] .* dt .+ 0.5 .* oldforce[i,:] .* dt^2;\n        v[i,:] .+= 0.5 .* (oldforce[i,:] .+ force[i,:]) .* dt;\n        oldforce[i,:] .= force[i,:];\n    end\nend\nTring running this code with julia nbodySerial.jl; the main loop takes ~6m on Uu. Obvisoulty, the most CPU-intensive part is force evaluation – this is what you want to accelerate.\nThere are many small arrays in the code – let’s use SharedArrays and fill them in parallel, e.g. you would replace\nforce = zeros(Float32, npart, 3);\nwith\nforce = SharedArray{Float32}(npart,3);\nWhen updating shared arrays, you have a choice: either update array[localindices(array)] on each worker, or use a parallel for loop with reduction. I suggest the latter. What do you want to reduce? Hint: what else do you compute besides the force in that loop? For code syntax, check parallelFor.jl code in this earlier section.\n\n\n\n\nResults\nWith default 20 particles and \\(10^5\\) steps the code runs slower in parallel on Uu:\n\n\n\nCode\nTime\n\n\n\n\njulia nbodySerial.jl (serial runtime)\n340s\n\n\njulia -p 1 nbodyDistributedShared.jl (2 processes)\n358s\n\n\n\nThis is the same problem we discussed in the Chapel course: with a fine-grained parallel code the communication overhead dominates the problem. As we increase the problem size, we should see the benefit from parallelization. E.g. with 1000 particles and 10 steps:\n\n\n\nCode\nTime\n\n\n\n\njulia nbodySerial.jl (serial runtime)\n83.7s\n\n\njulia -p 1 nbodyDistributedShared.jl (2 processes)\n47.9s\n\n\n\nHere is what I got on Cedar with 100 particles and \\(10^3\\) steps:\n\n\n\nRun\nTime\n\n\n\n\nserial\n1m23s\n\n\n2 cores\n46s\n\n\n4 cores\n29s\n\n\n8 cores\n22s\n\n\n16 cores\n18s\n\n\n32 cores\n19s\n\n\n\n\n\nLinks\n\n“Julia at Scale” forum\nBaolai Ge’s (SHARCNET) webinar “Julia: Parallel computing revisited”\nWestGrid’s March 2021 webinar “Parallel programming in Julia”\nJulia performance tips\n“Think Julia: How to Think Like a Computer Scientist” by Ben Lauwens and Allen Downey is a good introduction to basic Julia for beginners"
  },
  {
    "objectID": "julia/shared-arrays.html",
    "href": "julia/shared-arrays.html",
    "title": "SharedArrays.jl",
    "section": "",
    "text": "Local vs shared arrays in Julia\nLet’s reiterate this concept in Julia. Any variables created in the control process are only accessible on the control process. In order to make the content stored in a variable accessible by another process, we either need to copy it to the other process or use a shared variable.\nIn the following example\nn = 10\na = zeros(n)\n@distributed for i=1:n\n    a[i] = i\nend\nwe attempt to assign values to array a concurrently by distributing the task in the loop to workers randomly (determined by Julia). But this is not going to happen\nprintln(a)\n[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nThis is because, the distributed assignments took place on workers, not on the control process. Let’s see what’s on workers\n@everywhere function show()\n    println(a)\nend\nfor p in workers()\n    remotecall_fetch(show,p)\nend\nThe output might surprise us\n      From worker 2:    [1.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n      From worker 3:    [0.0, 0.0, 0.0, 4.0, 5.0, 6.0, 0.0, 0.0, 0.0, 0.0]\n      From worker 4:    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 8.0, 0.0, 0.0]\n      From worker 5:    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9.0, 10.0]\nTwo things worth noting here. First, a gets copied on the workers, otherwise, the workers will have nothing to work on as they don’t have access to variables defined on the control process. Second, the result shows, each worker does only a portion of the work, thanks to the rule of @distributed. As a result, workers have different “images” of the variable a afterwards.\nWith package SharedArrays, a shared array of type SharedArray will have a universal view across all process. The following example illustrates the effect of using shared arrays\nusing SharedArrays\nn = 10\na = SharedArray{Float64}(n)\n@distributed for i=1:n\n    a[i] = i\nend\n\n@everywhere SharedArrays\n@everywhere showa()\n    println(a)\nend\nfor p in workers()\n    remotecall_fetch(showa,p)\nend\nThis is the output\n      From worker 2:    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n      From worker 3:    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n      From worker 4:    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n      From worker 5:    [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\nEvery worker has the same content. Note in the above we used remotecall rather than the macro @fetchfrom, as seen elsewhere. We will explain why in the section of pitfalls.\n\n\nShared arrays\nUnlike distributed DArray from DistributedArrays.jl, a SharedArray object is stored in full on the control process, but it is shared across all workers on the same node, with a significant cache on each worker. SharedArrays package is part of Julia’s Standard Library (comes with the language).\n\nSimilar to DistributedArrays, you can read elements using their global indices from any worker.\nUnlike with DistributedArrays, with SharedArrays you can write into any part of the array from any worker using their global indices. This makes it very easy to parallelize any serial code.\n\nThere are certain downsides to SharedArray (compared to DistributedArrays.jl): 1. The ability to write into the same array elements from multiple processes creates the potential for a race condition and indeterministic outcome with a poorly written code! 1. You are limited to a set of workers on the same node (due to SharedArray’s intrinsic implementation). 1. You will have very skewed (non-uniform across processes) memory usage.\nLet’s start with serial Julia (julia) and initialize a 1D shared array:\nusing Distributed, SharedArrays\naddprocs(4)\na = SharedArray{Float64}(30);\na[:] .= 1.0           # assign from the control process\n@fetchfrom 2 sum(a)   # correct (30.0)\n@fetchfrom 3 sum(a)   # correct (30.0)\n@sync @spawnat 2 a[:] .= 2.0   # can assign from any worker!\n@fetchfrom 3 sum(a)            # correct (60.0)\nYou can use a function to initialize an array, however, pay attention to the result:\nb = SharedArray{Int64}((1000), init = x -> x .= 0);    # use a function to initialize `b`\nb = SharedArray{Int64}((1000), init = x -> x .+= 1)   # each worker updates the entire array in-place!\n\nKey idea: each worker runs this function!\n\nLet’s fill each element with its corresponding myd() value:\n@everywhere println(myid())     # let's use these IDs in the next function\nc = SharedArray{Int64}((20), init = x -> x .= myid())   # indeterminate outcome! each time a new result\nEach worker updates every element, but the order in which they do this varies from one run to another, producing indeterminate outcome. Let’s see how to avoid such unexpected outcomes.\n\n\nPartition of shared arrays\nJulia defines a “virtual boundary” around the portion of a shared array mapped to a worker. Let’s take a look at the following example. We create a shared array u of length 17.\nN = 17\nu = SharedArray{Float64}(n)\nThen we use function localindices to see the start and end indices of the partitions of the array object u. First we run this on the control process\nlocalindices(u)\nThe output might be a little surprise\n1:0\nNext we run localindices(u) on each worker\n@everywhere using SharedArrays\nfor p in workers()\n    @fetchfrom p println(localindices(u))\nend\nNow the output is what we expected\n      From worker 2:    1:4\n      From worker 3:    5:8\n      From worker 4:    9:12\n      From worker 5:    13:17\nNow come back to the task we want to accomplish while we hit the unexpected outcomes. What we really want is each worker should fill only its assigned block (parallel init, same result every time. This can be achieved as follows\nc = SharedArray{Int64}((20), init = x -> x[localindices(x)] .= myid())\n\n\nAnother way to avoid a race condition: use parallel for loop\nLet’s initialize a 2D SharedArray:\na = SharedArray{Float64}(100,100);\n@distributed for i in 1:100     # parallel for loop split across all workers\n    for j in 1:100\n        a[i,j] = myid()           # ID of the worker that initialized this element\n    end\nend\nfor i in workers()\n    @spawnat i println(localindices(a))   # weird: shows 1D indices for 2D array\nend\na                           # available on all workers\na[1:10,1:10]                # on the control process\n@fetchfrom 2 a[1:10,1:10]   # on worker 2\n\n\nPitfall of using shared objects\nLet’s have a look at the following example, restart Julia with julia -p 4\nusing SharedArrays\na = SharedArray{Float64}(5_000_000);\nvarinfo()\nWe will see the output looks like this\n  name                    size summary                              \n  –––––––––––––––– ––––––––––– –––––––––––––––––––––––––––––––––––––\n  A                 38.148 MiB 5000000-element SharedVector{Float64}\n  Base                         Module                               \n  Core                         Module                               \n  Distributed       39.861 MiB Module                               \n  InteractiveUtils 253.909 KiB Module                               \n  Main                         Module                               \n  ans               38.148 MiB 5000000-element SharedVector{Float64}\nIf we run varinfo() on a worker process, say, 3\n@everywhere using InteractiveUtils\n@fetchfrom 3 varinfo()\nwe get\n  name              size summary\n  ––––––––––– –––––––––– –––––––\n  Base                   Module \n  Core                   Module \n  Distributed 39.155 MiB Module \n  Main                   Module \nWe do not see the variable A per se. But we do see the same amount of data 39.861 MiB claimed by Distributed as on the control process.\nIf we try to set the values in A on worker 3 to its worker ID with the following code\n@everywhere using SharedArrays\n@everywhere function set_to_myid()\n    idx = localindices(A);\n    A[idx] .= myid();\nend\n@fetchfrom 3 set_to_myid()\nwe will get the following error\nERROR: On worker 3:\nUndefVarError: A not defined\nThis suggests that, the data is shared across all processes, but the name space of the variable itself is not.\nWe now modify the function set_to_myid a bit as follows\n@everywhere function set_to_myid(a)\n    idx = localindices(a);\n    a[idx] .= myid();\nend\nremotecall_fetch(set_to_myid,3,A)\nThis time it should not give any error. The portion of A is properly set. If we check the output of varinfo() again, wee\n@fetchfrom 3 varinfo()\n  name              size summary                                      \n  ––––––––––– –––––––––– –––––––––––––––––––––––––––––––––––––––––––––\n  Base                   Module                                       \n  Core                   Module                                       \n  Distributed 39.159 MiB Module                                       \n  Main                   Module                                       \n  set_to_myid    0 bytes set_to_myid (generic function with 2 methods)\nStill, A is not present. But the assignment of worker ID to A on worker 3 has worked.\nIf we run the following command\n@fetchfrom 3 A[localindices(A)] .= myid()\nit works, but it has a different meaning. It copies A to worker 3 and performs the operation of assignments there. This becomes evident when we see the output of varinfo on worker 3\n  name              size summary                                      \n  ––––––––––– –––––––––– –––––––––––––––––––––––––––––––––––––––––––––\n  A           38.147 MiB 5000000-element SharedVector{Float64}        \n  Base                   Module                                       \n  Core                   Module                                       \n  Distributed 39.161 MiB Module                                       \n  Main                   Module                                       \n  set_to_myid    0 bytes set_to_myid (generic function with 2 methods)\nThe conclusion so far is, use remotecall to execute the code on workers. Use macros @fetch etc carefully.\n\n\nSolving 1D heat equation using shared array\nConsider a (simplified) physics problem: A rod of length \\([-L,L]\\) heated in the middle, then the heat source is removed. The temperature distribution \\(T(x,t)\\) across the rod over time can be simulated by the following equation\n\\[ T(x,t+t) = T(x-x,t) + T(x+x,t) \\]\non a set of evenly spaced points apart by \\(\\Delta x\\). The initial condition is shown in the diagram below. At \\(t=0\\), \\(T(x,0) = T_0\\) for \\(-h \\leq x \\leq h\\) and \\(T(x,0) = 0\\) elsewhere.\n\nAt both ends, we impose the boundary conditions \\(T(-L,t)=0\\) and \\(T(L,t)=0\\).\nThe solution of the temperature across the rod is a bell shaped curve being flattened over time. The figure below shows a snapshot of the solution (with \\(T_0 = 1\\)) on an interval \\([-1,1]\\) at certain time.\n\nThis example is used in many of our training courses, for example, MPI, Fortran and Python courses to illustrate parallel processing in advanced research computing. A more “accurate” formula involving three spatial points \\(x_i - \\Delta x\\), \\(x_i\\) and \\(x_i + \\Delta x\\) for computing the temperature at the next time step \\(t_n+\\Delta t\\) for interior points \\(i=1,\\ldots,N\\) is given below\n\\[ T(x_i,t_n+t) = (1-2k)T(x_i,t_n) + k((T(x_{i-1},t_n)+T(x_{i+1},t_n)) \\]\nwhere \\(k\\) is a parameter that shall be chosen properly in order for the numerical procedure to be stable.\nNote in the formula, the temperature \\(T(x_i,t_n+\\Delta t)\\) at the next time step \\(t_{n+1} = t_n +\\Delta t\\) can be computed explicitly using the values of \\(T\\) at three points at current time step \\(t_n\\), which are all known. This allows us to compute all grid values of \\(T\\) at time \\(t_n+\\Delta t\\) independent of each other, hence to achieve parallelism.\nLet \\(U_i^n\\) denote the value of \\(T(x_i,t_n)\\) at grid points \\(x_i\\), \\(i=1,\\ldots,N\\) at time \\(t_n\\), we use the short notation\n\\[ U_i^{n+1} = (1-2k)U_i^n + k(U_{i-1}^n + U_{i+1}^n). \\]\nfor \\(i=1,\\ldots,N\\). This can be translated into the following code with one dimensional two arrays unew[1:N] and u[1:N] holding values at the \\(N\\) grid points at \\(t_{n+1}\\) and \\(t_n\\) respectively11 Note that 2k is not a typo, it is a legal Julia expression, meaning 2*k.\nfor i=1:N\n    unew[i] = (1-2k)u[i] + k*(u[i-1] + u[i+1])\nend\nThis loop in fact can be replaced by the following one line of code using a single array u[1:N]\nu[2:N-1] = (1-2k)*u[2:N-1] + k*(u[1:N-2) + u[3:N])\nIn this case, vectorized operations on the right hand side take place first before the individual elements on the left hand side are updated.\nSerial code. A serial code is given below. The time evolution loop is at the end of the code.\nusing Plots, Base\n\n# Input parameters\na = 1.0\nn = 65          # Number of end points 1 to n (n-1 intervals).\ndt = -0.0005        # dt <= 0.5*dx^2/a, ignored if set negative\nk = 0.1\nnum_steps = 10000       # Number of stemps in t.\noutput_freq = 1     # Number of stemps per display.\nxlim = zeros(Float64,2)\nxlim[1] = -1.0\nxlim[2] = 1.0\nheat_range = zeros(Float64,2)\nheat_range[1] = -0.1\nheat_range[2] = 0.1\nheat_temp = 1.0\n\n# Set the k value\ndx = (xlim[2] - xlim[1])/(n-1)\nif (dt > 0)\n    k = a*dt/(dx*dx)\nend\n\n# Create space for u; create coordinates x for demo\nx = xlim[1] .+ dx*(collect(1:n) .-1);\nu = zeros(Float64,n);\n\n# Set initial condition\nix = findall(x->(heat_range[1] .< x .&& x .< heat_range[2]),x);\n@. u[ix] = heat_temp;\n\n# Display plot (it could be really slow on some systems to launch)\ndisplay(plot(x,u[1:n],lw=3,ylim=(0,1),label=(\"u\")))\n\n# Compute the solution over time\nfor j=1:num_steps\n    # Compute the solution for the next time step\n    u[2:n-1] = (1.0-2.0k)*u[2:n-1] + k*(u[1:n-2]+u[3:n])\n  \n    # Display the solution (comment it out for pro\n    if (j % output_freq == 0)\n        display(plot(x,u[1:n],lw=3,ylim=(0,1),label=(\"u\")))\n    end        \nend\nParallel solution. We divide the domain - the set of grid points - into subdomains and assign each of them and the computational tasks to a worker. That’s the basic idea\n\nLet \\(P\\) be the number of participating processes - workers in Julia’s term. The way of partitioning the domain is not unique. We choose a simple one: each subdomain gets \\(N \\div P\\) points and the last one includes the remainder, that is, for subdomain \\(1\\) to \\(P-1\\), the number of local grid points\n\\[ N_{} = N P \\]\nand for the last subdomain\n\\[ N_P = N P + N P. \\]\nThis is in fact what Julia does for the partition of one dimensional arrays. For example, let \\(N=17\\). The following code shows how the shared array u[1:N] is “partitioned” on each worker\nusing SharedArrays\nN = 17\nu = zeros(N)\nfor p = workers() # Assume we have created 4 workers\n    @fetchfrom p println(localindices(u))\nend\nThe output of the start and end indices of each subset on each of the workers looks like the following\n      From worker 2:    1:4\n      From worker 3:    5:8\n      From worker 4:    9:12\n      From worker 5:    13:17\nThe calculation of\nu[2:N-1] = (1-2k)*u[2:N-1] + k*(u[1:N-2) + u[3:N])\nis now done on each subset of the grid points, as shown in the diagram below\n\nWe need to replace the start and end indices with the local ones l1 and lN\nu[l1:lN] = (1-2k)*u[l1:lN] + k*(u[l1-1:lN-1) + u[l1+1:lN+1])\nNote for the left most subset, we need to skip the very first one, as it is the boundary point, no need to compute the solution for. So is for the right most one, we need to skip the very last one.\nWe define a function update that computes the solution only for the portion that the worker owns it. For demo purpose, we have it compute the local start and end indices as well, which could be set in a different way.\n@everywhere function update(u,me)\n    # Determine the start and end indices\n    l1 = np*(me - 1) + 1;\n    ln = l1 + np + n % num_workers - 1;\n\n    # Skip the left most and right most end points\n    if (me == 1) \n        l1 = 2;\n    end\n    if (me == num_workers)\n        ln = n - 1;\n    end \n    u[l1:ln] = (1.0-2k)*u[l1:ln] + k*(u[l1-1:ln-1]+u[l1+1:ln+1])\nend\nNow our parallel version of the code looks like the following.\nfor j=1:num_steps \n    @sync begin\n        for p=1:num_workers\n            @async remotecall(update,p+1,u,p);\n        end\n    end           \n    if (j % output_freq == 0)\n        display(plot(x,u,lw=3,ylim=(0,1),label=(\"u\")))\n    end\nend\nThe time evolution for loop is on the control process. Inside the loop, it calls the function update on workers at each time step (in a fork-join fashion). The workers are synchronized after they complete their own computation before moving ahead to the time step. The display is executed on the control process\n\nExercise 1D heat equation using shared arrays\n\nUse the serial code as the base. Write a skeleton of the parallel code\n\n\nusing Base, Distributed, SharedArrays\nusing Plots\n\n# Input parameters\n... ...\n\n# Set the k value\n... ...\n\n# Set x-coordinates for plot\n... ...\n\n# For demo purpose, set number of workers to 4\nnum_workers = nworkers() # Number of workder processes\nnp = div(n,nprocs())    # Number of points local to the process\n\n# Allocate spaces\nu = SharedArray{Float64}(n);\nu .= 0;\n\n# Set initial condition\nix = findall(x->(heat_range[1] .< x .&& x .< heat_range[2]),x);\n@. u[ix] = heat_temp;\n\n# Broadcast parameters to all\n@everywhere k=$k\n@everywhere n=$n\n@everywhere np=$np\n@everywhere num_workers=$num_workers\n@everywhere l1=1\n@everywhere ln=1\n\n# Define the function update on all processes\n@everywhere using SharedArrays\n@everywhere function get_partition(me)\n    # Determine the start and end indices l1, ln (they are global)\n    l1 = ... \n    ln = ...\n\n    # Skip the left most and right most end points\n    if (me == 1) \n        l1 = 2;\n    end\n    if (me == num_workers)\n        ln = n - 1;\n    end \nend\ndisplay(plot(x,u,lw=3,ylim=(0,1),label=(\"u\")))\n\n# Get partition info\nfor p in workers()\n    @async remotecall_fetch(get_partition,p,p-1)\nend\n\n# Update u in time on workders\n@time begin\nfor j=1:num_steps \n    @sync begin\n        for p in workers()\n            @async remotecall(update,p,u);\n        end\n    end           \n    if (j % output_freq == 0)\n        display(plot(x,u,lw=3,ylim=(0,1),label=(\"u\")))\n    end\nend\nend\n\nsleep(10)\n\n\nComplete the function get_partition\n\n\n@everywhere function get_partition(me)\n    # Determine the start and end indices\n    l1 = ... \n    ln = ...\n\n    # Skip the left most and right most end points\n    if (me == 1) \n        l1 = 2;\n    end\n    if (me == num_workers)\n        ln = n - 1;\n    end \nend\n\n\nWrite a function update as follows\n\n\n@everywhere function update(u)\n    u[l1:ln] = (1.0-2k)*u[l1:ln] + k*(u[l1-1:ln-1]+u[l1+1:ln+1])\nend\n\n\nComplete the parallel code and see if you can get the same output (graphical display) as the serial code."
  },
  {
    "objectID": "julia/threads-julia-set.html",
    "href": "julia/threads-julia-set.html",
    "title": "Parallelizing the Julia set with Base.Threads",
    "section": "",
    "text": "The project is the mathematical problem to compute a Julia set – no relation to Julia language! A Julia set is defined as a set of points on the complex plane that remain bound under infinite recursive transformation \\(f(z)\\). We will use the traditional form \\(f(z)=z^2+c\\), where \\(c\\) is a complex constant. Here is our algorithm:\nWe should get something conceptually similar to this figure (here \\(c = 0.355 + 0.355i\\); we’ll get drastically different fractals for different values of \\(c\\)):\nNote: you might want to try these values too: - \\(c = 1.2e^{1.1πi}\\) \\(~\\Rightarrow~\\) original textbook example - \\(c = -0.4-0.59i\\) and 1.5X zoom-out \\(~\\Rightarrow~\\) denser spirals - \\(c = 1.34-0.45i\\) and 1.8X zoom-out \\(~\\Rightarrow~\\) beans - \\(c = 0.34-0.05i\\) and 1.2X zoom-out \\(~\\Rightarrow~\\) connected spiral boots\nBelow is the serial code juliaSetSerial.jl. If you are running Julia on your own computer, make sure you have the required packages:\nLet’s study the code:\nLet’s run this code with julia juliaSetSerial.jl. On my laptop it reports 931.024 ms.\n}}\n}}\nThis code will produce the file test.nc that you can download to your computer and render with ParaView or other visualization tool."
  },
  {
    "objectID": "julia/threads-julia-set.html#parallelizing",
    "href": "julia/threads-julia-set.html#parallelizing",
    "title": "Parallelizing the Julia set with Base.Threads",
    "section": "Parallelizing",
    "text": "Parallelizing\n\nLoad Base.Threads.\nAdd @threads before the outer loop, and time this parallel loop.\n\nOn my laptop with 8 threads the timing is 193.942 ms (4.8X speedup) which is good but not great – certainly worse than linear speedup … The speedup on Uu cluster is not great either. There could be several potential problems:\n\nFalse sharing effect (cache issues with multiple threads writing into adjacent array elements).\nLess than perfect load balancing between different threads.\nRow-major vs. column-major loop order for filling in the stability array.\nSome CPU cores are slower efficiency cores, and they are slowing down the whole calculation.\n\n\nTake-home exercise “Fractal.2”\nHow would you fix this issue? If you manage to get a speedup closer to 8X with Base.Threads on 8 cores, we would love to hear your solution! Please only check the {{<a “/bad-speedup-solution” “solution”>}} once you work on the problem yourself."
  },
  {
    "objectID": "julia/threads-slow-series.html",
    "href": "julia/threads-slow-series.html",
    "title": "Multi-threading with Base.Threads",
    "section": "",
    "text": "Important: Today we are working on a compute node inside an interactive job scheduled with salloc. Do not run Julia on the login node!\nLet’s start Julia by typing julia in bash:\nIf instead we start with julia -t 4 (or prior to v1.5 with JULIA_NUM_THREADS=4 julia):\nWhen launched from this interface, these four threads will run on several CPU cores on a compute node – likely a combination of concurrent and parallel execution, especially considering the restrictions from your salloc job.\nLet’s run our first multi-threaded code:\nThis would split the loop between 4 threads running on two CPU cores: each core would be taking turns running two of your threads (and likely threads from other users).\nLet’s now fill an array with values in parallel:\nHere we are filling this array in parallel, and no thread will overwrite another thread’s result. In other words, this code is thread-safe.\nLet’s initialize a large floating array:\nand then fill it with values using a single thread, and time this operation:\nOn Uu I get 14.38s, 14.18s, 14.98s with one thread.\nLet’s now time parallel execution with 4 threads on 2 CPU cores:\nOn Uu I get 6.57s, 6.19s, 6.10s – this is ~2X speedup, as expected."
  },
  {
    "objectID": "julia/threads-slow-series.html#lets-add-reduction",
    "href": "julia/threads-slow-series.html#lets-add-reduction",
    "title": "Multi-threading with Base.Threads",
    "section": "Let’s add reduction",
    "text": "Let’s add reduction\nWe will compute the sum \\(\\sum_{i=1}^{10^6}i\\) with multiple threads. Consider this code:\ntotal = 0\n@threads for i = 1:Int(1e6)\n    global total += i          # use `total` from global scope\nend\nprintln(\"total = \", total)\nThis code is not thread-safe:\n\nrace condition: multiple threads updating the same variable at the same time\na new result every time\nunfortunately, @threads does not have built-in reduction support\n\nLet’s make it thread-safe (one of many possible solutions) using an atomic variable total. Only one thread can update an atomic variable at a time; all other threads have to wait for this variable to be released before they can write into it.\ntotal = Atomic{Int64}(0)\n@threads for i in 1:Int(1e6)\n    atomic_add!(total, i)\nend\nprintln(\"total = \", total[])   # need to use [] to access atomic variable's value\nNow every time we get the same result. This code is supposed to be much slower: threads are waiting for others to finish updating the variable, so with 4 threads and one variable there should be a lot of waiting … Atomic variables were not really designed for this type of usage … Let’s do some benchmarking!"
  },
  {
    "objectID": "julia/threads-slow-series.html#benchmarking-in-julia",
    "href": "julia/threads-slow-series.html#benchmarking-in-julia",
    "title": "Multi-threading with Base.Threads",
    "section": "Benchmarking in Julia",
    "text": "Benchmarking in Julia\nWe already know that we can use @time macro for timing our code. Let’s do summation of integers from 1 to Int64(1e8) using a serial code:\nn = Int64(1e8)\ntotal = Int128(0)   # 128-bit for the result!\n@time for i in 1:n\n    global total += i\nend\nprintln(\"total = \", total)\nOn Uu I get 10.87s, 10.36s, 11.07s. Here @time also includes JIT compilation time (marginal here). Let’s switch to @btime from BenchmarkTools: it runs the code several times, reports the shortest time, and prints the result only once. Therefore, with @btime you don’t need to precompile the code.\nusing BenchmarkTools\nn = Int64(1e8)\ntotal = Int128(0)   # 128-bit for the result!\n@btime for i in 1:n\n    global total += i\nend\nprintln(\"total = \", total)\n10.865 s\nNext we’ll package this code into a function:\nfunction quick(n)\n    total = Int128(0)   # 128-bit for the result!\n    for i in 1:n\n        total += i\n    end\n    return(total)\nend\n@btime quick(Int64(1e8))    # correct result, 1.826 ns runtime\n@btime quick(Int64(1e9))    # correct result, 1.825 ns runtime\n@btime quick(Int64(1e15))   # correct result, 1.827 ns runtime\nIn all these cases we see ~2 ns running time – this can’t be correct! What is going on here? It turns out that Julia is replacing the summation with the exact formula \\(n(n+1)/2\\)!\nWe want to: 1. force computation \\(~\\Rightarrow~\\) we’ll compute something more complex than simple integer summation, so that it cannot be replaced with a formula 1. exclude compilation time \\(~\\Rightarrow~\\) we’ll package the code into a function + precompile it 1. make use of optimizations for type stability \\(~\\Rightarrow~\\) package into a function + precompile it 1. time only the CPU-intensive loops"
  },
  {
    "objectID": "julia/threads-slow-series.html#slow-series",
    "href": "julia/threads-slow-series.html#slow-series",
    "title": "Multi-threading with Base.Threads",
    "section": "Slow series",
    "text": "Slow series\nWe could replace integer summation \\(\\sum_{i=1}^\\infty i\\) with the harmonic series, however, the traditional harmonic series \\(\\sum\\limits_{k=1}^\\infty{1\\over k}\\) diverges. It turns out that if we omit the terms whose denominators in decimal notation contain any digit or string of digits, it converges, albeit very slowly (Schmelzer & Baillie 2008), e.g.\n\nBut this slow convergence is actually good for us: our answer will be bounded by the exact result (22.9206766192…) on the upper side. We will sum all the terms whose denominators do not contain the digit “9”.\nWe will have to check if “9” appears in each term’s index i. One way to do this would be checking for a substring in a string:\nif !occursin(\"9\", string(i))\n    <add the term>\nend\nIt turns out that integer exclusion is ∼4X faster (thanks to Paul Schrimpf from the Vancouver School of Economics @UBC for this code!):\nfunction digitsin(digits::Int, num)   # decimal representation of `digits` has N digits\n    base = 10\n    while (digits ÷ base > 0)   # `digits ÷ base` is same as `floor(Int, digits/base)`\n        base *= 10\n    end\n    # `base` is now the first Int power of 10 above `digits`, used to pick last N digits from `num`\n    while num > 0\n        if (num % base) == digits     # last N digits in `num` == digits\n            return true\n        end\n        num ÷= 10                     # remove the last digit from `num`\n    end\n    return false\nend\nif !digitsin(9, i)\n    <add the term>\nend\nLet’s now do the timing of our serial summation code with 1e8 terms:\nfunction slow(n::Int64, digits::Int)\n    total = Float64(0)    # this time 64-bit is sufficient!\n    for i in 1:n\n        if !digitsin(digits, i)\n            total += 1.0 / i\n        end\n    end\n    return total\nend\n@btime slow(Int64(1e8), 9)   # total = 13.277605949858103, runtime 2.986 s"
  },
  {
    "objectID": "julia/threads-slow-series.html#st-multi-threaded-version-using-an-atomic-variable",
    "href": "julia/threads-slow-series.html#st-multi-threaded-version-using-an-atomic-variable",
    "title": "Multi-threading with Base.Threads",
    "section": "1st multi-threaded version: using an atomic variable",
    "text": "1st multi-threaded version: using an atomic variable\nRecall that with an atomic variable only one thread can write to this variable at a time: other threads have to wait before this variable is released, before they can write. With several threads running in parallel, there will be a lot of waiting involved, and the code should be relatively slow.\nusing Base.Threads\nusing BenchmarkTools\nfunction slow(n::Int64, digits::Int)\n    total = Atomic{Float64}(0)\n    @threads for i in 1:n\n        if !digitsin(digits, i)\n            atomic_add!(total, 1.0 / i)\n        end\n    end\n    return total[]\nend\n@btime slow(Int64(1e8), 9)\n\nExercise “Threads.1”\nPut this version of slow() along with digitsin() into a file atomicThreads.jl and run it from the bash terminal (or from from REPL). First, time this code with 1e8 terms using one thread (serial run julia atomicThreads.jl). Next, time it with 2 or 4 threads (parallel run julia -t 2 atomicThreads.jl). Did you get any speedup? Make sure you obtain the correct numerical result.\n\nWith one thread I measured 2.838 s. The runtime stayed essentially the same (now we are using atomic_add()) which makes sense: with one thread there is no waiting for the variable to be released.\nWith four threads, I measured 5.261 s – let’s discuss! Is this what we expected?\n\nExercise “Threads.2”\nLet’s run the previous exercise as a batch job with sbatch.\n\n\nHint: you will need to go to the login node and submit a multi-core job with sbatch shared.sh. When finished, do not forget to go back to (or restart) your interactive job."
  },
  {
    "objectID": "julia/threads-slow-series.html#nd-version-alternative-thread-safe-implementation",
    "href": "julia/threads-slow-series.html#nd-version-alternative-thread-safe-implementation",
    "title": "Multi-threading with Base.Threads",
    "section": "2nd version: alternative thread-safe implementation",
    "text": "2nd version: alternative thread-safe implementation\nIn this version each thread is updating its own sum, so there is no waiting for the atomic variable to be released? Is this code faster?\nusing Base.Threads\nusing BenchmarkTools\nfunction slow(n::Int64, digits::Int)\n    total = zeros(Float64, nthreads())\n    @threads for i in 1:n\n        if !digitsin(digits, i)\n            total[threadid()] += 1.0 / i\n        end\n    end\n    return sum(total)\nend\n@btime slow(Int64(1e8), 9)\n\nUpdate: Pierre Fortin brought to our attention the false sharing effect. It arises when several threads are writing into variables placed close enough to each other to end up in the same cache line. Cache lines (typically ~32-128 bytes in size) are chunks of memory handled by the cache. If any two threads are updating variables (such as two neighbouring elements in our total array here) that end up in the same cache line, the cache line will have to migrate between the two threads’ caches, reducing the performance.\nIn general, you want to align shared global data (thread partitions in the array total in our case) to cache line boundaries, or avoid storing thread-specific data in an array indexed by the thread id or rank. Pierre suggested a solution using the function space() which introduces some spacing between array elements so that data from different threads do not end up in the same cache line:\nusing Base.Threads\nusing BenchmarkTools\n\nfunction digitsin(digits::Int, num)   # decimal representation of `digits` has N digits\n    base = 10\n    while (digits ÷ base > 0)   # `digits ÷ base` is same as `floor(Int, digits/base)`\n        base *= 10\n    end\n    # `base` is now the first Int power of 10 above `digits`, used to pick last N digits from `num`\n    while num > 0\n        if (num % base) == digits     # last N digits in `num` == digits\n            return true\n        end\n        num ÷= 10                     # remove the last digit from `num`\n    end\n    return false\nend\n\n# Our initial function:\nfunction slow(n::Int64, digits::Int)\n    total = zeros(Float64, nthreads())\n    @threads for i in 1:n\n        if !digitsin(digits, i)\n            total[threadid()] += 1.0 / i\n        end\n    end\n    return sum(total)\nend\n\n# Function optimized to prevent false sharing:\nfunction space(n::Int64, digits::Int)\n    space = 8 # assume a 64-byte cache line, hence 8 Float64 elements per cache line\n    total = zeros(Float64, nthreads()*space)\n    @threads for i in 1:n\n        if !digitsin(digits, i)\n            total[threadid()*space] += 1.0 / i\n        end\n    end\n    return sum(total)\nend\n\n@btime slow(Int64(1e8), 9)\n@btime space(Int64(1e8), 9)\nHere are the timings from two successive calls to slow() and space() on uu.c3.ca:\n[~/tmp]$ julia separateSums.jl \n  2.836 s (7 allocations: 656 bytes)\n  2.882 s (7 allocations: 704 bytes)\n[~/tmp]$ julia -t 4 separateSums.jl \n  935.609 ms (23 allocations: 2.02 KiB)\n  687.972 ms (23 allocations: 2.23 KiB)\n[~/tmp]$ julia -t 10 separateSums.jl\n  608.226 ms (53 allocations: 4.73 KiB)\n  275.662 ms (54 allocations: 5.33 KiB)\nThe speedup is substantial! Thank you Pierre!\nWe see similar speedup with space = 4, but not quite with space = 2, suggesting that we are dealing with 32-byte cache lines on our system.\n\nExercise “Threads.3”\nSave this code as separateSums.jl (along with other necessary bits) and run it on four threads from the command line julia -t 4 separateSums.jl. What is your new code’s timing?\n\nWith four threads I measured 992.346 ms – let’s discuss!"
  },
  {
    "objectID": "julia/threads-slow-series.html#rd-multi-threaded-version-using-heavy-loops",
    "href": "julia/threads-slow-series.html#rd-multi-threaded-version-using-heavy-loops",
    "title": "Multi-threading with Base.Threads",
    "section": "3rd multi-threaded version: using heavy loops",
    "text": "3rd multi-threaded version: using heavy loops\nThis version is classical task parallelism: we divide the sum into pieces, each to be processed by an individual thread. For each thread we explicitly compute the start and finish indices it processes.\nusing Base.Threads\nusing BenchmarkTools\nfunction slow(n::Int64, digits::Int)\n    numthreads = nthreads()\n    threadSize = floor(Int64, n/numthreads)   # number of terms per thread (except last thread)\n    total = zeros(Float64, numthreads);\n    @threads for threadid in 1:numthreads\n        local start = (threadid-1)*threadSize + 1\n        local finish = threadid < numthreads ? (threadid-1)*threadSize+threadSize : n\n        println(\"thread $threadid: from $start to $finish\");\n        for i in start:finish\n            if !digitsin(digits, i)\n                total[threadid] += 1.0 / i\n            end\n        end\n    end\n    return sum(total)\nend\n@btime slow(Int64(1e8), 9)\nLet’s time this version together with heavyThreads.jl: 984.076 ms – is this the fastest version?\n\nExercise “Threads.4”\nWould the runtime be different if we use 2 threads instead of 4?\n\nFinally, below are the timings on Cedar with heavyThreads.jl. Note that the times reported here were measured with 1.6.2. Going from 1.5 to 1.6, Julia saw quite a big improvement (~30%) in performance, plus a CPU on Cedar is different from a vCPU on Uu, so treat these numbers only as relative to each other.\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=...\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\n#SBATCH --account=def-someuser\nmodule load julia\njulia -t $SLURM_CPUS_PER_TASK heavyThreads.jl\n\n\n\nCode\nserial\n2 cores\n4 cores\n8 cores\n16 cores\n\n\nTime\n7.910 s\n4.269 s\n2.443 s\n1.845 s\n1.097 s"
  },
  {
    "objectID": "julia/threads-slow-series.html#task-parallelism-with-base.threads-building-a-dynamic-scheduler",
    "href": "julia/threads-slow-series.html#task-parallelism-with-base.threads-building-a-dynamic-scheduler",
    "title": "Multi-threading with Base.Threads",
    "section": "Task parallelism with Base.Threads: building a dynamic scheduler",
    "text": "Task parallelism with Base.Threads: building a dynamic scheduler\nIn addition to @threads (automatically parallelize a loop with multiple threads), Base.Threads includes Threads.@spawn that runs a task (an expression / function) on any available thread and then immediately returns to the main thread.\nConsider this:\nusing Base.Threads\nimport Base.Threads: @spawn # has to be explicitly imported to avoid potential conflict with Distributed.@spawn\nnthreads()                  # make sure you have access to multiple threads\nthreadid()                  # always shows 1 = local thread\nfetch(@spawn threadid())    # run this function on another available thread and get the result\nEvery time you run this, you will get a semi-random reponse, e.g.\nfor i in 1:30\n    print(fetch(@spawn threadid()), \" \")\nend\n\n\n\nYou can think of @spawn as a tool to dynamically offload part of your computation to another thread – this is classical task parallelism, unlike @threads which is data parallelism.\nWith @spawn it is up to you to write an algorithm to subdivide your computation into multiple threads. With a large loop, one possibility is to divide the loop into two pieces, offload the first piece to another thread and run the other one locally, and then recursively subdivide these pieces into smaller chunks. With N subdivisions you will have 2^N tasks running on a fixed number of threads, and only one of these tasks will not be scheduled with @spawn.\nusing Base.Threads\nimport Base.Threads: @spawn\nusing BenchmarkTools\n\nfunction digitsin(digits::Int, num)\n    base = 10\n    while (digits ÷ base > 0)\n        base *= 10\n    end\n    while num > 0\n        if (num % base) == digits\n            return true\n        end\n        num ÷= 10\n    end\n    return false\nend\n\n@doc \"\"\"\na, b are the left and right edges of the current interval;\nnumsubs is the number of subintervals, each will be assigned to a thread;\nnumsubs will be rounded up to the next power of 2,\ni.e. setting numsubs=5 will effectively use numsubs=8\n\"\"\" ->\nfunction slow(n::Int64, digits::Int, a::Int64, b::Int64, numsubs=16)\n    if b-a > n/numsubs\n        mid = (a+b)>>>1   # shift by 1 bit to the right\n        finish = @spawn slow(n, digits, a, mid, numsubs)\n        t2 = slow(n, digits, mid+1, b, numsubs)\n        return fetch(finish) + t2\n    end\n    t = Float64(0)\n    println(\"computing on thread \", threadid())\n    for i in a:b\n        if !digitsin(digits, i)\n            t += 1.0 / i\n        end\n    end\n    return t\nend\n\nn = Int64(1e8)\n@btime slow(n, 9, 1, n, 1)    # run the code in serial (one interval, use one thread)\n@btime slow(n, 9, 1, n, 4)    # 4 intervals, each scheduled to run on 1 of the threads\n@btime slow(n, 9, 1, n, 16)   # 16 intervals, each scheduled to run on 1 of the threads\nWith four threads and numsubs=4, in one of my tests the runtime went down from 2.986 s (serial) to 726.044 ms. However, depending on the number of subintervals, Julia might decide not to use all four threads! Consider this:\njulia> nthreads()\n4\n\njulia> n = Int64(1e9)\n1000000000\n\njulia> @btime slow(n, 9, 1, n, 1)    # serial run (one interval, use one thread)\ncomputing on thread 1\ncomputing on thread 1\ncomputing on thread 1\ncomputing on thread 1\n  29.096 s (12 allocations: 320 bytes)\n14.2419130103833\n\njulia> @btime slow(n, 9, 1, n, 4)    # 4 intervals\ncomputing on thread 1 - this line was printed 4 times\ncomputing on thread 2 - this line was printed 5 times\ncomputing on thread 3 - this line was printed 6 times\ncomputing on thread 4 - this line was printed once\n  14.582 s (77 allocations: 3.00 KiB)\n14.2419130103818\n\njulia> @btime slow(n, 9, 1, n, 128)    # 128 intervals\ncomputing on thread 1 - this line was printed 132 times\ncomputing on thread 2 - this line was printed 130 times\ncomputing on thread 3 - this line was printed 131 times\ncomputing on thread 4 - this line was printed 119 times\n  11.260 s (2514 allocations: 111.03 KiB)\n14.24191301038047"
  },
  {
    "objectID": "julia/threadsx-julia-set.html",
    "href": "julia/threadsx-julia-set.html",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "",
    "text": "So far with ThreadsX, most of our parallel codes featured reduction – recall the functions ThreadsX.mapreduce() and ThreadsX.sum(). However, in the Julia set problem we want to process an array without reduction.\nLet’s first modify the serial code! We will use another function from Base library:\nLet’s modify our serial code juliaSetSerial.jl:\nRunning this new, vectorized version of the serial code on my laptop, I see @btime report 1.011 s."
  },
  {
    "objectID": "julia/threadsx-julia-set.html#parallelizing-the-vectorized-code",
    "href": "julia/threadsx-julia-set.html#parallelizing-the-vectorized-code",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "Parallelizing the vectorized code",
    "text": "Parallelizing the vectorized code\n\nLoad ThreadsX library.\nReplace map() with ThreadsX.map().\n\nWith 8 threads on my laptop, the runtime went down to 180.815 – 5.6X speedup. On 8 cores on Uu I see 6.5X speedup."
  },
  {
    "objectID": "julia/threadsx-julia-set.html#alternative-parallel-solution",
    "href": "julia/threadsx-julia-set.html#alternative-parallel-solution",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "Alternative parallel solution",
    "text": "Alternative parallel solution\nJeremiah O’Neil suggested an alternative, slightly faster implementation using ThreadsX.foreach (not covered in this workshop):\nfunction juliaSet(height, width)\n    stability = zeros(Int32, height, width)\n    ThreadsX.foreach(1:height) do i\n        for j = 1:width\n            point = (2*(j-0.5)/width-1) + (2*(i-0.5)/height-1)im\n            stability[i,j] = pixel(point)\n        end\n    end\n    return stability\nend"
  },
  {
    "objectID": "julia/threadsx-julia-set.html#running-multi-threaded-julia-codes-on-a-production-cluster",
    "href": "julia/threadsx-julia-set.html#running-multi-threaded-julia-codes-on-a-production-cluster",
    "title": "Parallelizing the Julia set with ThreadsX",
    "section": "Running multi-threaded Julia codes on a production cluster",
    "text": "Running multi-threaded Julia codes on a production cluster\nBefore we jump to multi-processing in Julia, let us remind you how to run multi-threaded Julia codes on an HPC cluster.\n#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=...\n#SBATCH --mem-per-cpu=3600M\n#SBATCH --time=00:10:00\n#SBATCH --account=def-user\nmodule load julia\njulia -t $SLURM_CPUS_PER_TASK juliaSetThreadsX.jl\nThere may be some other lines after loading the Julia module, e.g. setting some variables, if you have installed packages into a non-standard location (see our introduction).\nRunning the last example on Cedar cluster with julia/1.7.0, @btime reported 2.467 s (serial) and 180.003 ms (16 cores) – 13.7X speedup."
  },
  {
    "objectID": "julia/threadsx-slow-series.html",
    "href": "julia/threadsx-slow-series.html",
    "title": "Multi-threading with ThreadsX",
    "section": "",
    "text": "As you saw in the previous section, Base.Threads does not have a built-in parallel reduction. You can implement it yourself by hand, but all solutions are somewhat awkward, and you can run into problems with thread safety and performance (slow atomic variables, false sharing, etc) if you don’t pay attention.\nEnter ThreadsX, a multi-threaded Julia library that provides parallel versions of some of the Base functions. To see the list of supported functions, use the double-TAB feature inside REPL:\nAs you see in this example, not all functions in ThreadsX are well-documented, but this is exactly the point: they reproduce the functionality of their Base serial equivalents, so you can always look up help on a corresponding serial function.\nConsider this Base function:\nThis function allows an alternative syntax:\nTo parallelize either snippet, replace mapreduce with ThreadsX.mapreduce, assuming you are running Julia with multiple threads. Do not time this code, as this computation is very fast, and the timing will mostly likely be dominated by an overhead from launching and terminating multiple threads. Instead, let’s parallelize and time the slow series."
  },
  {
    "objectID": "julia/threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.mapreduce",
    "href": "julia/threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.mapreduce",
    "title": "Multi-threading with ThreadsX",
    "section": "Parallelizing the slow series with ThreadsX.mapreduce",
    "text": "Parallelizing the slow series with ThreadsX.mapreduce\nIn this and other examples we assume that you have already defined digitsin(). Save the following as mapreduce.jl:\nusing BenchmarkTools, ThreadsX\nfunction slow(n::Int64, digits::Int)\n    total = ThreadsX.mapreduce(+,1:n) do i\n        if !digitsin(digits, i)\n            1.0 / i\n        else\n            0.0\n        end\n    end\n    return total\nend\ntotal = @btime slow(Int64(1e9), 9)\nprintln(\"total = \", total)   # total = 14.241913010384215\nWith 8 CPU cores, I see:\n$ julia mapreduce.jl        # runtime with 1 thread: 5.255 s\n$ julia -t 8 mapreduce.jl   # runtime with 8 threads: 900.995 ms\n\nExercise “ThreadsX.1”\nUsing the compact (one-line) if-else notation, shorten this code by four lines. Time the new, shorter code with one and several threads."
  },
  {
    "objectID": "julia/threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.sum",
    "href": "julia/threadsx-slow-series.html#parallelizing-the-slow-series-with-threadsx.sum",
    "title": "Multi-threading with ThreadsX",
    "section": "Parallelizing the slow series with ThreadsX.sum",
    "text": "Parallelizing the slow series with ThreadsX.sum\n?sum\n?Threads.sum\nThe expression in the round brackets below is a generator. It generates a sequence on the fly without storing individual elements, thus taking very little memory.\n(i for i in 1:10)\ncollect(i for i in 1:10)   # construct a vector (this one takes more space)\n[i for i in 1:10]          # functionally the same (vector)\nLet’s use a generator with \\(10^9\\) elements to compute our slow series sum:\nusing BenchmarkTools\n@btime sum(!digitsin(9, i) ? 1.0/i : 0 for i in 1:1_000_000_000)\n   # serial code: 5.061 s, prints 14.2419130103833\nIt is very easy to parallelize:\nusing BenchmarkTools, ThreadsX\n@btime ThreadsX.sum(!digitsin(9, i) ? 1.0/i : 0 for i in 1:1_000_000_000)\n   # with 8 threads: 906.420 ms, prints 14.241913010381973\n\nExercise “ThreadsX.2”\nThe expression [i for i in 1:10 if i%2==1] produces an array of odd integers between 1 and 10. Using this syntax, remove zero terms from the last generator, i.e. write a parallel code for summing the slow series with a generator that contains only non-zero terms. It should run slightly faster than the code with the original generator.\n\n\n\n\nFinally, let’s rewrite our code applying a function to all integers in a range:\nfunction numericTerm(i)\n    !digitsin(9, i) ? 1.0/i : 0\nend\n@btime ThreadsX.sum(numericTerm, 1:Int64(1e9))            # 890.466 ms, same result\n\nExercise “ThreadsX.3”\nRewrite the last code replacing sum with mapreduce. Hint: look up help for mapreduce()."
  },
  {
    "objectID": "julia/threadsx-slow-series.html#other-parallel-functions",
    "href": "julia/threadsx-slow-series.html#other-parallel-functions",
    "title": "Multi-threading with ThreadsX",
    "section": "Other parallel functions",
    "text": "Other parallel functions\nThreadsX provides various parallel functions for sorting. Sorting is intrinsically hard to parallelize, so do not expect 100% parallel efficiency. Let’s take a look at sort!():\nn = Int64(1e8)\nr = rand(Float32, (n));\nr[1:10]      # first 20 elements, same as first(r,10)\nlast(r,10)   # last 10 elements\n\n?sort              # underneath uses QuickSort (for numeric arrays) or MergeSort\n@btime sort!(r);   # 1.391 s, serial sorting\n\nr = rand(Float32, (n));\n@btime ThreadsX.sort!(r);   # 586.541 ms, parallel sorting with 8 threads\n?ThreadsX.sort!             # there is actually a good manual page\n\n# similar speedup for integers\nr = rand(Int32, (n));\n@btime sort!(r);   # 889.817 ms\n\nr = rand(Int32, (n));\n@btime ThreadsX.sort!(r);   # 390.082 ms with 8 threads\nSearching for extrema is much more parallel-friendly:\nn = Int64(1e9)\nr = rand(Int32, (n));        # make sure we have enough memory\n@btime maximum(r)            # 288.200 ms\n@btime ThreadsX.maximum(r)   # 31.879 ms with 8 threads\nFinally, another useful function is ThreadsX.map() without reduction – we will take a closer look at it in one of the following sections.\nTo sum up this section, ThreadsX.jl provides a super easy way to parallelize some of the Base library functions. It includes multi-threaded reduction and shows very impressive parallel performance. To list the supported functions, use ThreadsX.<TAB>, and don’t forget to use the built-in help pages."
  },
  {
    "objectID": "ml/audio_dataloader.html",
    "href": "ml/audio_dataloader.html",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "ml/audio_dataloader.html#download-and-unzip-data",
    "href": "ml/audio_dataloader.html#download-and-unzip-data",
    "title": "Creating an audio DataLoader",
    "section": "Download and unzip data",
    "text": "Download and unzip data\nPyTorch comes with many classic datasets.\n\nExamples:\n\nlist of available datasets for vision\nlist of audio datasets\nlist of texts datasets\n\n\nThis is convenient to develop and test your model, or to compare its performance with existing models using these datasets.\nHere, we will use the YESNO dataset which can be accessed through the torchaudio.datasets.YESNO class:\nhelp(torchaudio.datasets.YESNO)\nHelp on class YESNO in module torchaudio.datasets.yesno:\n\nclass YESNO(torch.utils.data.dataset.Dataset)\n\n |  YESNO(root: Union[str, pathlib.Path], url: str =\n |    'http://www.openslr.org/resources/1/waves_yesno.tar.gz', \n |    folder_in_archive: str = 'waves_yesno', \n |    download: bool = False) -> None\n |  \n |  Args:\n |    root (str or Path): Path to the directory where the dataset is found \n |      or downloaded.\n |    url (str, optional): The URL to download the dataset from.\n |      (default: \"http://www.openslr.org/resources/1/waves_yesno.tar.gz\")\n |    folder_in_archive (str, optional):\n |      The top-level directory of the dataset. (default: \"waves_yesno\")\n |    download (bool, optional):\n |      Whether to download the dataset if it is not found at root path. \n |      (default: False).\nThe root argument sets the location of the downloaded data.\n\nWhere to store this data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\nIn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-<group>, where <group> is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-<group>.\n\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus use ~/projects/def-sponsor00/data as the root argument for torchaudio.datasets.yesno):\nyesno_data = torchaudio.datasets.YESNO(\n    '~/projects/def-sponsor00/data/',\n    download=True)"
  },
  {
    "objectID": "ml/audio_dataloader.html#explore-the-data",
    "href": "ml/audio_dataloader.html#explore-the-data",
    "title": "Creating an audio DataLoader",
    "section": "Explore the data",
    "text": "Explore the data\nA data point in YESNO is a tuple of waveform, sample_rate, and labels (the labels are 1 for “yes” and 0 for “no”).\nLet’s have a look at the first data point:\n\nyesno_data[0]\n\n(tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n          -2.2583e-03, -1.3733e-03]]),\n 8000,\n [0, 0, 0, 0, 1, 1, 1, 1])\n\n\nOr, more nicely:\n\nwaveform, sample_rate, labels = yesno_data[0]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))\n\nWaveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n         -2.2583e-03, -1.3733e-03]])\nSample rate: 8000\nLabels: [0, 0, 0, 0, 1, 1, 1, 1]\n\n\nYou can also plot the data. For this, we will use pyplot from matplotlib.\nLet’s look at the waveform:\n\nplt.figure()\nplt.plot(waveform.t().numpy())"
  },
  {
    "objectID": "ml/audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "href": "ml/audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "title": "Creating an audio DataLoader",
    "section": "Split the data into a training set and a testing set",
    "text": "Split the data into a training set and a testing set\n\ntrain_size = int(0.8 * len(yesno_data))\ntest_size = len(yesno_data) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(yesno_data, [train_size, test_size])"
  },
  {
    "objectID": "ml/audio_dataloader.html#create-training-and-testing-dataloaders",
    "href": "ml/audio_dataloader.html#create-training-and-testing-dataloaders",
    "title": "Creating an audio DataLoader",
    "section": "Create training and testing DataLoaders",
    "text": "Create training and testing DataLoaders\nDataLoaders are Python iterables created by the torch.utils.data.DataLoader class from a dataset and a sampler.\nWe already have a dataset (yesno_data). Now we need a sampler (or sampling strategy) to draw samples from it. The sampling strategy contains the batch size, whether the data get shuffled prior to sampling, the number of workers used if the data is loaded in parallel, etc.\nTo create a training DataLoader with shuffled data and batch size of 1 (the default), we run:\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n\ndata_loader is an iterable of 0.8*60=48 elements (80% of the 60 samples in the YESNO dataset):\n\nlen(train_loader)\n\n48\n\n\nWe do the same to create the testing DataLoader:\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)"
  },
  {
    "objectID": "ml/audio_dataloader.html#why-do-we-need-to-create-a-dataloader",
    "href": "ml/audio_dataloader.html#why-do-we-need-to-create-a-dataloader",
    "title": "Creating an audio DataLoader",
    "section": "Why do we need to create a DataLoader?",
    "text": "Why do we need to create a DataLoader?\nA DataLoader is the iterable that “presents” data to a model. When we train a model, we run it for each element of the DataLoader in a for loop:\nfor i in data_loader:\n    <run some model>"
  },
  {
    "objectID": "ml/autograd.html",
    "href": "ml/autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Imagine how hard it would be to write the chain rules of neural networks (with so many derivatives!) in backpropagation manually.\nPyTorch has automatic differentiation abilities—meaning that it can track all the operations conducted on tensors and do the backprop for you—thanks to its package torch.autograd.\nLet’s have a first look at it."
  },
  {
    "objectID": "ml/autograd.html#tracking-computations",
    "href": "ml/autograd.html#tracking-computations",
    "title": "Automatic differentiation",
    "section": "Tracking computations",
    "text": "Tracking computations\nPyTorch does not track all the computations on all the tensors (this would be extremely memory intensive!). To start tracking computations on a vector, set the .requires_grad attribute to True:\nimport torch\n\nx = torch.rand(3, 7, requires_grad=True)\nprint(x)\n\nThe grad_fun attribute\nWhenever a tensor is created by an operation involving a tracked tensor, it has a grad_fun attribute:\nx = torch.ones(2, 4, requires_grad=True)\nprint(x)\ny = x + 1\nprint(y)\nprint(y.grad_fn)\n\n\nJudicious tracking\nYou don’t want to track more than is necessary. There are multiple ways to avoid tracking what you don’t want to.\nYou can simply stop tracking computations on a tensor with the method detach:\nx = torch.rand(4, 3, requires_grad=True)\nprint(x)\nprint(x.detach_())\nYou can change its requires_grad flag:\nx = torch.rand(4, 3, requires_grad=True)\nprint(x)\nprint(x.requires_grad_(False))\nAlternatively, you can wrap any code you don’t want to track with with torch.no_grad():\nwith torch.no_grad():\n    <some code>"
  },
  {
    "objectID": "ml/autograd.html#calculating-gradients",
    "href": "ml/autograd.html#calculating-gradients",
    "title": "Automatic differentiation",
    "section": "Calculating gradients",
    "text": "Calculating gradients\nAfter you have performed a number of operations on x and obtained a final object (let’s call it loss since in the context of neural networks, the output of the loss function is the starting place of the backpropagation process), you can get the gradient of any object y with:\nloss.backward()\nprint(y.grad)"
  },
  {
    "objectID": "ml/autograd.html#example",
    "href": "ml/autograd.html#example",
    "title": "Automatic differentiation",
    "section": "Example",
    "text": "Example\nLet’s go over a simple example:\n\nlet real be the tensor of some real values\nlet predicted be the tensor given by some model trying to predict these real values after an iteration\n\nWe will calculate the first derivative (first step of the backpropagation) manually and with the torch.autograd package to really understand what that package does.\nLet’s fill real and predicted with random values since we don’t have a real situation with a real network (but let’s make sure to start recording the history of computations performed on predicted):\nreal = torch.rand(3, 8)\nprint(real)\n\npredicted = torch.rand(3, 8, requires_grad=True)\nprint(predicted)\nSeveral loss functions can be used in machine learning, let’s use:\n\\[\\text{loss}=\\sum_{}^{} (\\text{predicted} - \\text{real})^2\\]\nloss = (predicted - real).pow(2).sum()\nNow, to train a model, after each forward-pass, we need to go through the backpropagation to adjust the weights and biases of the model. That means, we need to calculate all the derivatives, starting from the derivative of the predicted values up to the derivatives of the weights and biases.\nHere, we will only do the very first step: calculate the derivative of predicted.\n\nManual derivative calculation\nThe formula for this first derivative, with the loss function we used, is:\n\\[\\text{gradient}_\\text{predicted}=2(\\text{predicted} - \\text{real})\\]\nThere is no point in adding this operation to predicted’s computation history, so we will exclude it with with torch.no_grad():\nwith torch.no_grad():\n    manual_gradient_predicted = 2.0 * (predicted - real)\n\nprint(manual_gradient_predicted)\n\n\nAutomatic derivative calculation\nNow, with torch.autograd:\nloss.backward()\nSince we tracked computations on predicted, we can calculate its gradient with:\nauto_gradient_predicted = predicted.grad\nprint(auto_gradient_predicted)\n\n\nComparison\nThe result is the same, as can be tested with:\nprint(manual_gradient_predicted.eq(auto_gradient_predicted).all())\nThe calculation of this first derivative of backpropagation was simple enough. But to propagate all the derivatives calculations backward through the chain rule would quickly turn into a deep calculus problem.\nWith torch.autograd, calculating the gradients of all the other elements of the network is as simple as calling them with the attribute grad once the function torch.Tensor.backward() has been run."
  },
  {
    "objectID": "ml/checkpoints.html",
    "href": "ml/checkpoints.html",
    "title": "Saving/loading models and checkpointing",
    "section": "",
    "text": "You can save a model by serializing its internal state dictionary. The state dictionary is a Python dictionary that contains the parameters of your model.\ntorch.save(model.state_dict(), \"model.pth\")"
  },
  {
    "objectID": "ml/checkpoints.html#loading-models",
    "href": "ml/checkpoints.html#loading-models",
    "title": "Saving/loading models and checkpointing",
    "section": "Loading models",
    "text": "Loading models\nTo recreate your model, you first need to recreate its structure:\nmodel = Net()\nThen you can load the state dictionary containing the parameters values into it:\nmodel.load_state_dict(torch.load(\"model.pth\"))"
  },
  {
    "objectID": "ml/checkpoints.html#create-a-checkpoint",
    "href": "ml/checkpoints.html#create-a-checkpoint",
    "title": "Saving/loading models and checkpointing",
    "section": "Create a checkpoint",
    "text": "Create a checkpoint\ntorch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            ...\n            }, PATH)"
  },
  {
    "objectID": "ml/checkpoints.html#resume-training-from-a-checkpoint",
    "href": "ml/checkpoints.html#resume-training-from-a-checkpoint",
    "title": "Saving/loading models and checkpointing",
    "section": "Resume training from a checkpoint",
    "text": "Resume training from a checkpoint\nmodel = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.train()"
  },
  {
    "objectID": "ml/choosing_frameworks.html",
    "href": "ml/choosing_frameworks.html",
    "title": "Which framework to choose?",
    "section": "",
    "text": "With the growing popularity of machine learning, many frameworks have appeared in various languages. One of the questions you might be facing is: which tool should I choose?\nThe main focus here is on the downsides of proprietary tools."
  },
  {
    "objectID": "ml/choosing_frameworks.html#points-worth-considering",
    "href": "ml/choosing_frameworks.html#points-worth-considering",
    "title": "Which framework to choose?",
    "section": "Points worth considering",
    "text": "Points worth considering\nThere are a several points you might want to consider in making that choice. For instance, what tools do people use in your field? (what tools are used in the papers you read?) What tools are your colleagues and collaborators using?\nLooking a bit further into the future, whether you are considering staying in academia or working in industry could also influence your choice. If the former, paying attention to the literature is key, if the latter, it may be good to have a look at the trends in job postings.\nSome frameworks offer collections of already-made toolkits. They are thus easy to start using and do not require a lot of programming experience. On the other hand, they may feel like black boxes and they are not very customizable. Scikit-learn and Keras—usually run on top of TensorFlow—fall in that category. Lower level tools allow you full control and tuning of your models, but can come with a steeper learning curve.\nPyTorch, developed by Facebook’s AI Research lab, has seen a huge increase in popularity in research in recent years due to its highly pythonic syntax, very convenient tensors, just-in-time (JIT) compilation, dynamic computation graphs, and because it is free and open-source.\nSeveral libraries are now adding a higher level on top of PyTorch: fastai, which we will use in this course, PyTorch Lightning, and PyTorch Ignite. fastai, in addition to the convenience of being able to write a model in a few lines of code, allows to dive as low as you choose into the PyTorch code, thus making it unconstrained by the optional ease of use. It also adds countless functionality. The downside of using this added layer is that it can make it less straightforward to install on a machine or to tweak and customize.\nThe most popular machine learning library currently remains TensorFlow, developed by the Google Brain Team. While it has a Python API, its syntax can be more obscure.\nJulia’s syntax is well suited for the implementation of mathematical models, GPU kernels can be written directly in Julia, and Julia’s speed is attractive in computation hungry fields. So Julia has also seen the development of many ML packages such as Flux or Knet. The user base of Julia remains quite small however.\nMy main motivation in writing this section however is to raise awareness about one question that should really be considered: whether the tool you decide to learn and use in your research is open-source or proprietary."
  },
  {
    "objectID": "ml/choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "href": "ml/choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "title": "Which framework to choose?",
    "section": "Proprietary tools: a word of caution",
    "text": "Proprietary tools: a word of caution\nAs a student, it is tempting to have the following perspective:\n\nMy university pays for this very expensive license. I have free access to this very expensive tool. It would be foolish not to make use of it while I can!\n\nWhen there are no equivalent or better open-source tools, that might be true. But when superior open-source tools exist, these university licenses are more of a trap than a gift.\nHere are some of the reasons you should be wary of proprietary tools:\n\nResearchers who do not have access to the tool cannot reproduce your methods\nLarge Canadian universities may offer a license for the tool, but grad students in other countries, independent researchers, researchers in small organizations, etc. may not have access to a free license (or tens of thousands of dollars to pay for it).\n\n\nOnce you graduate, you may not have access to the tool anymore\nOnce you leave your Canadian institution, you may become one of those researchers who do not have access to that tool. This means that you will not be able to re-run your thesis analyses, re-use your methods, and apply the skills you learnt. The time you spent learning that expensive tool you could play with for free may feel a lot less like a gift then.\n\n\nYour university may stop paying for a license\nAs commercial tools fall behind successful open-source ones, some universities may decide to stop purchasing a license. It happened during my years at SFU with an expensive and clunky citation manager which, after having been promoted for years by the library through countless workshops, got abandoned in favour of a much better free and open-source one.\n\n\nYou may get locked-in\nProprietary tools often come with proprietary formats and, depending on the tool, it may be painful (or impossible) to convert your work to another format. When that happens, you are locked-in.\n\n\nProprietary tools are often black boxes\nIt is often impossible to see the source code of proprietary software.\n\n\nLong-term access\nIt is often very difficult to have access to old versions of proprietary tools (and this can be necessary to reproduce old studies). When companies disappear, the tools they produced usually disappear with them. open-source tools, particularly those who have their code under version control in repositories such as GitHub, remain fully accessible (including all stages of development), and if they get abandoned, their maintenance can be taken over or restarted by others.\n\n\nThe licenses you have access to may be limiting and a cause of headache\nFor instance, the Alliance does not have an unlimited number of MATLAB licenses. Since these licenses come with complex rules (one license needed for each node, additional licenses for each toolbox, additional licenses for newer tools, etc.), it can quickly become a nightmare to navigate through it all. You may want to have a look at some of the comments in this thread.\n\n\nProprietary tools fall behind popular open-source tools\nEven large teams of software engineers cannot compete against an active community of researchers developing open-source tools. When open-source tools become really popular, the number of users contributing to their development vastly outnumbers what any company can provide. The testing, licensing, and production of proprietary tools are also too slow to keep up with quickly evolving fields of research. (Of course, open-source tools which do not take off and remain absolutely obscure do not see the benefit of a vast community.)\n\n\nProprietary tools often fail to address specialized edge cases needed in research\nIt is not commercially sound to develop cutting edge capabilities so specialized in a narrow subfield that they can only target a minuscule number of customers. But this is often what research needs. With open-source tools, researchers can develop the capabilities that fit their very specific needs. So while commercial tools are good and reliable for large audiences, they are often not the best in research. This explains the success of R over tools such as SASS or Stata in the past decade."
  },
  {
    "objectID": "ml/choosing_frameworks.html#conclusion",
    "href": "ml/choosing_frameworks.html#conclusion",
    "title": "Which framework to choose?",
    "section": "Conclusion",
    "text": "Conclusion\nAll that said, sometimes you don’t have a choice over the tool to use for your research as this may be dictated by the culture in your field or by your supervisor. But if you are free to choose and if superior or equal open-source alternatives exist and are popular, do not fall in the trap of thinking that because your university and the Alliance pay for a license, you should make use of it. It may be free for you—for now—but it can have hidden costs."
  },
  {
    "objectID": "ml/concept.html",
    "href": "ml/concept.html",
    "title": "Overarching concept of deep learning",
    "section": "",
    "text": "Neural networks learn by adjusting their parameters automatically in an iterative manner. This is derived from Arthur Samuel’s concept.\nIt is important to get a good understanding of this process, so let’s go over it step by step."
  },
  {
    "objectID": "ml/concept.html#decide-on-an-architecture",
    "href": "ml/concept.html#decide-on-an-architecture",
    "title": "Overarching concept of deep learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training. This is set. The type of architecture you choose (e.g. CNN, Transformer, etc.) depends on the type of data you have (e.g. vision, textual, etc.). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ml/concept.html#set-some-initial-parameters",
    "href": "ml/concept.html#set-some-initial-parameters",
    "title": "Overarching concept of deep learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ml/concept.html#get-some-labelled-data",
    "href": "ml/concept.html#get-some-labelled-data",
    "title": "Overarching concept of deep learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models."
  },
  {
    "objectID": "ml/concept.html#make-sure-to-keep-some-data-for-testing",
    "href": "ml/concept.html#make-sure-to-keep-some-data-for-testing",
    "title": "Overarching concept of deep learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ml/concept.html#pass-data-and-parameters-through-the-architecture",
    "href": "ml/concept.html#pass-data-and-parameters-through-the-architecture",
    "title": "Overarching concept of deep learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ml/concept.html#the-outputs-of-the-model-are-predictions",
    "href": "ml/concept.html#the-outputs-of-the-model-are-predictions",
    "title": "Overarching concept of deep learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ml/concept.html#compare-those-predictions-to-the-train-labels",
    "href": "ml/concept.html#compare-those-predictions-to-the-train-labels",
    "title": "Overarching concept of deep learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ml/concept.html#calculate-train-loss",
    "href": "ml/concept.html#calculate-train-loss",
    "title": "Overarching concept of deep learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ml/concept.html#adjust-parameters",
    "href": "ml/concept.html#adjust-parameters",
    "title": "Overarching concept of deep learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation.\nThis is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ml/concept.html#from-model-to-program",
    "href": "ml/concept.html#from-model-to-program",
    "title": "Overarching concept of deep learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process.\nWhile the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.\n\nWhen the training is over, the parameters become fixed. Which means that our model now behaves like a classic program."
  },
  {
    "objectID": "ml/concept.html#evaluate-the-model",
    "href": "ml/concept.html#evaluate-the-model",
    "title": "Overarching concept of deep learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ml/concept.html#use-the-model",
    "href": "ml/concept.html#use-the-model",
    "title": "Overarching concept of deep learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs. This is when we put our model to actual use to solve some problem."
  },
  {
    "objectID": "ml/high_level_frameworks.html",
    "href": "ml/high_level_frameworks.html",
    "title": "High-level frameworks for PyTorch",
    "section": "",
    "text": "Several popular higher-level frameworks are built on top of PyTorch and make the code easier to write and run:\nThe following tag trends on Stack Overflow might give an idea of the popularity of these frameworks over time (catalyst doesn’t have any Stack Overflow tag):\nIf this data is to be believed, ignite never really took off (it also has a lower number of stars on GitHub), fast-ai was extremely popular when it came out, but its usage is going down, and PyTorch-lightning is currently the most popular."
  },
  {
    "objectID": "ml/high_level_frameworks.html#should-you-use-one-and-which-one",
    "href": "ml/high_level_frameworks.html#should-you-use-one-and-which-one",
    "title": "High-level frameworks for PyTorch",
    "section": "Should you use one (and which one)?",
    "text": "Should you use one (and which one)?\nLearning raw PyTorch is probably the best option for research. PyTorch is stable and here to stay. Higher-level frameworks may rise and drop in popularity and today’s popular one may see little usage tomorrow.\nRaw PyTorch is also the most flexible, the closest to the actual computations happening in your model, and probably the easiest to debug.\nDepending on your deep learning trajectory, you might find some of these tools useful though:\n\nIf you work in industry, you might want or need to get results quickly\nSome operations (e.g. parallel execution on multiple GPUs) can be tricky in raw PyTorch, while being extremely streamlined when using e.g. PyTorch-lightning\nEven in research, it might make sense to spend more time thinking about the structure of your model and the functioning of a network instead of getting bogged down in writing code\n\n\nBefore moving to any of these tools, it is probably a good idea to get a good knowledge of raw PyTorch: use these tools to simplify your workflow, not cloud your understanding of what your code is doing."
  },
  {
    "objectID": "ml/hpc.html",
    "href": "ml/hpc.html",
    "title": "Machine learning on production clusters",
    "section": "",
    "text": "This lesson is a summary of relevant information while using Python in an HPC context for deep learning.\nWhen you ssh into one of the Alliance clusters, you log into the login node.\nEverybody using a cluster uses that node to enter the cluster. Do not run anything computationally intensive on this node or you would make the entire cluster very slow for everyone. To run your code, you need to start an interactive job or submit a batch job to Slurm (the job scheduler used by the Alliance clusters)."
  },
  {
    "objectID": "ml/hpc.html#plots",
    "href": "ml/hpc.html#plots",
    "title": "Machine learning on production clusters",
    "section": "Plots",
    "text": "Plots\nDo not run code that displays plots on screen. Instead, have them written to files."
  },
  {
    "objectID": "ml/hpc.html#data",
    "href": "ml/hpc.html#data",
    "title": "Machine learning on production clusters",
    "section": "Data",
    "text": "Data\n\nCopy files to/from the cluster\n\nFew files\nIf you need to copy files to or from the cluster, you can use scp from your local machine.\n\nCopy file from your computer to the cluster\n[local]$ scp </local/path/to/file> <user>@<hostname>:<path/in/cluster>\n\nExpressions between the < and > signs need to be replaced by the relevant information (without those signs).\n\n\n\nCopy file from the cluster to your computer\n[local]$ scp <user>@<hostname>:<cluster/path/to/file> </local/path>\n\n\n\nLarge amount of data\nUse Globus for large data transfers.\n\nThe Alliance is starting to store classic ML datasets on its clusters. So if your research uses a common dataset, it may be worth inquiring whether it might be available before downloading a copy.\n\n\n\n\nLarge collections of files\nThe Alliance clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with tar. Failing to do so will affect performance not just for you, but for all users of the cluster.\n$ tar cf <data>.tar <path/to/dataset/directory>/*\n\n\nIf you want to also compress the files, replace tar cf with tar czf\nAs a modern alternative to tar, you can use Dar"
  },
  {
    "objectID": "ml/hpc.html#interactive-jobs",
    "href": "ml/hpc.html#interactive-jobs",
    "title": "Machine learning on production clusters",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for code testing and development. They are not however the most efficient way to run code, so you should limit their use to testing and development.\nYou start an interactive job with:\n$ salloc --account=def-<account> --cpus-per-task=<n> --gres=gpu:<n> --mem=<mem> --time=<time>\nOur training cluster does not have GPUs, so for this workshop, do not use the --gres=gpu:<n> option.\nFor the workshop, you also don’t have to worry about the --account=def-<account> option (or, if you want, you can use --account=def-sponsor00).\nOur training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, please be very mindful when running interactive jobs: if you request a lot of CPUs for a long time, the other workshop attendees won’t be able to use the cluster anymore until your interactive job requested time ends (even if you aren’t running any code).\nHere are my suggestions so that we don’t run into this problem:\n\nOnly start interactive jobs when you need to understand what Python is doing at every step, or to test, explore, and develop code (so where an interactive Python shell is really beneficial). Once you have a model, submit a batch job to Slurm instead\nWhen running interactive jobs on this training cluster, only request 1 CPU (so --cpus-per-task=1)\nOnly request the time that you will really use (e.g. for the lesson on Python tensors, maybe 30 min to 1 hour seems reasonable)\nIf you don’t need your job allocation anymore before it runs out, you can relinquish it with Ctrl+d\n\n\nBe aware that, on Cedar, you are not allowed to submit jobs from ~/home. Instead, you have to submit jobs from ~/scratch or ~/project."
  },
  {
    "objectID": "ml/hpc.html#batch-jobs",
    "href": "ml/hpc.html#batch-jobs",
    "title": "Machine learning on production clusters",
    "section": "Batch jobs",
    "text": "Batch jobs\nAs soon as you have a working Python script, you want to submit a batch job instead of running an interactive job. To do that, you need to write an sbatch script.\n\nJob script\n\nHere is an example script:\n\n#!/bin/bash\n#SBATCH --job-name=<name>*            # job name\n#SBATCH --account=def-<account>\n#SBATCH --time=<time>                 # max walltime in D-HH:MM or HH:MM:SS\n#SBATCH --cpus-per-task=<number>      # number of cores\n#SBATCH --gres=gpu:<type>:<number>    # type and number of GPU(s) per node\n#SBATCH --mem=<mem>                   # max memory (default unit is MB) per node\n#SBATCH --output=%x_%j.out*           # file name for the output\n#SBATCH --error=%x_%j.err*            # file name for errors\n#SBATCH --mail-user=<email_address>*\n#SBATCH --mail-type=ALL*\n\n# Load modules\n# (Do not use this in our workshop since we aren't using GPUs)\n# (Note: loading the Python module is not necessary\n# when you activate a Python virtual environment)\n# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5\n\n# Create a variable with the directory for your ML project\nSOURCEDIR=~/<path/project/dir>\n\n# Activate your Python virtual environment\nsource ~/env/bin/activate\n\n# Transfer and extract data to a compute node\nmkdir $SLURM_TMPDIR/data\ntar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\n\n# Run your Python script on the data\npython $SOURCEDIR/<script>.py $SLURM_TMPDIR/data\n\n\n%x will get replaced by the script name and %j by the job number\nIf you compressed your data with tar czf, you need to extract it with tar xzf\nSBATCH options marked with a * are optional\nThere are various other options for email notifications\n\n\nYou may wonder why we transferred data to a compute node. This makes any I/O operation involving your data a lot faster, so it will speed up your code. Here is how this works:\nFirst, we create a temporary data directory in $SLURM_TMPDIR:\n$ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/<user>.<jobid>.0. Anything in it gets deleted when the job is done.\n\nThen we extract the data into it:\n$ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\nIf your data is not in a tar file, you can simply copy it to the compute node running your job:\n$ cp -r ~/projects/def-<user>/<data> $SLURM_TMPDIR/data\n\n\nJob handling\n\nSubmit a job\n$ cd </dir/containing/job>\n$ sbatch <jobscript>.sh\n\n\nCheck the status of your job(s)\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\n\n\nCancel a job\n$ scancel <jobid>\n\n\nDisplay efficiency measures of a completed job\n$ seff <jobid>"
  },
  {
    "objectID": "ml/hpc.html#gpus",
    "href": "ml/hpc.html#gpus",
    "title": "Machine learning on production clusters",
    "section": "GPU(s)",
    "text": "GPU(s)\n\nGPU types\nSeveral Alliance clusters have GPUs. Their numbers and types differ:\n From the Alliance Wiki\nThe default is 12G P100, but you can request another type with SBATCH --gres=gpu:<type>:<number> (example: --gres=gpu:p100l:1 to request a 16G P100 on Cedar). Please refer to the Alliance Wiki for more details.\n\n\nNumber of GPU(s)\nTry running your model on a single GPU first.\nIt is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The lesson on distributed computing with PyTorch gives a few information as to when you might benefit from using several GPUs and provides some links to more resources. We will also offer workshops on distributed ML in the future. In any event, you should test your model before asking for several GPUs.\n\n\nCPU/GPU ratio\nHere are the Alliance recommendations:\nBéluga:\nNo more than 10 CPU per GPU.\nCedar:\nP100 GPU: no more than 6 CPU per GPU.\nV100 GPU: no more than 8 CPU per GPU.\nGraham:\nNo more than 16 CPU per GPU."
  },
  {
    "objectID": "ml/hpc.html#code-testing",
    "href": "ml/hpc.html#code-testing",
    "title": "Machine learning on production clusters",
    "section": "Code testing",
    "text": "Code testing\nIt might be wise to test your code in an interactive job before submitting a really big batch job to Slurm.\n\nActivate your Python virtual environment\n$ source ~/env/bin/activate\n\n\nStart an interactive job\n\nExample:\n\n$ salloc --account=def-<account> --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=0:30:0\n\n\nPrepare the data\nCreate a temporary data directory in $SLURM_TMPDIR:\n(env) $ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/<user>.<jobid>.0. Anything in it gets deleted when the job is done.\n\nExtract the data into it:\n(env) $ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\n\n\nTry to run your code\nPlay in Python to test your code:\n(env) $ python\n>>> import torch\n>>> ...\nTo exit the virtual environment, run:\n(env) $ deactivate"
  },
  {
    "objectID": "ml/hpc.html#checkpoints",
    "href": "ml/hpc.html#checkpoints",
    "title": "Machine learning on production clusters",
    "section": "Checkpoints",
    "text": "Checkpoints\nLong jobs should have a checkpoint at least every 24 hours. This ensures that an outage won’t lead to days of computation lost and it will help get the job started by the scheduler sooner.\nFor instance, you might want to have checkpoints every n epochs (choose n so that n epochs take less than 24 hours to run).\nIn PyTorch, you can create dictionaries with all the information necessary and save them as .tar files with torch.save(). You can then load them back with torch.load().\nThe information you want to save in each checkpoint includes the model’s state_dict, the optimizer’s state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.\n\nFor example, saving a checkpoint during training could look something like this:\n\ntorch.save({\n    'epoch': <last epoch run>,\n    'model_state_dict': net.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': <latest loss>,\n}, <path/to/checkpoint-file.tar>)\n\nTo restart, initialize the model and optimizer, load the dictionary, and resume training:\n\n# Initialize the model and optimizer\nmodel = <your model>\noptimizer = <your optimizer>\n\n# Load the dictionary\ncheckpoint = torch.load(<path/to/checkpoint-file.tar>)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# Resume training\nmodel.train()"
  },
  {
    "objectID": "ml/hpc.html#tensorboard-on-the-cluster",
    "href": "ml/hpc.html#tensorboard-on-the-cluster",
    "title": "Machine learning on production clusters",
    "section": "TensorBoard on the cluster",
    "text": "TensorBoard on the cluster\nTensorBoard allows to visually track your model metrics (e.g. loss, accuracy, model graph, etc.). It requires a lot of processing power however, so if you want to use it on an Alliance cluster, do not run it from the login node. Instead, run it as part of your job. This section guides you through the whole workflow.\n\nLaunch TensorBoard\nFirst, you need to launch TensorBoard in the background (with a trailing &) before running your Python script. To do so, ad to your sbatch script:\ntensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n...\n\ntensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &\npython $SOURCEDIR/<script>.py $SLURM_TMPDIR/data\n\n\nCreate a connection between the compute node and your computer\nOnce the job is running, you need to create a connection between the compute node running TensorBoard and your computer.\nFirst, you need to find the hostname of the compute node running the Tensorboard server. This is the value under NODELIST for your job when you run:\n$ sq\nThen, from your computer, enter this ssh command:\n[local]$ ssh -N -f -L localhost:6006:<node hostname>:6006 <user>@<cluster>.computecanada.ca\n\nReplace <node hostname> by the compute node hostname you just identified, <user> by your user name, and <cluster> by the name of the Alliance cluster hostname—e.g. beluga, cedar, graham.\n\n\n\nAccess TensorBoard\nYou can now open a browser (on your computer) and go to http://localhost:6006 to monitor your model running on a compute node in the cluster!"
  },
  {
    "objectID": "ml/hpc.html#running-several-similar-jobs",
    "href": "ml/hpc.html#running-several-similar-jobs",
    "title": "Machine learning on production clusters",
    "section": "Running several similar jobs",
    "text": "Running several similar jobs\nA number of ML tasks (e.g. hyperparameter optimization) require running several instances of similar jobs. Grouping them into a single job with GLOST or GNU Parallel reduces the stress on the scheduler."
  },
  {
    "objectID": "ml/index.html",
    "href": "ml/index.html",
    "title": "Deep learning with PyTorch",
    "section": "",
    "text": "Date:\nThursday May 4, 2023\nTime:\n9am–5pm (with a two-hour break from noon to 2pm)\nInstructor:\nMarie-Hélène Burle (Simon Fraser University)\nPrerequisites:\nBasic knowledge of Python.\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there)."
  },
  {
    "objectID": "ml/mnist.html",
    "href": "ml/mnist.html",
    "title": "Classifying the MNIST dataset",
    "section": "",
    "text": "In this workshop, we will classify the MNIST dataset—a classic of machine learning—with PyTorch."
  },
  {
    "objectID": "ml/mnist.html#the-mnist-dataset",
    "href": "ml/mnist.html#the-mnist-dataset",
    "title": "Classifying the MNIST dataset",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nThe MNIST is a classic dataset commonly used for testing machine learning systems. It consists of pairs of images of handwritten digits and their corresponding labels.\nThe images are composed of 28x28 pixels of greyscale RGB codes ranging from 0 to 255 and the labels are the digits from 0 to 9 that each image represents.\nThere are 60,000 training pairs and 10,000 testing pairs.\nThe goal is to build a neural network which can learn from the training set to properly identify the handwritten digits and which will perform well when presented with the testing set that it has never seen. This is a typical case of supervised learning.\n\nNow, let’s explore the MNIST with PyTorch."
  },
  {
    "objectID": "ml/mnist.html#download-unzip-and-transform-the-data",
    "href": "ml/mnist.html#download-unzip-and-transform-the-data",
    "title": "Classifying the MNIST dataset",
    "section": "Download, unzip, and transform the data",
    "text": "Download, unzip, and transform the data\n\nWhere to store the data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-<group>, where <group> is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-<group>.\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus all access the MNIST data in ~/projects/def-sponsor00/data.\n\n\nHow to obtain the data?\nThe dataset can be downloaded directly from the MNIST website, but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\nhelp(torchvision.datasets.MNIST)\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -> None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\nNote that here too, the root argument sets the location of the downloaded data and we will use /project/def-sponsor00/data/.\n\n\nPrepare the data\nFirst, let’s load the needed libraries:\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n\nThe MNIST dataset already consists of a training and a testing sets, so we don’t have to split the data manually. Instead, we can directly create 2 different objects with the same function (train=True selects the train set and train=False selects the test set).\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: 0.1307 and 0.3081 respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\nSo we first need to define a transformation:\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n\n\nWe can now create our data objects\n\nTraining data\n\nRemember that train=True selects the training set of the MNIST.\n\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\n\n\nTest data\n\ntrain=False selects the test set.\n\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)"
  },
  {
    "objectID": "ml/mnist.html#exploring-the-data",
    "href": "ml/mnist.html#exploring-the-data",
    "title": "Classifying the MNIST dataset",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nData inspection\nFirst, let’s check the size of train_data:\n\nprint(len(train_data))\n\n60000\n\n\nThat makes sense since the MNIST’s training set has 60,000 pairs. train_data has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let’s double-check with the first element:\n\nprint(len(train_data[0]))\n\n2\n\n\nSo far, so good. We can print that first pair:\n\nprint(train_data[0])\n\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n\n\nAnd you can see that it is a tuple with:\n\nprint(type(train_data[0]))\n\n<class 'tuple'>\n\n\nWhat is that tuple made of?\n\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n\n<class 'torch.Tensor'>\n<class 'int'>\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects train_data and test_data) and the integer of the first label (which you can see is 5 when you print train_data[0][1]).\nSo since train_data[0][0] is the tensor representing the image of the first pair, let’s check its size:\n\nprint(train_data[0][0].size())\n\ntorch.Size([1, 28, 28])\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values—the values of each pixel on a gray scale—so the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n\nYour turn:\n\nRun the following:\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\nAnd think about what each of them represents.\nThen explore the test_data object.\n\n\n\nPlotting an image from the data\nFor this, we will use pyplot from matplotlib.\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with torch.squeeze:\nimg = torch.squeeze(train_data[0][0])\nThen, we plot it with pyplot, but since we are in a cluster, instead of showing it to screen with plt.show(), we save it to file:\nplt.imshow(img, cmap='gray')\nThis is what that first image looks like:\n\nAnd indeed, it matches the first label we explored earlier (train_data[0][1]).\n\n\nPlotting an image with its pixel values\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() < thresh else 'black')"
  },
  {
    "objectID": "ml/mnist.html#batch-processing",
    "href": "ml/mnist.html#batch-processing",
    "title": "Classifying the MNIST dataset",
    "section": "Batch processing",
    "text": "Batch processing\nPyTorch provides the torch.utils.data.DataLoader class which combines a dataset and an optional sampler and provides an iterable (while training or testing our neural network, we will iterate over that object). It allows, among many other things, to set the batch size and shuffle the data.\nSo our last step in preparing the data is to pass it through DataLoader.\n\nCreate DataLoaders\n\nTraining data\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\n\nTest data\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n\n\n\nPlot a full batch of images with their labels\nNow that we have passed our data through DataLoader, it is easy to select one batch from it. Let’s plot an entire batch of images with their labels.\nFirst, we need to get one batch of training images and their labels:\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\nThen, we can plot them:\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)"
  },
  {
    "objectID": "ml/mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "href": "ml/mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "title": "Classifying the MNIST dataset",
    "section": "Time to build a NN to classify the MNIST",
    "text": "Time to build a NN to classify the MNIST\nLet’s build a multi-layer perceptron (MLP): the simplest neural network. It is a feed-forward (i.e. no loop), fully-connected (i.e. each neuron of one layer is connected to all the neurons of the adjacent layers) neural network with a single hidden layer.\n\n\nLoad packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nThe torch.nn.functional module contains all the functions of the torch.nn package.\nThese functions include loss functions, activation functions, pooling functions…\n\n\nCreate a SummaryWriter instance for TensorBoard\nwriter = SummaryWriter()\n\n\nDefine the architecture of the network\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nPython’s class inheritance gives our subclass all the functionality of torch.nn.Module while allowing us to customize it.\n\n\nDefine a training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\nDefine a testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nDefine a function main() which runs our network\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n\nRun the network\nmain()\n\n\nWrite pending events to disk and close the TensorBoard\nwriter.flush()\nwriter.close()\nThe code is working. Time to actually train our model!\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\nLastly, you will go through your allocation quickly.\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with salloc) or in Jupyter, then, launch an sbatch job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.\n\nConcrete example with our training cluster: this cluster only has 1 GPU. If you want to use it in Jupyter, you have to request it for your Jupyter session. This means that the entire time your Jupyter session is active, nobody else can use that GPU. While you let your session idle or do tasks that do not require a GPU, this is not a good use of resources."
  },
  {
    "objectID": "ml/mnist.html#lets-train-and-test-our-model",
    "href": "ml/mnist.html#lets-train-and-test-our-model",
    "title": "Classifying the MNIST dataset",
    "section": "Let’s train and test our model",
    "text": "Let’s train and test our model\n\nLog in the training cluster\nOpen a terminal and SSH to our training cluster as we saw in the first lesson.\n\n\nLoad necessary modules\nFirst, we need to load the Python and CUDA modules. This is done with the Lmod tool through the module command. Here are some key Lmod commands:\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail <tool>\n\n# Load a module\n$ module load <module>[/<version>]\nHere are the modules we need:\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2\n\n\nInstall Python packages\nYou also need the Python packages matplotlib, torch, torchvision, and tensorboard.\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with pip.\n\nDo not use Anaconda.\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in $HOME where it creates a very large number of small files. It can also create conflicts by modifying .bashrc.\n\nFor this workshop, since we all need the same packages, I already created a virtual environment that we will all use. All you have to do is to activate it with:\n$ source ~/projects/def-sponsor00/env/bin/activate\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n(env) $ deactivate\n\nFor future reference, below is how you would install packages on a real Alliance cluster (but please don’t do it in the training cluster as it is unnecessary and would only slow it down).\nCreate a virtual environment:\n$ virtualenv --no-download ~/env\nActivate the virtual environment:\n$ source ~/env/bin/activate\nUpdate pip:\n(env) $ pip install --no-index --upgrade pip\nInstall the packages you need in the virtual environment:\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard\n\n\n\nWrite a Python script\nCreate a directory for this project and cd into it:\nmkdir mnist\ncd mnist\nStart a Python script with the text editor of your choice:\nnano nn.py\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()\n\n\nWrite a Slurm script\nWrite a shell script with the text editor of your choice:\nnano nn.sh\nThis is what you want in that script:\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n\n--time accepts these formats: “min”, “min:s”, “h:min:s”, “d-h”, “d-h:min” & “d-h:min:s”\n%x will get replaced by the script name & %j by the job number\n\n\n\nSubmit a job\nFinally, you need to submit your job to Slurm:\n$ sbatch ~/mnist/nn.sh\nYou can check the status of your job with:\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\nYou can cancel it with:\n$ scancel <jobid>\nOnce your job has finished running, you can display efficiency measures with:\n$ seff <jobid>"
  },
  {
    "objectID": "ml/mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "href": "ml/mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "title": "Classifying the MNIST dataset",
    "section": "Let’s explore our model’s metrics with TensorBoard",
    "text": "Let’s explore our model’s metrics with TensorBoard\nTensorBoard is a web visualization toolkit developed by TensorFlow which can be used with PyTorch.\nBecause we have sent our model’s metrics logs to TensorBoard as part of our code, a directory called runs with those logs was created in our ~/mnist directory.\n\nLaunch TensorBoard\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don’t look right).\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\nsalloc --time=1:0:0 --mem=2000M\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by pip):\nsource ~/projects/def-sponsor00/env/bin/activate\nThen we can launch TensorBoard in the background:\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job.\n\n\nConnect to TensorBoard from your computer\nFrom a new terminal on your computer, run:\nssh -NfL localhost:6006:<hostname>:6006 userxxx@uu.c3.ca\n\nReplace <hostname> by the name of the compute node running your salloc job. You can find it by looking at your prompt (your prompt shows <username>@<hostname>).\nReplace <userxxx> by your user name.\n\nNow, you can open a browser on your computer and access TensorBoard at http://localhost:6006."
  },
  {
    "objectID": "ml/nn.html",
    "href": "ml/nn.html",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn.html#types-of-learning",
    "href": "ml/nn.html#types-of-learning",
    "title": "Concepts:",
    "section": "Types of learning",
    "text": "Types of learning\nThere are now more types of learning than those presented here. But these initial types are interesting because they will already be familiar to you.\n\nSupervised learning\nYou have been doing supervised machine learning for years without looking at it in the framework of machine learning:\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output \\((x_i, y_i)\\) pairs.\nGoal:\nIf \\(X\\) is the space of inputs and \\(Y\\) the space of outputs, the goal is to find a function \\(h\\) so that\nfor each \\(x_i \\in X\\):\n\n\\(h_\\theta(x_i)\\) is a predictor for the corresponding value \\(y_i\\)\n\n(\\(\\theta\\) represents the set of parameters of \\(h_\\theta\\)).\n\n→ i.e. we want to find the relationship between inputs and outputs.\n\n\nUnsupervised learning\nHere too, you are familiar with some forms of unsupervised learning that you weren’t thinking about in such terms:\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data (training set of \\(x_i\\)).\nGoal:\nFind structure within the data.\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn.html#artificial-neural-networks",
    "href": "ml/nn.html#artificial-neural-networks",
    "title": "Concepts:",
    "section": "Artificial neural networks",
    "text": "Artificial neural networks\nArtificial neural networks (ANN) are one of the machine learning models (other models include decision trees or Bayesian networks). Their potential and popularity has truly exploded in recent years and this is what we will focus on in this course.\nArtificial neural networks are a series of layered units mimicking the concept of biological neurons: inputs are received by every unit of a layer, computed, then transmitted to units of the next layer. In the process of learning, experience strengthens some connections between units and weakens others.\nIn biological networks, the information consists of action potentials (neuron membrane rapid depolarizations) propagating through the network. In artificial ones, the information consists of tensors (multidimensional arrays) of weights and biases: each unit passes a weighted sum of an input tensor with an additional—possibly weighted—bias through an activation function before passing on the output tensor to the next layer of units.\n\n\nThe bias allows to shift the output of the activation function to the right or to the left (i.e. it creates an offset).\n\nSchematic of a biological neuron:\n\n\nFrom Dhp1080, Wikipedia\n\nSchematic of an artificial neuron:\n\n\nModified from O.C. Akgun & J. Mei 2019\n\nWhile biological neurons are connected in extremely intricate patterns, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65–90 billion neurons. ANN have much fewer units.\nNeurons in mouse cortex:\n\n\nNeurons are in green, the dark branches are blood vessels. Image by Na Ji, UC Berkeley\n\nNeural network with 2 hidden layers:\n\n\nFrom The Maverick Meerkat\n\nThe information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don’t produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function and the output can take any of the values possible with that function.\nThreshold potential in biological neurons:\n\n\nModified from Blacktc, Wikimedia\n\nSome of the most common activation functions in artificial neurons:\n\n\nFrom Diganta Misra 2019\n\nWhich activation function to use depends on the type of problem and the available computing budget. Some early functions have fallen out of use while new ones have emerged (e.g. sigmoid got replaced by ReLU which is easier to train).\n\nLearning\nThe process of learning in biological NN happens through neuron death or growth and through the creation or loss of synaptic connections between neurons. In ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations (cross entropy is the difference between the predicted and the real distributions).\n\n\nFrom xkcd.com\n\n\n\nGradient descent\nThere are several gradient descent methods:\nBatch gradient descent uses all examples in each iteration and is thus slow for large datasets (the parameters are adjusted only after all the samples have been processed).\nMini-batch gradient descent is an intermediate approach: it uses mini-batch sized examples in each iteration. This allows a vectorized approach (and hence parallelization).\nThe Adam optimization algorithm is a popular variation of mini-batch gradient descent.\nStochastic gradient descent uses one example in each iteration. It is thus much faster than batch gradient descent (the parameters are adjusted after each example). But it does not allow any vectorization.\n\n\nFrom Imad Dabbura\n\n\n\n3Blue1Brown by Grant Sanderson videos\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network.\n\nWhat are NN? (19 min)\n\nWatch this video beyond the acknowledgement as the function ReLU (a really important function in modern neural networks) is introduced at the very end.\n\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus.\n\n\n\nHow do NN learn? (21 min)\n\n\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)\n\n\n\n\nTypes of ANN\n\nFully connected neural networks\n\n\nFrom Glosser.ca, Wikipedia\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.\n\n\nConvolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. in image recognition).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters.\nOptionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions. The stride then dictates how the subarea is moved across the image. Max-pooling is one of the forms of pooling which uses the maximum for each subarea.\n\n\nRecurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. in speech recognition).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop).\n\n\nTransformers\nA combination of two RNNs or sets of RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning. Such models were slow to process a lot of data.\nIn 2014 and 2015, the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThis blog post by Jay Alammar—a blogger whose high-quality posts have been referenced in MIT and Stanford courses—explains this in a high-level visual fashion.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nJay Alammar has also a blog post on the transformer. The post includes a 30 min video.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (Natural Language Processing). Examples include BERT (Bidirectional Encoder Representations from Transformers) from Google and GPT-3 (Generative Pre-trained Transformer-3) from OpenAI.\nJay Alammar has yet another great blog post on these advanced NLP models.\n\n\n\nDeep learning\nThe first layer of a neural net is the input layer. The last one is the output layer. All the layers in-between are called hidden layers. Shallow neural networks have only one hidden layer and deep networks have two or more hidden layers. When an ANN is deep, we talk about Deep Learning (DL).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn_building.html",
    "href": "ml/nn_building.html",
    "title": "Building a neural network",
    "section": "",
    "text": "Key to creating neural networks in PyTorch is the torch.nn package which contains the nn.Module and a forward method which returns an output from some input.\nLet’s build a neural network to classify the MNIST."
  },
  {
    "objectID": "ml/nn_building.html#load-packages",
    "href": "ml/nn_building.html#load-packages",
    "title": "Building a neural network",
    "section": "Load packages",
    "text": "Load packages\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
    "objectID": "ml/nn_building.html#define-the-architecture-of-the-network",
    "href": "ml/nn_building.html#define-the-architecture-of-the-network",
    "title": "Building a neural network",
    "section": "Define the architecture of the network",
    "text": "Define the architecture of the network\nFirst, we need to define the architecture of the network.\nThere are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    <definition of your subclass>        \nYour subclass is derived from the base class and inherits its properties.\nPyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\nclass Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      \n      # First 2D convolutional layer, taking in 1 input channel (image),\n      # outputting 32 convolutional features.\n      # Convolution adds each pixel of an image to its neighbours,\n      # weighted by the kernel (a small matrix).\n      # Here, the kernel is square and of size 3*3\n      # Convolution helps to extract features from the input\n      # (e.g. edge detection, blurriness, sharpeness...)\n      self.conv1 = nn.Conv2d(1, 32, 3)\n      # Second 2D convolutional layer, taking in the 32 input channels,\n      # outputting 64 convolutional features, with a kernel size of 3*3\n      self.conv2 = nn.Conv2d(32, 64, 3)\n\n      # Dropouts randomly blocks a fraction of the neurons during training\n      # This is a regularization technique which prevents overfitting\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n\n      # First fully connected layer\n      self.fc1 = nn.Linear(9216, 128)\n      # Second fully connected layer that outputs our 10 labels\n      self.fc2 = nn.Linear(128, 10)"
  },
  {
    "objectID": "ml/nn_building.html#set-the-flow-of-data-through-the-network",
    "href": "ml/nn_building.html#set-the-flow-of-data-through-the-network",
    "title": "Building a neural network",
    "section": "Set the flow of data through the network",
    "text": "Set the flow of data through the network\nThe feed-forward algorithm is defined by the forward function.\n\nclass Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      self.conv1 = nn.Conv2d(1, 32, 3)\n      self.conv2 = nn.Conv2d(32, 64, 3)\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n      self.fc1 = nn.Linear(9216, 128)\n      self.fc2 = nn.Linear(128, 10)\n\n    # x represents the data\n    def forward(self, x):\n      # Pass data through conv1\n      x = self.conv1(x)\n      # Use the rectified-linear activation function over x\n      x = F.relu(x)\n\n      x = self.conv2(x)\n      x = F.relu(x)\n\n      # Run max pooling over x\n      x = F.max_pool2d(x, 2)\n      # Pass data through dropout1\n      x = self.dropout1(x)\n      # Flatten x with start_dim=1\n      x = torch.flatten(x, 1)\n      # Pass data through fc1\n      x = self.fc1(x)\n      x = F.relu(x)\n      x = self.dropout2(x)\n      x = self.fc2(x)\n\n      # Apply softmax to x\n      output = F.log_softmax(x, dim=1)\n      return output\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout2d(p=0.25, inplace=False)\n  (dropout2): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)"
  },
  {
    "objectID": "ml/nn_training.html",
    "href": "ml/nn_training.html",
    "title": "Training a model",
    "section": "",
    "text": "Before we can train a model, we need to:\n\nload the needed packages,\nget the data,\ncreate data loaders for training and testing,\ndefine a model.\n\nLet’s do this for the FashionMNIST dataset:\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\ntraining_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=10)\ntest_dataloader = DataLoader(test_data, batch_size=10)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = Net()"
  },
  {
    "objectID": "ml/nn_training.html#hyperparameters",
    "href": "ml/nn_training.html#hyperparameters",
    "title": "Training a model",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nWhile the learning parameters of a model (weights and biases) are the values that get adjusted through training (and they will become part of the final program, along with the model architecture, once training is over), hyperparameters control the training process.\nThey include:\n\nbatch size: number of samples passed through the model before the parameters are updated,\nnumber of epochs: number iterations,\nlearning rate: size of the incremental changes to model parameters at each iteration. Smaller values yield slow learning speed, while large values may miss minima.\n\nLet’s define them here:\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5"
  },
  {
    "objectID": "ml/nn_training.html#define-the-loss-function",
    "href": "ml/nn_training.html#define-the-loss-function",
    "title": "Training a model",
    "section": "Define the loss function",
    "text": "Define the loss function\nTo assess the predicted outputs of our model against the true values from the labels, we also need a loss function (e.g. mean square error for regressions: nn.MSELoss or negative log likelihood for classification: nn.NLLLoss)\nThe machine learning literature is rich in information about various loss functions.\nHere is an example with nn.CrossEntropyLoss which combines nn.LogSoftmax and nn.NLLLoss:\nloss_fn = nn.CrossEntropyLoss()"
  },
  {
    "objectID": "ml/nn_training.html#initialize-the-optimizer",
    "href": "ml/nn_training.html#initialize-the-optimizer",
    "title": "Training a model",
    "section": "Initialize the optimizer",
    "text": "Initialize the optimizer\nThe optimization algorithm determines how the model parameters get adjusted at each iteration.\nThere are many optimizers and you need to search in the literature which one performs best for your time of model and data.\nBelow is an example with stochastic gradient descent:\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n\nlr is the learning rate\nmomentum is a method increasing convergence rate and reducing oscillation for SDG"
  },
  {
    "objectID": "ml/nn_training.html#define-the-train-and-test-loops",
    "href": "ml/nn_training.html#define-the-train-and-test-loops",
    "title": "Training a model",
    "section": "Define the train and test loops",
    "text": "Define the train and test loops\nFinally, we need to define the train and test loops.\nThe train loop:\n\ngets a batch of training data from the DataLoader,\nresets the gradients of model parameters with optimizer.zero_grad(),\ncalculates predictions from the model for an input batch,\ncalculates the loss for that set of predictions vs. the labels on the dataset,\ncalculates the backward gradients over the learning parameters (that’s the backpropagation) with loss.backward(),\nadjusts the parameters by the gradients collected in the backward pass with optimizer.step().\n\nThe test loop evaluates the model’s performance against the test data.\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
  },
  {
    "objectID": "ml/nn_training.html#train",
    "href": "ml/nn_training.html#train",
    "title": "Training a model",
    "section": "Train",
    "text": "Train\nTo train our model, we just run the loop over the epochs:\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Training completed\")"
  },
  {
    "objectID": "ml/pretrained_models.html",
    "href": "ml/pretrained_models.html",
    "title": "Finding pretrained models for transfer learning",
    "section": "",
    "text": "Training models from scratch requires way too much data, time, and computing power (or money) to be a practical option. This is why transfer learning has become such a common practice: by starting with models trained on related problems, you are saving time and achieving good results with little data.\nNow, where do you find such models?\nIn this workshop, we will have a look at some of the most popular pre-trained models repositories and libraries (Model Zoo, PyTorch Hub, and Hugging Face); see how you can also search models in the literature and on GitHub; and finally learn how to import models into PyTorch.\n\n\nPrerequisites:\nIf you want to follow the hands-on part of this workshop, please make sure to have an up-to-date version of PyTorch on your laptop."
  },
  {
    "objectID": "ml/resources.html",
    "href": "ml/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Alliance wiki ML page\n\n\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal\n\n\n\nAdvice and sources from ML research student\n\n\n\nStack Overflow [machine-learning] tag\nStack Overflow [deep-learning] tag\nStack Overflow [supervised-learning] tag\nStack Overflow [unsupervised-learning] tag\nStack Overflow [semisupervised-learning] tag\nStack Overflow [reinforcement-learning] tag\nStack Overflow [transfer-learning] tag\nStack Overflow [machine-learning-model] tag\nStack Overflow [learning-rate] tag\nStack Overflow [bayesian-deep-learning] tag\n\n\n\ndeeplearning.ai\nfast.ai\nGoogle\n\n\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ml/resources.html#pytorch",
    "href": "ml/resources.html#pytorch",
    "title": "Resources",
    "section": "PyTorch",
    "text": "PyTorch\nAlliance wiki PyTorch page\n\n\nDocumentation\nPyTorch website\nPyTorch documentation\nPyTorch tutorials\nPyTorch online courses\nPyTorch examples\n\n\nGetting help\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\nStack Overflow [pytorch-dataloader] tag\nStack Overflow [pytorch-ignite] tag\n\n\nPre-trained models\nPyTorch Hub"
  },
  {
    "objectID": "ml/resources.html#python",
    "href": "ml/resources.html#python",
    "title": "Resources",
    "section": "Python",
    "text": "Python\nAlliance wiki Python page\n\nIDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\n\nShell\nIPython\nbpython\nptpython\n\n\nGetting help\nStack Overflow [python] tag"
  },
  {
    "objectID": "ml/resources.html#fastai",
    "href": "ml/resources.html#fastai",
    "title": "Resources",
    "section": "fastai",
    "text": "fastai\n\nDocumentation\nManual\nTutorials\nPeer-reviewed paper\n\n\nBook\nPaperback version\nFree MOOC version of part 1 of the book\nJupyter notebooks version of the book\n\n\nGetting help\nDiscourse forum"
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Training events mailing list",
    "section": "",
    "text": "If you want to get informed about upcoming training events offered by SFU on behalf of Western Universities, please subscribe to our mailing list: \n(We will only email you about training events.)"
  },
  {
    "objectID": "r_intro/basics.html",
    "href": "r_intro/basics.html",
    "title": "First steps in R",
    "section": "",
    "text": "In this section, we take our first few steps in R: we will access the R documentation, see how to set R options, and talk about a few concepts."
  },
  {
    "objectID": "r_intro/basics.html#help-and-documentation",
    "href": "r_intro/basics.html#help-and-documentation",
    "title": "First steps in R",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser."
  },
  {
    "objectID": "r_intro/basics.html#r-settings",
    "href": "r_intro/basics.html#r-settings",
    "title": "First steps in R",
    "section": "R settings",
    "text": "R settings\nSettings are saved in a .Rprofile file. You can edit the file directly in any text editor or from within R.\nList all options:\noptions()\nReturn the value of a particular option:\n\ngetOption(\"help_type\")\n\n[1] \"html\"\n\n\nSet an option:\noptions(help_type = \"html\")"
  },
  {
    "objectID": "r_intro/basics.html#assignment",
    "href": "r_intro/basics.html#assignment",
    "title": "First steps in R",
    "section": "Assignment",
    "text": "Assignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (<-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na <- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\n\nLet’s confirm that a doesn’t exist anymore in the environment:\n\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory."
  },
  {
    "objectID": "r_intro/basics.html#copy-on-modify",
    "href": "r_intro/basics.html#copy-on-modify",
    "title": "First steps in R",
    "section": "Copy-on-modify",
    "text": "Copy-on-modify\nWhile some languages (e.g. Python) do not make a copy if you modify a mutable object, R does.\nLet’s have a look at Python:\n>>> a = [1, 2, 3]\n>>> b = a\n>>> b\n[1, 2, 3]\n>>> a[0] = 4\n>>> a\n[4, 2, 3]\n>>> b\n[4, 2, 3]\nModifying a also modifies b. If you want to keep b unchanged, you need to explicitly make a copy of a.\nNow, let’s see what happens in R:\n> a <- c(1, 2, 3)\n> b <- a\n> b\n[1] 1 2 3\n> a[1] <- 4\n> a\n[1] 4 2 3\n> b\n[1] 1 2 3\nHere, the default is to create a new copy in memory when a is transformed so that b remains unchanged. This is more intuitive, but more memory intensive."
  },
  {
    "objectID": "r_intro/basics.html#comments",
    "href": "r_intro/basics.html#comments",
    "title": "First steps in R",
    "section": "Comments",
    "text": "Comments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na <- 3  # This is also a comment"
  },
  {
    "objectID": "r_intro/control_flow.html#conditionals",
    "href": "r_intro/control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\n\ntest_sign <- function(x) {\n  if (x > 0) {\n    \"x is positif\"\n  } else if (x < 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\""
  },
  {
    "objectID": "r_intro/control_flow.html#loops",
    "href": "r_intro/control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice that here we need to use the print() function."
  },
  {
    "objectID": "r_intro/data_structure.html",
    "href": "r_intro/data_structure.html",
    "title": "Data types and structures",
    "section": "",
    "text": "This section covers the various data types and structures available in R."
  },
  {
    "objectID": "r_intro/data_structure.html#summary-of-structures",
    "href": "r_intro/data_structure.html#summary-of-structures",
    "title": "Data types and structures",
    "section": "Summary of structures",
    "text": "Summary of structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray"
  },
  {
    "objectID": "r_intro/data_structure.html#atomic-vectors",
    "href": "r_intro/data_structure.html#atomic-vectors",
    "title": "Data types and structures",
    "section": "Atomic vectors",
    "text": "Atomic vectors\n\nWith a single element\n\na <- 2\na\n\n[1] 2\n\ntypeof(a)\n\n[1] \"double\"\n\nstr(a)\n\n num 2\n\nlength(a)\n\n[1] 1\n\ndim(a)\n\nNULL\n\n\nThe dim attribute of a vector doesn’t exist (hence the NULL). This makes vectors different from one-dimensional arrays which have a dim of 1.\nYou might have noticed that 2 is a double (double precision floating point number, equivalent of “float” in other languages). In R, this is the default, even if you don’t type 2.0. This prevents the kind of weirdness you can find in, for instance, Python.\nIn Python:\n>>> 2 == 2.0\nTrue\n>>> type(2) == type(2.0)\nFalse\n>>> type(2)\n<class 'int'>\n>>> type(2.0)\n<class 'float'>\nIn R:\n> 2 == 2.0\n[1] TRUE\n> typeof(2) == typeof(2.0)\n[1] TRUE\n> typeof(2)\n[1] \"double\"\n> typeof(2.0)\n[1] \"double\"\nIf you want to define an integer variable, you use:\n\nb <- 2L\nb\n\n[1] 2\n\ntypeof(b)\n\n[1] \"integer\"\n\nmode(b)\n\n[1] \"numeric\"\n\nstr(b)\n\n int 2\n\n\nThere are six vector types:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex\nraw\n\n\n\nWith multiple elements\n\nc <- c(2, 4, 1)\nc\n\n[1] 2 4 1\n\ntypeof(c)\n\n[1] \"double\"\n\nmode(c)\n\n[1] \"numeric\"\n\nstr(c)\n\n num [1:3] 2 4 1\n\n\n\nd <- c(TRUE, TRUE, NA, FALSE)\nd\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(d)\n\n[1] \"logical\"\n\nstr(d)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\ne <- c(\"This is a string\", 3, \"test\")\ne\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(e)\n\n[1] \"character\"\n\nstr(e)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nf <- c(TRUE, 3, FALSE)\nf\n\n[1] 1 3 0\n\ntypeof(f)\n\n[1] \"double\"\n\nstr(f)\n\n num [1:3] 1 3 0\n\n\n\ng <- c(2L, 3, 4L)\ng\n\n[1] 2 3 4\n\ntypeof(g)\n\n[1] \"double\"\n\nstr(g)\n\n num [1:3] 2 3 4\n\n\n\nh <- c(\"string\", TRUE, 2L, 3.1)\nh\n\n[1] \"string\" \"TRUE\"   \"2\"      \"3.1\"   \n\ntypeof(h)\n\n[1] \"character\"\n\nstr(h)\n\n chr [1:4] \"string\" \"TRUE\" \"2\" \"3.1\"\n\n\nThe binary operator : is equivalent to the seq() function and generates a regular sequence of integers:\n\ni <- 1:5\ni\n\n[1] 1 2 3 4 5\n\ntypeof(i)\n\n[1] \"integer\"\n\nstr(i)\n\n int [1:5] 1 2 3 4 5\n\nidentical(2:8, seq(2, 8))\n\n[1] TRUE"
  },
  {
    "objectID": "r_intro/data_structure.html#matrices",
    "href": "r_intro/data_structure.html#matrices",
    "title": "Data types and structures",
    "section": "Matrices",
    "text": "Matrices\n\nj <- matrix(1:12, nrow = 3, ncol = 4)\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\ntypeof(j)\n\n[1] \"integer\"\n\nstr(j)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(j)\n\n[1] 12\n\ndim(j)\n\n[1] 3 4\n\n\nThe default is byrow = FALSE. If you want the matrix to be filled in by row, you need to set this argument to TRUE:\n\nk <- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nk\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12"
  },
  {
    "objectID": "r_intro/data_structure.html#arrays",
    "href": "r_intro/data_structure.html#arrays",
    "title": "Data types and structures",
    "section": "Arrays",
    "text": "Arrays\n\nl <- array(as.double(1:24), c(3, 2, 4))\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\ntypeof(l)\n\n[1] \"double\"\n\nstr(l)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(l)\n\n[1] 24\n\ndim(l)\n\n[1] 3 2 4"
  },
  {
    "objectID": "r_intro/data_structure.html#lists",
    "href": "r_intro/data_structure.html#lists",
    "title": "Data types and structures",
    "section": "Lists",
    "text": "Lists\n\nm <- list(2, 3)\nm\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\ntypeof(m)\n\n[1] \"list\"\n\nstr(m)\n\nList of 2\n $ : num 2\n $ : num 3\n\nlength(m)\n\n[1] 2\n\ndim(m)\n\nNULL\n\n\nAs with atomic vectors, lists do not have a dim attribute. Lists are in fact a different type of vectors.\nLists can be heterogeneous:\n\nn <- list(2L, 3, c(2, 1), FALSE, \"string\")\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\ntypeof(n)\n\n[1] \"list\"\n\nstr(n)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\nlength(n)\n\n[1] 5"
  },
  {
    "objectID": "r_intro/data_structure.html#data-frames",
    "href": "r_intro/data_structure.html#data-frames",
    "title": "Data types and structures",
    "section": "Data frames",
    "text": "Data frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\no <- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(o)\n\n[1] \"list\"\n\nstr(o)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(o)\n\n[1] 2\n\ndim(o)\n\n[1] 3 2"
  },
  {
    "objectID": "r_intro/functions.html",
    "href": "r_intro/functions.html",
    "title": "Function definition",
    "section": "",
    "text": "R comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions."
  },
  {
    "objectID": "r_intro/functions.html#syntax",
    "href": "r_intro/functions.html#syntax",
    "title": "Function definition",
    "section": "Syntax",
    "text": "Syntax\nHere is the syntax to define a new function:\nname <- function(arguments) {\n  body\n}"
  },
  {
    "objectID": "r_intro/functions.html#example",
    "href": "r_intro/functions.html#example",
    "title": "Function definition",
    "section": "Example",
    "text": "Example\nLet’s define a function that we call compare which will compare the value between 2 numbers:\n\ncompare <- function(x, y) {\n  x == y\n}\n\n\ncompare is the name of our function.\nx and y are the placeholders for the arguments that our function will accept (our function will need 2 arguments to run successfully).\nx == y is the body of the function, that is, the computation performed by our function.\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE"
  },
  {
    "objectID": "r_intro/functions.html#section",
    "href": "r_intro/functions.html#section",
    "title": "Function definition",
    "section": "",
    "text": "Note that the result of the last statement is printed automatically:\n\ntest <- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to return other results, you need to explicitly use the print() function:\n\ntest <- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3"
  },
  {
    "objectID": "r_intro/index.html",
    "href": "r_intro/index.html",
    "title": "Intro R",
    "section": "",
    "text": "Date:\nTuesday May 2, 2023\nTime:\n9am–noon\nInstructor:\nMarie-Hélène Burle (Simon Fraser University)\nPrerequisites:\nThis introductory course does not require any previous experience.\nSoftware:\nAs we will provide access to an RStudio Server, no installation is required."
  },
  {
    "objectID": "r_intro/indexing.html",
    "href": "r_intro/indexing.html",
    "title": "Indexing",
    "section": "",
    "text": "This section covers indexing from the various data structures."
  },
  {
    "objectID": "r_intro/indexing.html#indexing-atomic-vectors",
    "href": "r_intro/indexing.html#indexing-atomic-vectors",
    "title": "Indexing",
    "section": "Indexing atomic vectors",
    "text": "Indexing atomic vectors\n\nHere is an example with an atomic vector of size one:\n\nIndexing in R starts at 1 and is done with square brackets next to the element to index:\n\nx <- 2\nx\n\n[1] 2\n\nx[1]\n\n[1] 2\n\n\nWhat happens if we index out of range?\n\nx[2]\n\n[1] NA\n\n\n\nExample for an atomic vector with multiple elements:\n\n\nx <- c(2, 4, 1)\nx\n\n[1] 2 4 1\n\nx[2]\n\n[1] 4\n\nx[2:4]\n\n[1]  4  1 NA"
  },
  {
    "objectID": "r_intro/indexing.html#indexing-matrices-and-arrays",
    "href": "r_intro/indexing.html#indexing-matrices-and-arrays",
    "title": "Indexing",
    "section": "Indexing matrices and arrays",
    "text": "Indexing matrices and arrays\n\nx <- matrix(1:12, nrow = 3, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nx[2, 3]\n\n[1] 8\n\nx <- array(as.double(1:24), c(3, 2, 4))\nx\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nx[2, 1, 3]\n\n[1] 14"
  },
  {
    "objectID": "r_intro/indexing.html#indexing-lists",
    "href": "r_intro/indexing.html#indexing-lists",
    "title": "Indexing",
    "section": "Indexing lists",
    "text": "Indexing lists\n\nx <- list(2L, 3:8, c(2, 1), FALSE, \"string\")\nx\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3 4 5 6 7 8\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\n\nIndexing a list returns a list:\n\nx[3]\n\n[[1]]\n[1] 2 1\n\ntypeof(x[3])\n\n[1] \"list\"\n\n\nTo extract elements of a list, double square brackets are required:\n\nx[[3]]\n\n[1] 2 1\n\ntypeof(x[[3]])\n\n[1] \"double\"\n\n\n\n\nYour turn:\n\nTry to extract the number 7 from this list."
  },
  {
    "objectID": "r_intro/indexing.html#indexing-data-frames",
    "href": "r_intro/indexing.html#indexing-data-frames",
    "title": "Indexing",
    "section": "Indexing data frames",
    "text": "Indexing data frames\n\nx <- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\nx\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\n\nA data frame is a list of atomic vectors representing the various columns:\n\nx[1]\n\n  country\n1  Canada\n2     USA\n3  Mexico\n\ntypeof(x[1])\n\n[1] \"list\"\n\nx[[1]]\n\n[1] \"Canada\" \"USA\"    \"Mexico\"\n\ntypeof(x[[1]])\n\n[1] \"character\"\n\n\nIndexing dataframes can also be done using the column names:\n\nx$country\n\n[1] \"Canada\" \"USA\"    \"Mexico\"\n\nidentical(x[[1]], x$country)\n\n[1] TRUE"
  },
  {
    "objectID": "r_intro/packages.html",
    "href": "r_intro/packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions, constants, and/or data developed by the community that add functionality to R.\nIn this section, we look at where to find packages and how to install them."
  },
  {
    "objectID": "r_intro/packages.html#looking-for-packages",
    "href": "r_intro/packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder\nYour peers and the literature"
  },
  {
    "objectID": "r_intro/packages.html#package-documentation",
    "href": "r_intro/packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nList of CRAN packages\nPackage documentation"
  },
  {
    "objectID": "r_intro/packages.html#managing-r-packages",
    "href": "r_intro/packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"package-name\")\nremove.packages(\"package-name\")\nupdate_packages()"
  },
  {
    "objectID": "r_intro/packages.html#loading-packages",
    "href": "r_intro/packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")"
  },
  {
    "objectID": "r_intro/plotting.html",
    "href": "r_intro/plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "This section focuses on plotting in R with the package ggplot2 from the tidyverse."
  },
  {
    "objectID": "r_intro/plotting.html#the-data",
    "href": "r_intro/plotting.html#the-data",
    "title": "Plotting",
    "section": "The data",
    "text": "The data\nR comes with a number of datasets. You can get a list by running data(). The ggplot2 package provides additional ones. We will use the mpg dataset from ggplot2.\nTo access the data, let’s load the package:\n\nlibrary(ggplot2)\n\nHere is what that dataset looks like:\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans      drv     cty   hwy fl   \n   <chr>        <chr>      <dbl> <int> <int> <chr>      <chr> <int> <int> <chr>\n 1 audi         a4           1.8  1999     4 auto(l5)   f        18    29 p    \n 2 audi         a4           1.8  1999     4 manual(m5) f        21    29 p    \n 3 audi         a4           2    2008     4 manual(m6) f        20    31 p    \n 4 audi         a4           2    2008     4 auto(av)   f        21    30 p    \n 5 audi         a4           2.8  1999     6 auto(l5)   f        16    26 p    \n 6 audi         a4           2.8  1999     6 manual(m5) f        18    26 p    \n 7 audi         a4           3.1  2008     6 auto(av)   f        18    27 p    \n 8 audi         a4 quattro   1.8  1999     4 manual(m5) 4        18    26 p    \n 9 audi         a4 quattro   1.8  1999     4 auto(l5)   4        16    25 p    \n10 audi         a4 quattro   2    2008     4 manual(m6) 4        20    28 p    \n   class  \n   <chr>  \n 1 compact\n 2 compact\n 3 compact\n 4 compact\n 5 compact\n 6 compact\n 7 compact\n 8 compact\n 9 compact\n10 compact\n# ℹ 224 more rows\n\n\n?mpg will give you information on the variables. In particular:\n\ndispl contains data on engine displacement (a measure of engine size and thus power) in litres (L).\nhwy contains data on fuel economy while driving on highways in miles per gallon (mpg).\ndrv represents the type of drive train (front-wheel drive, rear wheel drive, 4WD).\nclass represents the type of car.\n\nWe are interested in the relationship between engine size and fuel economy and see how the type of drive train and/or the type of car might affect this relationship."
  },
  {
    "objectID": "r_intro/plotting.html#base-r-plotting",
    "href": "r_intro/plotting.html#base-r-plotting",
    "title": "Plotting",
    "section": "Base R plotting",
    "text": "Base R plotting\nR contains built-in plotting capability thanks to the plot() function.\nA basic version of our plot would be:\n\nplot(\n  mpg$displ,\n  mpg$hwy,\n  main = \"Fuel consumption per engine size on highways\",\n  xlab = \"Engine size (L)\",\n  ylab = \"Fuel economy (mpg) on highways\"\n)"
  },
  {
    "objectID": "r_intro/plotting.html#grammar-of-graphics",
    "href": "r_intro/plotting.html#grammar-of-graphics",
    "title": "Plotting",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\nLeland Wilkinson developed the concept of grammar of graphics in his 2005 book The Grammar of Graphics. By breaking down statistical graphs into components following a set of rules, any plot can be described and constructed in a rigorous fashion.\nThis was further refined by Hadley Wickham in his 2010 article A Layered Grammar of Graphics and implemented in the package ggplot2 (that’s what the 2 “g” stand for in “ggplot”).\nggplot2 has become the dominant graphing package in R. Let’s see how to construct a plot with this package."
  },
  {
    "objectID": "r_intro/plotting.html#plotting-with-ggplot2",
    "href": "r_intro/plotting.html#plotting-with-ggplot2",
    "title": "Plotting",
    "section": "Plotting with ggplot2",
    "text": "Plotting with ggplot2\n\nYou can find the ggplot2 cheatsheet here.\n\n\nThe Canvas\nThe first component is the data:\n\nggplot(data = mpg)\n\n\n\n\n\nThis can be simplified into ggplot(mpg).\n\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))\n\n\n\n\n\nThis can be simplified into ggplot(mpg, aes(x = displ, y = hwy)).\n\n\n\nGeometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data. The type of “geom” defines the type of representation (e.g. boxplot, histogram, bar chart).\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\nWe can colour-code the points in the scatterplot based on the drv variable, showing the lower fuel efficiency of 4WD vehicles:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\n\n\n\nOr we can colour-code them based on the class variable:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\n\n\n\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function that aids at seeing patterns in the data with geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThanks to the colour-coding of the types of car, we can see that the cluster of points in the top right corner all belong to the same type: 2 seaters. Those are outliers with high power, yet high few efficiency due to their smaller size.\nThe default smoothing function uses the LOESS (locally estimated scatterplot smoothing) method, which is a nonlinear regression. But maybe a linear model would actually show the general trend better. We can change the method by passing it as an argument to geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOf course, we could apply the smoothing function to each class instead of the entire data. It creates a busy plot but shows that the downward trend remains true within each type of car:\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOther arguments to geom_smooth() can set the line width, color, or whether or not the standard error (se) is shown:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nColour scales\nIf we want to change the colour scale, we add another layer for this:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nscale_color_brewer(), based on color brewer 2.0, is one of many methods to change the color scale. Here is the list of available scales for this particular method:\n\n\n\nLabels\nWe can keep on adding layers. For instance, the labs() function allows to set title, subtitle, captions, tags, axes labels, etc.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nThemes\nAnother optional layer sets one of several preset themes.\nEdward Tufte developed, amongst others, the principle of data-ink ratio which emphasizes that ink should be used primarily where it communicates meaningful messages. It is indeed common to see charts where more ink is used in labels or background than in the actual representation of the data.\nThe default ggplot2 theme could be criticized as not following this principle. Let’s change it:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe theme() function allows to tweak the theme in any number of ways. For instance, what if we don’t like the default position of the title and we’d rather have it centered?\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nMany things can be changed thanks to the theme() function. For instance, we can move the legend to give more space to the actual graph:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAs you could see, ggplot2 works by adding a number of layers on top of each other, all following a standard set of rules, or “grammar”. This way, a vast array of graphs can be created by organizing simple components."
  },
  {
    "objectID": "r_intro/plotting.html#ggplot2-extensions",
    "href": "r_intro/plotting.html#ggplot2-extensions",
    "title": "Plotting",
    "section": "ggplot2 extensions",
    "text": "ggplot2 extensions\nThanks to its vast popularity, ggplot2 has seen a proliferation of packages extending its capabilities.\n\nCombining plots\nFor instance the patchwork package allows to easily combine multiple plots on the same frame.\nLet’s add a second plot next to our plot. To add plots side by side, we simply add them to each other. We also make a few changes to the labels to improve the plots integration:\n\nlibrary(patchwork)\n\nggplot(mpg, aes(x = displ, y = hwy)) +        # First plot\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.75),           # Better legend position\n    legend.background = element_rect(         # Add a frame to the legend\n      linewidth=0.1,\n      linetype=\"solid\",\n      colour = \"black\"\n    )\n  ) +\n  ggplot(mpg, aes(x = displ, y = hwy)) +      # Second plot\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    x = \"Engine size (L)\",\n    y = element_blank(),                      # Remove redundant label\n    color = \"Type of drive train\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.87),\n    legend.background = element_rect(\n      linewidth=0.1,\n      linetype=\"solid\",\n      colour = \"black\"\n    )\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nExtensions list\nAnother popular extension is the gganimate package which allows to create data animations.\nA full list of extensions for ggplot2 is shown below (here is the website):"
  },
  {
    "objectID": "r_intro/publishing.html",
    "href": "r_intro/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "You might have heard of R Markdown: a way to intertwine code and prose in a single scientific document. The company behind R Markdown has now developed its successor: Quarto.\n\nQuarto allows the creation of webpages, websites, presentations, books, pdf, etc. from code in R, Python, or Julia and markdown text.\nIf you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto."
  },
  {
    "objectID": "r_intro/resources.html",
    "href": "r_intro/resources.html",
    "title": "Resources",
    "section": "",
    "text": "The R community is dynamic and offers a lot of online resources, from IDEs to Q&A, to workshops, books, or publications.\nThis section provides a selection of useful sites.\n\n\nMain sites\n\nR website\nComprehensive R Archive Network (CRAN): R versions and packages\n\n\n\nPosit and RStudio IDE\n\nPosit website (Posit was formerly called RStudio Inc.)\nPosit cheatsheets\n\n\n\nForums and Q&A\n\nStack Overflow [r] tag wiki\nStack Overflow [r] tag questions\nPosit Discourse\n\n\n\nDocumentation as pdf\n\nContributed documentation\nIntro books\n\n\n\nSoftware Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis\nData analysis using R in the digital humanities\n\n\n\nOnline books\n\nR for Data Science (heavily based on the tidyverse)\nR Packages (how to create packages)\nR Programming for Data Science\nMastering Software Development in R\n\n\n\nR research\n\nThe R Journal"
  },
  {
    "objectID": "r_intro/run_r_intro.html",
    "href": "r_intro/run_r_intro.html",
    "title": "Running R for this course",
    "section": "",
    "text": "This section covers the various ways R can be run and get you started using our RStudio server for this course."
  },
  {
    "objectID": "r_intro/run_r_intro.html#running-r",
    "href": "r_intro/run_r_intro.html#running-r",
    "title": "Running R for this course",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server."
  },
  {
    "objectID": "r_intro/run_r_intro.html#accessing-our-temporary-rstudio-server",
    "href": "r_intro/run_r_intro.html#accessing-our-temporary-rstudio-server",
    "title": "Running R for this course",
    "section": "Accessing our temporary RStudio server",
    "text": "Accessing our temporary RStudio server\nYou do not need to install anything on your machine for this course as we will provide access to a temporary RStudio server.\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry. This will take you to a JupyterHub. There, click on the “RStudio” button and the RStudio server will open in a new tab."
  },
  {
    "objectID": "r_intro/run_r_intro.html#using-rstudio",
    "href": "r_intro/run_r_intro.html#using-rstudio",
    "title": "Running R for this course",
    "section": "Using RStudio",
    "text": "Using RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r_intro/tidyverse.html",
    "href": "r_intro/tidyverse.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent and more similar to programming languages which were developed by computer scientists rather than statisticians.\nYou can think of it as a more modern version of R."
  },
  {
    "objectID": "r_intro/tidyverse.html#base-r-or-tidyverse",
    "href": "r_intro/tidyverse.html#base-r-or-tidyverse",
    "title": "Introduction to the tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n“Base R” refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one you should use is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn’t require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm."
  },
  {
    "objectID": "r_intro/tidyverse.html#a-glimpse-of-the-tidyverse",
    "href": "r_intro/tidyverse.html#a-glimpse-of-the-tidyverse",
    "title": "Introduction to the tidyverse",
    "section": "A glimpse of the tidyverse",
    "text": "A glimpse of the tidyverse\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nPosit (the company formerly known as RStudio Inc. behind the tidyverse) developed a series of useful cheatsheets. Below are links to the ones you are the most likely to use as you get started with R.\n\nData import\nThe first thing you often need to do is to import your data into R. This is done with readr.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nData transformation\nYou then often need to transformation your data into the right format. This is done with the packages dplyr and tidyr.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\nVisualization in the tidyverse is done with the ggplot2 package which we will explore in the next section.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\nThe package forcats offers the tidyverse approach to working with factors.\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with strings\nstringr is for strings.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with dates\nlubridate will help you deal with dates.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\nFinally, purrr is the tidyverse equivalent to the apply functions in base R: a way to run functions on functions.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r_intro/why.html",
    "href": "r_intro/why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?"
  },
  {
    "objectID": "r_intro/why.html#why-r",
    "href": "r_intro/why.html#why-r",
    "title": "R: why and for whom?",
    "section": "Why R?",
    "text": "Why R?\nHere are a number of reasons why you might want to consider using R:\n\nFree and open source\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs available (RStudio, Emacs ESS)"
  },
  {
    "objectID": "r_intro/why.html#for-whom",
    "href": "r_intro/why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data"
  },
  {
    "objectID": "r_intro/why.html#downsides-of-r",
    "href": "r_intro/why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage"
  },
  {
    "objectID": "r_parallel/clusters.html",
    "href": "r_parallel/clusters.html",
    "title": "Running R on HPC clusters",
    "section": "",
    "text": "This section will show you how to use R once you have logged in to a remote cluster via SSH."
  },
  {
    "objectID": "r_parallel/clusters.html#log-in-to-our-temporary-training-cluster",
    "href": "r_parallel/clusters.html#log-in-to-our-temporary-training-cluster",
    "title": "Running R on HPC clusters",
    "section": "Log in to our temporary training cluster",
    "text": "Log in to our temporary training cluster\nAs mentioned previously, once you have developed your code in an interactive fashion, running scripts allows you to request large hardware resources only when you need them (i.e. when your code is executed).\nThis prevents these heavy resources from sitting idle when not in use (for instance when you think or type code), saving you money on commercial clusters or waiting time on Alliance clusters.\nSo let’s log in to the temporary training cluster."
  },
  {
    "objectID": "r_parallel/clusters.html#loading-modules",
    "href": "r_parallel/clusters.html#loading-modules",
    "title": "Running R on HPC clusters",
    "section": "Loading modules",
    "text": "Loading modules\nOnce in the cluster, we need to make the modules (software) we need available.\n\nC compiler\nTo compile R packages, we need a C compiler.\nIn theory, one could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can be compiled by any C compiler—also including Clang and LLVM—but the default GCC compiler is the best way to avoid headaches).\nIt is thus much simpler to always load a gcc module before loading an r module.\n\n\nR module\nTo see what versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.2.1), run:\nmodule spider r/4.2.1\n\nStdEnv/2020 is a required module for this version.\nOn most Alliance clusters, it is automatically loaded, so you don’t need to include it. You can double-check with module list or you can include it (before r/4.2.1) just to be sure.\n\n\n\nLoad the modules\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1"
  },
  {
    "objectID": "r_parallel/clusters.html#installing-r-packages",
    "href": "r_parallel/clusters.html#installing-r-packages",
    "title": "Running R on HPC clusters",
    "section": "Installing R packages",
    "text": "Installing R packages\n\nFor this course, all packages have already been installed in a communal library. You thus don’t have to install anything.\n\nTo install a package, launch the interactive R console with:\nR\nIn the R console, run:\ninstall.packages(\"<package_name>\", repos=\"<url-cran-mirror>\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.\n\nTo leave the R console, press <Ctrl+D>."
  },
  {
    "objectID": "r_parallel/clusters.html#running-r-jobs",
    "href": "r_parallel/clusters.html#running-r-jobs",
    "title": "Running R on HPC clusters",
    "section": "Running R jobs",
    "text": "Running R jobs\nThere are two types of jobs that can be launched on an Alliance cluster: interactive jobs and batch jobs. We will practice both and discuss their respective merits and when to use which.\nFor this course, I purposefully built a rather small cluster (10 nodes with 4 CPUs and 30GB each) to give a tangible illustration of the constraints of resource sharing.\n\nInteractive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\n\nExample:\n\nsalloc --time=1:10:00 --mem-per-cpu=7000M --ntasks=8\nThis takes you to a compute node where you can now launch R to run computations:\nR\n\nThis however leads to the same inefficient use of resources as happens when running an RStudio server: all the resources that you requested are blocked for you while your job is running, whether you are making use of them (running heavy computations) or not (thinking, typing code, running computations that use only a fraction of the requested resources).\nInteractive jobs are thus best kept to develop code.\n\n\n\nScripts\nTo run an R script called <your_script>.R, you first need to write a job script:\n\nExample:\n\n\n<your_job>.sh\n\n#!/bin/bash\n#SBATCH --account=def-<your_account>\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3000M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"<your_job>\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1\nRscript <your_script>.R   # Note that R scripts are run with the command `Rscript`\n\n\nThen launch your job with:\nsbatch <your_job>.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@).\n\nBatch jobs are the best approach to run parallel computations, particularly when they require a lot of hardware.\nIt will save you lots of waiting time (Alliance clusters) or money (commercial clusters)."
  },
  {
    "objectID": "r_parallel/data.html",
    "href": "r_parallel/data.html",
    "title": "Data on HPC clusters",
    "section": "",
    "text": "So far, we have played with randomly created data. In your work, you will often need to work with real world data.\nHow do you move it to the cluster? Where should you store it?\nIt’s time to talk about data on HPC clusters."
  },
  {
    "objectID": "r_parallel/data.html#transferring-data-tofrom-the-cluster",
    "href": "r_parallel/data.html#transferring-data-tofrom-the-cluster",
    "title": "Data on HPC clusters",
    "section": "Transferring data to/from the cluster",
    "text": "Transferring data to/from the cluster\n\nSecure Copy Protocol\nSecure Copy Protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file username@hostname:\n# Copy a local file to some path on the cluster\nscp /local/path/file username@hostname:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp username@hostname:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp username@hostname:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp username@hostname:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder username@hostname:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\n\nSecure File Transfer Protocol\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations in an interactive shell. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp username@hostname\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp>.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp> help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l<command>. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp> pwd              # print remote working directory\nsftp> lpwd             # print local working directory\nsftp> ls               # list files in remote working directory\nsftp> lls              # list files in local working directory\nsftp> cd               # change the remote directory\nsftp> lcd              # change the local directory\nsftp> put local_file   # upload a file\nsftp> get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp> mkdir dir    # First create the directory\nsftp> put -r dir   # Then copy the content\nTo terminate the session, press <Ctrl+D>.\n\n\n\nSyncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions.\n\n\nHeavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service.\n\n\nWindows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file"
  },
  {
    "objectID": "r_parallel/data.html#files-management",
    "href": "r_parallel/data.html#files-management",
    "title": "Data on HPC clusters",
    "section": "Files management",
    "text": "Files management\nThe Alliance clusters are designed to handle large files very well. They are however slowed by the presence of many small files. It is thus important to know how to handle large collections of files by archiving them with tools such as tar and dar."
  },
  {
    "objectID": "r_parallel/data.html#where-to-store-data",
    "href": "r_parallel/data.html#where-to-store-data",
    "title": "Data on HPC clusters",
    "section": "Where to store data",
    "text": "Where to store data\nSupercomputers have several filesystems and you should familiarize yourself with the quotas and policies of the clusters you use.\nAll filesystems are mounted on all nodes so that you can access the data on any network storage from any node (e.g. something in /home, /project, or /scratch will be accessible from any login node or compute node).\nA temporary folder gets created directly on the compute nodes while a job is running. In situations with heavy I/O or involving many files, it is worth considering copying data to it as part of the job. In that case, make sure to copy the results back to network storage before the end of the job."
  },
  {
    "objectID": "r_parallel/index.html",
    "href": "r_parallel/index.html",
    "title": "Parallel R",
    "section": "",
    "text": "Date:\nTuesday May 2, 2023\nTime:\n2pm–5pm\nInstructor:\nMarie-Hélène Burle (Simon Fraser University)\nPrerequisites:\nBasic knowledge of R and some basic working knowledge of HPC (how to submit Slurm jobs and view their output).\nSoftware:\nWe will provide access to one of our Linux systems. To make use of it, attendees will need a remote secure shell (SSH) client installed on their computer. On Windows we recommend the free Home Edition of MobaXterm. On Mac and Linux computers, SSH is usually pre-installed (try typing ssh in a terminal to make sure it is there)."
  },
  {
    "objectID": "r_parallel/memory.html",
    "href": "r_parallel/memory.html",
    "title": "Memory management",
    "section": "",
    "text": "Memory can be a limiting factor and releasing it when not needed can be critical to avoid out of memory states. On the other hand, memoisation is an optimization technique which stores the results of heavy computations for re-use at the cost of increasing memory usage.\nMemory and speed are thus linked in a trade-off."
  },
  {
    "objectID": "r_parallel/memory.html#releasing-memory",
    "href": "r_parallel/memory.html#releasing-memory",
    "title": "Memory management",
    "section": "Releasing memory",
    "text": "Releasing memory\nIt is best to avoid creating very large intermediate objects (e.g. with nested functions or functions chained with the magrittr pipe), but if you must, remove them from the global environment with rm() when you don’t need them anymore. Once all the pointers to an object in memory are deleted, the garbage collector will clear its value and release the memory it used.\nAnother way to release the memory used by heavy intermediate objects is with functions: if you create those objects in the local environment of a function (instead of directly in the global environment), they will be cleared from memory as soon as the function has finished running.\nNote that in the case of a very large function, it might still be beneficial to run rm() inside the function to clear the memory for other processes coming next within that function. But this is a pretty rare case."
  },
  {
    "objectID": "r_parallel/memory.html#caching-in-memory",
    "href": "r_parallel/memory.html#caching-in-memory",
    "title": "Memory management",
    "section": "Caching in memory",
    "text": "Caching in memory\nMemoisation is a technique by which the results of heavy computations are stored in memory to avoid have to re-calculate them. This can be convenient in a variety of settings (e.g. to reduce calls to an API), but mostly, it can greatly improve the efficiency of some code such as recursive function calls.\nLet’s consider the calculation of the Fibonacci numbers as an example. Those numbers form a sequence starting with 0 and 11, after which each number is the sum of the previous two.1 Alternative versions have the sequence start with 1, 1 or with 1, 2.\nHere is a function that would return the nth Fibonacci number2:2 There are more efficient ways to calculate the Fibonacci numbers, but this inefficient function is a great example to show the advantage of memoisation.\nfib <- function(n) {\n  if(n == 0) {\n    return(0)\n  } else if(n == 1) {\n    return(1)\n  } else {\n    Recall(n - 1) + Recall(n - 2)\n  }\n}\nIt can be written more tersely as:\n\nfib <- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\n\nRecall() is a placeholder for the name of the recursive function. We could have used fib() instead, but Recall() is more robust as it allows for function renaming.\n\nMemoisation is very useful here because, for each Fibonacci number, we need to calculate the two preceding Fibonacci numbers and to calculate each of those we need to calculate the two Fibonacci numbers preceding that one and to calculate… etc. That is a large number of calculations, but, thanks to caching, we don’t have to calculate any one of them more than once.\nThe packages R.cache and memoise both allow for memoisation with an incredibly simple syntax.\nApplying the latter to our function gives us:\n\nlibrary(memoise)\n\nfibmem <- memoise(function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  fibmem(n - 1) + fib(n - 2)\n})\n\nWe can do some benchmarking to see the speedup for the 30th Fibonacci number:\n\nlibrary(bench)\n\nn <- 30\nmark(fib(n), fibmem(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 fib(n)         1.6s     1.6s     0.623    34.1KB     21.8\n2 fibmem(n)    36.7µs   38.2µs 24465.        429KB     17.1\n\n\nThe speedup is over 35,000!"
  },
  {
    "objectID": "r_parallel/optimizations.html",
    "href": "r_parallel/optimizations.html",
    "title": "Optimizations",
    "section": "",
    "text": "A lot of hardware is not the answer to poorly written code. Before considering parallelization, you should think about ways to optimize your code sequentially.\nWhy?\n\nNot all code can be parallelized.\nParallelization is costly (waiting time to access a cluster or money).\nThe optimization of the sequential code will also benefit the parallel code.\n\nIn many cases, writing better code will save you more computing time than parallelization.\nIn this section, we will cover several principles by playing with the programmatic implementation of the fizz buzz game."
  },
  {
    "objectID": "r_parallel/optimizations.html#toy-example",
    "href": "r_parallel/optimizations.html#toy-example",
    "title": "Optimizations",
    "section": "Toy example",
    "text": "Toy example\nFizz buzz is a children game to practice divisions. Players take turn counting out loud while replacing:\n\nany number divisible by 3 with the word “Fizz”,\nany number divisible by 5 with the word “Buzz”,\nany number divisible by both 3 and 5 with the word “FizzBuzz”.\n\nLet’s write functions to solve the game and time them to draw some general principles about more efficient code.\nWe will use bench::mark() to benchmark our solutions. Let’s load it:\n\nlibrary(bench)"
  },
  {
    "objectID": "r_parallel/optimizations.html#pre-allocate-memory",
    "href": "r_parallel/optimizations.html#pre-allocate-memory",
    "title": "Optimizations",
    "section": "Pre-allocate memory",
    "text": "Pre-allocate memory\nIn this first function, we create an empty object z of class integer and of length 0 that will hold the result of a loop, then we run the loop and at each iteration, we add a new value to z:\n\nf1 <- function(n) {\n  z <- integer()\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } else {\n      z[i] <- i\n    }\n  }\n  z\n}\n\nThe second function is very similar, but this time, we create an empty object z of class integer and of length matching the final length z will have after running the loop. This means that we are pre-allocating memory for the full vector before we run the loop instead of growing the vector at each iteration:\n\nf2 <- function(n) {\n  z <- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } else {\n      z[i] <- i\n    }\n  }\n  z\n}\n\nLet’s make sure that our functions work by testing it on a small number:\n\nf1(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\nf2(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nNow, let’s benchmark them for a large number:\n\nn <- 1e5\nmark(f1(n), f2(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f1(n)         135ms    139ms      7.17   16.55MB     23.3\n2 f2(n)         121ms    127ms      7.77    1.15MB     27.2\n\n\nf2() is consistently faster. While in this example the difference is very slight, pre-allocating the object that will hold the result of a loop before running the loop can make a huge difference."
  },
  {
    "objectID": "r_parallel/optimizations.html#arent-loops-a-big-no-no-in-r",
    "href": "r_parallel/optimizations.html#arent-loops-a-big-no-no-in-r",
    "title": "Optimizations",
    "section": "Aren’t loops a big ‘no no’ in R?",
    "text": "Aren’t loops a big ‘no no’ in R?\nBy now, you might be thinking: “Wait… aren’t loops a big ‘no no’ in R? I’ve always been told that they are slow and that one should always use functional programming! We are talking about optimization in this course and we are using loops?!?”\nThere are a lot of misconceptions around R loops. They can be very slow if you don’t pre-allocate memory. Otherwise they are almost always faster than functions (the apply() family or the tidyverse equivalent of the purrr::map() family). You can choose to use a functional programming approach for style and readability, but not for speed.\nLet’s test it.\nFirst we create a function:\n\nfb <- function(n) {\n  if(n %% 3 == 0 && n %% 5 == 0) {\n    \"FizzBuzz\"\n  } else if(n %% 3 == 0) {\n    \"Fizz\"\n  } else if(n %% 5 == 0) {\n    \"Buzz\"\n  } else {\n    n\n  }\n}\n\nThen we pass it through sapply(). We can test that it works on a small number:\n\nsapply(1:20, fb)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nFinally, we compare the timing with that of f2():\n\nmark(f2(n), sapply(1:n, fb))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression           min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>      <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f2(n)              132ms    146ms      6.94    1.15MB     27.8\n2 sapply(1:n, fb)    177ms    177ms      5.40    3.29MB     25.2\n\n\nAs you can see, the loop is faster."
  },
  {
    "objectID": "r_parallel/optimizations.html#avoid-unnecessary-operations",
    "href": "r_parallel/optimizations.html#avoid-unnecessary-operations",
    "title": "Optimizations",
    "section": "Avoid unnecessary operations",
    "text": "Avoid unnecessary operations\n\nExample 1\nCalling z as the last command in our function is the same as calling return(z).\nFrom the R documentation:\n\nIf the end of a function is reached without calling return, the value of the last evaluated expression is returned.\n\nNow, what about using print() instead?\n\nf3 <- function(n) {\n  z <- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } else {\n      z[i] <- i\n    }\n  }\n  print(z)\n}\n\nLet’s benchmark it against f2():\nmark(f2(n), f3(n))\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"     \"Fizz\"     \"22\"       \"23\"       \"Fizz\"    \n[25] \"Buzz\"     \"26\"       \"Fizz\"     \"28\"       \"29\"       \"FizzBuzz\"\n[31] \"31\"       \"32\"       \"Fizz\"     \"34\"       \"Buzz\"     \"Fizz\"    \n[37] \"37\"       \"38\"       \"Fizz\"     \"Buzz\"     \"41\"       \"Fizz\"\n...\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f2(1e+05)  151.88ms 157.65ms     6.30     1.25MB     29.9\n2 f3(1e+05)     3.25s    3.25s     0.308    1.04GB     26.8\nWhat happened?\nprint() returns its argument, but it additionally prints it to the standard output. This is why the mark() function printed the output of f3() before printing the timings.\nAs you can see, printing takes a long time.\n\nThe code in this website is run by Quarto. Since, by default, RStudio will only print the first 1,000 results, the timing you will get for f3() in RStudio will be much less bad as it won’t include the time it takes to print the remaining 99,000 results.\n\nIf you are evaluating f2() on its own (e.g. f2(20)), the returned result will also be printed to standard output and both functions will be equivalent. However, if you are using the function in another context, printing becomes an unnecessary and timely operation and f3() would be a very bad option. f3() is thus not a good function.\nHere is an example in which f3() would perform a totally unnecessary operation that f2() avoids:\n\na <- f2(20)\n\n\nNo unnecessary printing.\n\n\na <- f3(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n\nUnnecessary printing.\n\nFor 1e5, the difference in timing between running an unnecessary printing vs not is a factor of 21!\nEven worse would be to use:\n\nf4 <- function(n) {\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      print(\"FizzBuzz\")\n    } else if(i %% 3 == 0) {\n      print(\"Fizz\")\n    } else if(i %% 5 == 0) {\n      print(\"Buzz\")\n    } else {\n      print(i)\n    }\n  }\n}\n\nHere the difference in timing is a factor of 50…\n\n\nExample 2\nOne modulo operation and equality test can be removed by replacing i %% 3 == 0 && i %% 5 == 0 by i %% 15 == 0. The difference isn’t huge, but there is a slight speedup:\n\nf2bis <- function(n) {\n  z <- integer(n)\n  for(i in 1:n) {\n    if(i %% 15 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } else {\n      z[i] <- i\n    }\n  }\n  z\n}\n\nmark(f2(n), f2bis(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f2(n)         112ms    142ms      6.75    1.15MB     28.7\n2 f2bis(n)      105ms    106ms      9.44    1.22MB     34.0\n\n\n\n\nExample 3\nBut is f2() really the fastest function?\nLouis Arsenault-Mahjoubi—who attended this workshop—found ways to get rid of several operations and get a speedup of 1.7 over f2().\nFirst, we can assign 1:n to z instead of pre-allocating memory with an empty vector, thus rendering the assignment of i to z[i] unnecessary in the last else statement:\n\nf_louis1 <- function(n) {\n  z <- 1:n\n  for(i in z) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } \n  }\n  z\n}\n\nThis function works:\n\nf_louis1(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n… and is faster (speedup of 1.3):\n\nmark(f2bis(n), f_louis1(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>  <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f2bis(n)     108.8ms  109.7ms      9.04    1.15MB     34.4\n2 f_louis1(n)   80.7ms   82.8ms     12.1     1.15MB     39.7\n\n\nThen, we can prevent the repetitions of the modulo operations and equality tests by saving them to variables:\n\nf_louis2 <- function(n) {\n  z <- 1:n\n  for(i in z) {\n    div3 <- (i %% 3 == 0)\n    div5 <- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] <- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] <- \"Fizz\"\n    } else if(div5) {\n      z[i] <- \"Buzz\"\n    } \n  }\n  z\n}\n\nThis gets us an even greater speedup of 1.7:\n\nmark(f2bis(n), f_louis2(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression       min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr>  <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f2bis(n)     109.3ms  112.4ms      8.69    1.15MB     34.8\n2 f_louis2(n)   66.6ms   68.9ms     14.3     1.21MB     33.9"
  },
  {
    "objectID": "r_parallel/optimizations.html#replace-costly-operations-where-possible",
    "href": "r_parallel/optimizations.html#replace-costly-operations-where-possible",
    "title": "Optimizations",
    "section": "Replace costly operations where possible",
    "text": "Replace costly operations where possible\nNow imagine that we have a dataframe called dat with a first column called datvar filled with integers.\nWe want to write a function that will accept our dataframe as argument and play the fizz buzz game on the column datvar.\nOne could imagine the following function:\n\nf5 <- function(dat) {\n  z <- integer(length(dat[[1]]))\n  for(i in seq_along(dat[[1]])) {\n    if(dat[[1]][i] %% 3 == 0 && dat[[1]][i] %% 5 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(dat[[1]][i] %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(dat[[1]][i] %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } else {\n      z[i] <- dat[[1]][i]\n    }\n  }\n  z\n}\n\nIndexing a column from a dataframe in this fashion is a very costly operation. It is much more efficient to index with the name of the column:\n\nf6 <- function(dat) {\n  z <- integer(length(dat$datvar))\n  for(i in seq_along(dat$datvar)) {\n    if(dat$datvar[i] %% 3 == 0 && dat$datvar[i] %% 5 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(dat$datvar[i] %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(dat$datvar[i] %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } else {\n      z[i] <- dat$datvar[i]\n    }\n  }\n  z\n}\n\nNow, let’s create a random dataframe to benchmark f5() and f6():\n\nset.seed(123)\ndat <- data.frame(datvar = round(runif(n, 1, n)))\nmark(f5(dat), f6(dat))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f5(dat)       1.83s    1.83s     0.547    1.26MB     27.3\n2 f6(dat)    378.11ms 403.17ms     2.48     1.26MB     22.3"
  },
  {
    "objectID": "r_parallel/optimizations.html#avoid-repetitions-of-costly-operations",
    "href": "r_parallel/optimizations.html#avoid-repetitions-of-costly-operations",
    "title": "Optimizations",
    "section": "Avoid repetitions of costly operations",
    "text": "Avoid repetitions of costly operations\nThis made a big difference (speedup of 5), but notice that we are indexing the column 6 times in our function. Let’s remove the repetition of this operation:\n\nf7 <- function(dat) {\n  var <- dat$datvar\n  z <- integer(length(var))\n  for(i in seq_along(var)) {\n    if(var[i] %% 3 == 0 && var[i] %% 5 == 0) {\n      z[i] <- \"FizzBuzz\"\n    } else if(var[i] %% 3 == 0) {\n      z[i] <- \"Fizz\"\n    } else if(var[i] %% 5 == 0) {\n      z[i] <- \"Buzz\"\n    } else {\n      z[i] <- var[i]\n    }\n  }\n  z\n}\n\nLet’s benchmark all 3 versions:\n\nmark(f5(dat), f6(dat), f7(dat))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 3 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl>\n1 f5(dat)       1.81s    1.81s     0.553    1.15MB     27.7\n2 f6(dat)    354.24ms 387.45ms     2.58     1.15MB     24.5\n3 f7(dat)    143.27ms 145.31ms     6.86     1.25MB     18.9\n\n\nf7() gave us another speedup of almost 3 over f6(). f7() runs 14 times faster than our initial function!\n\nIndexing from a vector isn’t costly. There is thus no advantage at removing the repetition of that operation.\n\n\n\nYour turn:\n\nShow that this last statement is true."
  },
  {
    "objectID": "r_parallel/parallel_loops.html",
    "href": "r_parallel/parallel_loops.html",
    "title": "Parallel loops with foreach & doFuture",
    "section": "",
    "text": "The foreach package implements a looping construct without an explicit counter. It doesn’t require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel.\nUnlike loops, it creates variables (loops are used for their side-effect).\nLet’s look at an example to calculate the sum of 1e4 random vectors of length 3.\nBut first, what type of job shall we run here? This example is tiny and sequential. As the cluster for this course contains 10 nodes with 4 CPUs each, each of us can monopolize a CPU. We can thus use an interactive job:\nsalloc --time=30 --mem-per-cpu=7000M --ntasks=1\n\nMake sure to type this in the Bash terminal. We haven’t launched R yet!\n\n\nYou can see the full list of salloc options here.\n\nThen we can launch R interactively:\nR\n\nWe are now in the R terminal and can start typing R commands.\n\nWe will use foreach and iterators (which creates convenient iterators for foreach):\n\nlibrary(foreach)\nlibrary(iterators)\n\n\nClassic while loop:\n\n\nset.seed(2)\nresult1 <- numeric(3)            # Preallocate output container\ni <- 0                           # Initialise counter variable\n\nwhile(i < 1e4) {\n  result1 <- result1 + runif(3)  # Calculate the sum\n  i <- i + 1                     # Update the counter\n}\n\n\nWith foreach:\n\n\nset.seed(2)\nresult2 <- foreach(icount(1e4), .combine = '+') %do% runif(3)\n\nWe can verify that both expressions returned the same result:\n\nall.equal(result1, result2)\n\n[1] TRUE\n\n\nThe best part of foreach is that you can turn sequential loops into parallel ones by registering a parallel backend and replacing %do% with %dopar%.\nThere are many parallelization backends available: doFuture, doMC, doMPI, doParallel, doRedis, doRNG, doSNOW, and doAzureParallel.\nIn this lesson, we will use doFuture which allows to evaluate foreach expressions following any of the strategies of the future package.\nSo first, what is the future package?"
  },
  {
    "objectID": "r_parallel/parallel_loops.html#future-package",
    "href": "r_parallel/parallel_loops.html#future-package",
    "title": "Parallel loops with foreach & doFuture",
    "section": "future package",
    "text": "future package\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved. Futures thus allow for asynchronous and parallel evaluations.\nThe future package allows to evaluate futures sequentially or in various forms of parallelism while keeping code simple and consistent. The evaluation strategy is set thanks to the plan function:\n\nplan(sequential):\nFutures are evaluated sequentially in the current R session.\nplan(multisession):\nFutures are evaluated by new R sessions spawned in the background (multi-processing in shared memory).\nplan(multicore):\nFutures are evaluated in processes forked from the existing process (multi-processing in shared memory).\nplan(cluster):\nFutures are evaluated on an ad-hoc cluster (distributed parallelism across multiple nodes).\n\n\nConsistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na <- 1\n\nb %<-% {      # %<-% is used instead of <- to use futures\n  a <- 2\n}\n\na\n\n[1] 1"
  },
  {
    "objectID": "r_parallel/parallel_loops.html#dofuture-package",
    "href": "r_parallel/parallel_loops.html#dofuture-package",
    "title": "Parallel loops with foreach & doFuture",
    "section": "doFuture package",
    "text": "doFuture package\nThe doFuture package allows to evaluate foreach expressions across the evaluation strategies of the future package.\nLet’s go back to our foreach example. We had:\nlibrary(foreach)\n\nset.seed(2)\nresult2 <- foreach(icount(1e4), .combine = '+') %do% runif(3)\nWe could replace %do% with %dopar%:\nlibrary(foreach)\n\nset.seed(2)\nresult3 <- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\nSince we haven’t registered any parallel backend, the expression would still be evaluated sequentially. To run this in parallel, we would need to load doFuture, register it as a backend (with registerDoFuture()), and choose a parallel strategy (e.g. plan(multicore)):\nlibrary(foreach)\nlibrary(doFuture)\n\nregisterDoFuture()\nplan(multicore)\n\nset.seed(2)\nresult3 <- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\nOf course, we would also need to run this in a Slurm job with multiple CPUs.\nWith the overhead of parallelization however, it doesn’t make sense to parallelize such a short code, so let’s go over a proper toy example and do some benchmarking."
  },
  {
    "objectID": "r_parallel/parallel_loops.html#toy-example",
    "href": "r_parallel/parallel_loops.html#toy-example",
    "title": "Parallel loops with foreach & doFuture",
    "section": "Toy example",
    "text": "Toy example\n\nBefore getting started with this toy example, remember that you need to run the following:\n# Load necessary modules\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1\n\n# Start interactive job with 1CPU\nsalloc --time=30 --mem-per-cpu=7000M --ntasks=1\n\n\nLoad packages\nFor this toy example, we will use a modified version of one of the examples in the foreach vignette: we will build a classification model made of a forest of decision trees thanks to the randomForest package.\nBecause the code includes randomly generated numbers, we will use the doRNG package which replaces foreach::%dopar% with doRNG::%dorng%. This follows the recommendations of Pierre L’Ecuyer (1999)1 and ensures reproducibility.1 L’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.\nlibrary(doFuture)       # This will also load the `future` package\nlibrary(doRNG)          # This will also load the `foreach` package\nlibrary(randomForest)\nlibrary(bench)          # To do some benchmarking\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\n\n\nThe code to parallelize\nThe goal is to create a classifier based on some data (here a matrix of random numbers for simplicity) and a response variable (as factor). This model could then be passed in the predict() function with novel data to generate predictions of classification. But here we are only interested in the creation of the model as this is the part that is computationally intensive. We aren’t interested in actually using it.\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nrf <- foreach(ntree = rep(250, 8), .combine = combine) %do%\n  randomForest(x = traindata, y = fac, ntree = ntree)\n\nrf\nCall:\n randomForest(x = traindata, y = fac, ntree = ntree)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 31\n\n\nReference timing\nThis is the non parallelizable code with %do%:\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n[1] 5.66s\n\nbench::mark() is currently unable to provide memory information for parallel code. While it could output memory usage for this sequential run, we won’t be able to compare it with parallel runs, so we might as well not ask for that information.\n\n\n\nPlan sequential\nThis is the parallelizable foreach code, but run sequentially:\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\n# Using bench::mark()\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n[1] 5.78s\n\nNo surprise: those are similar.\n\n\n\nMulti-processing in shared memory\n\nNumber of cores\nfuture provides availableCores() to detect the number of available cores:\navailableCores()\ncgroups.cpuset\n             1\n\nSimilar to parallel::detectCores().\n\nThis detects the number of CPU cores available to us on the current compute node, that is, what we can use for shared memory multi-processing. Since we asked for a single task (--ntasks=1) and since by default Slurm grants one CPU per task, we have a single CPU available.\nTo be able to run our code in parallel, we need to have access to at least 2 CPUs each. So let’s quit the R session (with Ctrl+D or quit()—when asked whether to save a workspace image, answer n), terminate our interactive job (also with Ctrl+D) and ask for a different job.\n\nDon’t forget to relinquish your interactive job with Ctrl+D otherwise it will be running for the full 30 min, making the hardware it uses unavailable to all of us until the job expires.\n\nNow what job should we run?\nRemember that the cluster for this course is made of 10 nodes of 4 CPUs each. We want to test shared memory parallelism, so our job needs to stay within one node. We can thus ask for a maximum of 4 CPUs and we want to ensure that we aren’t getting them on different nodes. If we all ask for 4 CPUs in an interactive session, the first 10 of us will get them and the job requests for the rest of us will remain pending, waiting for resources to become available, for as long as the lucky 10 are running their session. That’s the big downside of interactive sessions.\nA much better approach here is to write the code in a script and run it with sbatch. That way, everybody will get to run their code with minimal delay.\nOpen a text file (let’s call it rf.R since it creates a random forest object) with the text editor of your choice, for instance nano:\nnano rf.R\nWe will first play with it to see how many cores are available to us, so write in your script:\n\n\nrf.R\n\nlibrary(future)   # Don't forget to load the packages in your script\navailableCores()\n\nSave and close the text editor.\nNow, we want to create a shell script for Slurm. Let’s call it rf.sh:\nnano rf.sh\nIn it lives the hardware request and the code that needs to run:\n\n\nrf.sh\n\n#!/bin/bash\n#SBATCH --time=10             # 10 min\n#SBATCH --mem-per-cpu=7000M\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=4\n\nRscript rf.R                  # This is the code that we are running\n\n\nYou can see the full list of sbatch options here.\n\nSave and close the text editor.\nWe can now run the batch script:\nsbatch rf.sh\nYou can monitor it with sq, but this should be quasi instant. The result will be written to a file called slurm-xx.out with xx being the number of the job that just ran.\n\nYou can specify the output file name in the options of your sbatch script.\n\nTo see the result, we can simply print the content of that file to screen (you can run ls to see the list of files in the current directory):\ncat slurm-xx.out    # Replace xx by the job number\nsystem\n     4\nWe now have 4 CPUs available on one node, so we can test shared memory parallelism.\n\n\nPlan multisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures.\nEdit the R script (with nano rf.R):\n\n\nrf.R\n\nlibrary(doFuture)\nlibrary(doRNG)\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(multisession)\n\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n\nRun the job with the new R script:\nsbatch rf.sh\nWe now get in the output file:\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 2.08s\n\nWe got a speedup of 5.78 / 2.08 = 2.8. Not bad considering that we have 4 CPU cores (the ideal speedup would be 4, but there is always some overhead to parallelization).\n\n\n\nPlan multicore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures.\nLet’s modify our R script again:\n\n\nrf.R\n\nlibrary(doFuture)\nlibrary(doRNG)\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(multicore)\n\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n\nRun the job:\nsbatch rf.sh\nWe get:\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 1.89s\n\nWe got a similar speedup of 5.78 / 1.89 = 3.1.\n\n\n\n\nMulti-processing in distributed memory\n\nCreate a cluster of workers\nTo test parallel execution in distributed memory, let’s ask Slurm for 8 tasks by editing our rf.sh script:\n\n\nrf.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7000M\n#SBATCH --ntasks=8\n\nRscript rf.R      # This is the code that we are running\n\nLet’s verify that we do get 8 tasks by accessing the SLURM_NTASKS environment variable from within R.\nEdit rf.R to contain the following:\n\n\nrf.R\n\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n\nRun the job:\nsbatch rf.sh\nWe get:\n[1] 8\nLet’s see which nodes we are using:\n\n\nrf.R\n\nsystem(\"srun hostname | cut -f 1 -d '.'\", intern = T)\n\nWe get:\n[1] \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\nTo run the RandomForest code with distributed parallelism using 8 CPU cores across both nodes, we will need to create a cluster of workers. We do this with the makeCluster() function from the base R parallel package: we create a character vector with the names of the nodes our tasks are running on and pass this vector to the makeCluster() function:\n## Create a character vector with the nodes names\nhosts <- system(\"srun hostname | cut -f 1 -d '.'\", intern = T)\n\n## Create the cluster of workers\ncl <- parallel::makeCluster(hosts)\nLet’s test it:\n\n\nrf.R\n\nlibrary(doFuture)\n\nhosts <- system(\"srun hostname | cut -f 1 -d '.'\", intern = T)\ncl <- parallel::makeCluster(hosts)\n\ncl\n\nIf we run this code, we get:\nLoading required package: foreach\nLoading required package: future\nsocket cluster with 8 nodes on hosts ‘node1’, ‘node2’\n\nMake sure that your code has finished running before printing the output file. Remember that you can monitor the job with sq.\n\n\n\nPlan cluster\nWe can now run the code in distributed memory parallelism:\n\n\nrf.R\n\nlibrary(doFuture)\nlibrary(doRNG)\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\n\nhosts <- system(\"srun hostname | cut -f 1 -d '.'\", intern = T)\ncl <- parallel::makeCluster(hosts)\nplan(cluster, workers = cl)\n\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n\nWe get:\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 1.16s\n\nSpeedup: 5.78 / 1.16 = 5.0. Here again, this is not bad with 8 CPU cores, considering the added overhead of message passing between both nodes.\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)\nHere, this is not necessary since our job stops running as soon as the execution is complete."
  },
  {
    "objectID": "r_parallel/parallel_types.html",
    "href": "r_parallel/parallel_types.html",
    "title": "Types of parallelism",
    "section": "",
    "text": "There are various ways to run code in parallel and it is important to have a clear understanding of what each method entails."
  },
  {
    "objectID": "r_parallel/parallel_types.html#multi-threading",
    "href": "r_parallel/parallel_types.html#multi-threading",
    "title": "Types of parallelism",
    "section": "Multi-threading",
    "text": "Multi-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nMulti-threading does not seem to be a common approach to parallelizing R code."
  },
  {
    "objectID": "r_parallel/parallel_types.html#multi-processing-in-shared-memory",
    "href": "r_parallel/parallel_types.html#multi-processing-in-shared-memory",
    "title": "Types of parallelism",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory."
  },
  {
    "objectID": "r_parallel/parallel_types.html#multi-processing-in-distributed-memory",
    "href": "r_parallel/parallel_types.html#multi-processing-in-distributed-memory",
    "title": "Types of parallelism",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory."
  },
  {
    "objectID": "r_parallel/performance.html",
    "href": "r_parallel/performance.html",
    "title": "Measuring performance:",
    "section": "",
    "text": "Before we talk about ways to improve performance, let’s see how to measure it."
  },
  {
    "objectID": "r_parallel/performance.html#when-should-you-care",
    "href": "r_parallel/performance.html#when-should-you-care",
    "title": "Measuring performance:",
    "section": "When should you care?",
    "text": "When should you care?\n\n“There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”\n— Donald Knuth\n\nOptimizing code takes time, can lead to mistakes, and may make code harder to read. Consequently, not all code is worth optimizing and before jumping into optimizations, you need a strategy.\nYou should consider optimizations when:\n\nyou have debugged your code (optimization comes last, don’t optimize a code that doesn’t run),\nyou will run a section of code (e.g. a function) many times (your optimization efforts will really pay off),\na section of code is particularly slow.\n\nHow do you know which sections of your code are slow? Don’t rely on intuition. You need to profile your code to identify bottlenecks."
  },
  {
    "objectID": "r_parallel/performance.html#profiling",
    "href": "r_parallel/performance.html#profiling",
    "title": "Measuring performance:",
    "section": "Profiling",
    "text": "Profiling\n\n“It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail.”\n— Donald Knuth\n\nR comes with a profiler: Rprof().\nprofvis is a newer tool, built by posit (formerly RStudio Inc). Under the hood, it runs Rprof() to collect data, then produces an interactive html widget with a flame graph that allows for an easy visual identification of slow sections of code.\nWhile this tool integrates well within the RStudio IDE, it is not very well suited for remote work on a cluster. One option is to profile your code with small data on your own machine. Another option is to use the base profiler Rprof() directly as in this example."
  },
  {
    "objectID": "r_parallel/performance.html#benchmarking",
    "href": "r_parallel/performance.html#benchmarking",
    "title": "Measuring performance:",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a much better option. It gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package is very similar, but it has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package that we will use for this course.\nThe main function from this package is mark(). You can pass as argument(s) one or multiple expressions that you want to benchmark. By default, it ensures that all expressions output the same result. If you want to remove this test, add the argument check = FALSE.\nWhile mark() gives memory usage and garbage collection information for sequential code, this functionality is not yet implemented for parallel code. When benchmarking parallel expressions, we will have to use the argument memory = FALSE.\nYou will see many examples throughout this course."
  },
  {
    "objectID": "r_parallel/rcpp.html",
    "href": "r_parallel/rcpp.html",
    "title": "Writing C++ in R with Rcpp",
    "section": "",
    "text": "Sometimes, parallelization is not an option, either because the code is hard to parallelize or because of lack of hardware. In such cases, one way to increase speed is to replace slow R code with C++. The package Rcpp makes this particularly easy by creating mappings between both languages."
  },
  {
    "objectID": "r_parallel/rcpp.html#back-to-fibonacci",
    "href": "r_parallel/rcpp.html#back-to-fibonacci",
    "title": "Writing C++ in R with Rcpp",
    "section": "Back to Fibonacci",
    "text": "Back to Fibonacci\nDo you remember the Fibonacci numbers? Here was a naive implementation in R:\n\nfib <- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\nThis function gives the nth number in the sequence.\n\nExample:\n\n\nfib(30)\n\n[1] 832040"
  },
  {
    "objectID": "r_parallel/rcpp.html#rcpp",
    "href": "r_parallel/rcpp.html#rcpp",
    "title": "Writing C++ in R with Rcpp",
    "section": "Rcpp",
    "text": "Rcpp\nLet’s translate this function in C++ within R!\nFirst we need to load the Rcpp package:\n\nlibrary(Rcpp)\n\nWe then use the function cppFunction() to assign to an R function a function written in C++:\n\nfibRcpp <- cppFunction( '\nint fibonacci(const int x) {\n   if (x == 0) return(0);\n   if (x == 1) return(1);\n   return (fibonacci(x - 1)) + fibonacci(x - 2);\n}\n' )\n\nWe can call our function as any R function:\nfibRcpp(30)\n[1] 832040\nWe can compare both functions:\nlibrary(bench)\n\nn <- 30\nmark(fib(n), fibRcpp(n))\n# A tibble: 2 × 13\n  expression      min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc\n  <bch:expr> <bch:tm> <bch:tm>     <dbl> <bch:byt>    <dbl> <int> <dbl>\n1 fib(n)        1.66s    1.66s     0.601    44.7KB     22.8     1    38\n2 fibRcpp(n)   1.08ms   1.08ms   901.       2.49KB      0     451     0\n  total_time result    memory                 time            \n    <bch:tm> <list>    <list>                 <list>          \n1      1.66s <dbl [1]> <Rprofmem [6,778 × 3]> <bench_tm [1]>  \n2   500.37ms <int [1]> <Rprofmem [1 × 3]>     <bench_tm [451]>\n  gc                \n  <list>            \n1 <tibble [1 × 3]>  \n2 <tibble [451 × 3]>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nThe speedup is 1,537, which is amazing.\nIn this particular example, we saw that memoisation gives an even more incredible speedup (35,000!), but while memoisation will only work in very specific situations (e.g. recursive function calls), using C++ code is a general method to provide speedup. It is particularly useful when:\n\nthere are large numbers of function calls (R is particularly slow with function calls),\nyou need data structures that are missing in R,\nyou want to create efficient packages (fast R packages are written in C++ and many use Rcpp).\n\n\nIn this example, we declared the C++ function directly in R. It is possible to use source files instead."
  },
  {
    "objectID": "r_parallel/resources_hpc.html",
    "href": "r_parallel/resources_hpc.html",
    "title": "Resources for HPC in R",
    "section": "",
    "text": "For introductory/general R resources, see this page instead.\n\n\nCRAN Task Views\n\nCRAN Task Views give information on packages relevant to certain topics.\n\n\nHigh-Performance and Parallel Computing with R\n\n\n\nRunning R on the Alliance clusters\n\nThree relevant pages from the Alliance wiki:\n\n\nGetting started\nRunning jobs\nRunning R\n\n\n\nOnline books\n\nAdvanced R\nEfficient R programming\n\n\n\nRcpp\n\nDocumentation and examples"
  },
  {
    "objectID": "r_parallel/run_r_hpc.html",
    "href": "r_parallel/run_r_hpc.html",
    "title": "Running R for this course",
    "section": "",
    "text": "You do not need to install anything on your machine for this course as we will provide access to a temporary remote cluster.\nFor the first part of this course, we will run R on a RStudio server running on the temporary cluster.\n\nRStudio is an IDE that provides a friendly interface to run R interactively. It is a great way to develop code and it makes profiling easy.\n\nFor the second part, we will log in to the same temporary cluster using Secure Shell, then run R scripts from the command line.\n\nOnce you have developed your code in an interactive fashion, running scripts allows you to request large hardware resources only when you need them (i.e. when your code is executed).\nThis prevents these heavy resources from sitting idle when not in use, as would happen in an interactive session.\nThis will save you a lot of money if you use a commercial cluster. If you use the Alliance clusters, it will save you lots of waiting time until you can run your code (the more resources you ask, the more time you have to wait to have access to them).\n\nNote that this cluster is only available during the course."
  },
  {
    "objectID": "r_parallel/run_r_hpc.html#accessing-the-temporary-rstudio-server",
    "href": "r_parallel/run_r_hpc.html#accessing-the-temporary-rstudio-server",
    "title": "Running R for this course",
    "section": "Accessing the temporary RStudio server",
    "text": "Accessing the temporary RStudio server\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry.\n\nUsing RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r_parallel/run_r_hpc.html#log-in-to-the-temporary-cluster-through-ssh",
    "href": "r_parallel/run_r_hpc.html#log-in-to-the-temporary-cluster-through-ssh",
    "title": "Running R for this course",
    "section": "Log in to the temporary cluster through SSH",
    "text": "Log in to the temporary cluster through SSH\n\nWe will give you the hostname during the workshop. Your username and password are the same as the ones you used for the RStudio server.\n\n\nOpen a terminal emulator\nWindows users:  install the free version of MobaXTerm and launch it.\nMacOS users:   launch Terminal.\nLinux users:     open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nMacOS and Linux users\nIn the terminal, run:\nssh <username>@<hostname>\n\nReplace the username and hostname by their values. For instance:\nssh user021@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully."
  },
  {
    "objectID": "scivis/index.html",
    "href": "scivis/index.html",
    "title": "Scientific visualization with ParaView",
    "section": "",
    "text": "Date:\nFriday May 5, 2023\nTime:\n9am–5pm (with a two-hour break from noon to 2pm)\nInstructor:\nAlex Razoumov (Simon Fraser University)\nPrerequisites:\nThis is an introductory course with no prior visualization experience required.\nSoftware:\nPlease install ParaView on your computer, and make sure you can start it before the course. As of this writing, the latest ParaView version is 5.11 – it should work nicely for our workshop. We will provide all sample data and codes for the exercises. Let us know before or during the course if you want to load your own dataset into ParaView.\n\n\n\nDownload today’s materials as a single ZIP file with slides (two PDFs), data and scripts inside. Unpack it where you can find it on your computer.\nInstall recent ParaView on your computer. Make sure it runs, i.e. you have installed the right version for your operating system and processor architecture."
  }
]